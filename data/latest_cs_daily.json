{
  "source": "arXiv API",
  "fetched_at": "2026-02-27T06:05:04.835067+00:00",
  "window_days": 30,
  "window_start": "2026-01-28T06:05:04.835067+00:00",
  "window_end": "2026-02-27T06:05:04.835067+00:00",
  "fetch_strategy": "incremental",
  "categories": [
    "cs.RO",
    "cs.CV",
    "cs.CL",
    "cs.SY"
  ],
  "total_count": 1128,
  "total_new_count": 0,
  "total_request_pages": 1,
  "fields": [
    {
      "code": "cs.RO",
      "name": "Robotics",
      "query": "cat:cs.RO",
      "feed_title": "arXiv Query: search_query=cat:cs.RO&id_list=&start=0&max_results=200",
      "count": 1128,
      "new_count": 0,
      "request_pages": 1,
      "papers": [
        {
          "id": "http://arxiv.org/abs/2602.23312v1",
          "title": "Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction",
          "summary": "Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model's architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.",
          "authors": [
            "Rafael R. Baptista",
            "André de Lima Salgado",
            "Ricardo V. Godoy",
            "Marcelo Becker",
            "Thiago Boaventura",
            "Gustavo J. G. Lahr"
          ],
          "published": "2026-02-26T18:20:26Z",
          "updated": "2026-02-26T18:20:26Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.AI",
            "cs.LG",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23312v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23287v1",
          "title": "Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning",
          "summary": "Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.",
          "authors": [
            "Demiana R. Barsoum",
            "Mahdieh Nejati Javaremi",
            "Larisa Y. C. Loke",
            "Brenna D. Argall"
          ],
          "published": "2026-02-26T18:01:25Z",
          "updated": "2026-02-26T18:01:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23287v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23283v1",
          "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots",
          "summary": "Mimicking the graceful motion of swimming animals remains a core challenge in soft robotics due to the complexity of fluid-structure interaction and the difficulty of controlling soft, biomimetic bodies. Existing modeling approaches are often computationally expensive and impractical for complex control or reinforcement learning needed for realistic motions to emerge in robotic systems. In this work, we present a tendon-driven fish robot modeled in an efficient underwater swimmer environment using a simplified, stateless hydrodynamics formulation implemented in the widespread robotics framework MuJoCo. With just two real-world swimming trajectories, we identify five fluid parameters that allow a matching to experimental behavior and generalize across a range of actuation frequencies. We show that this stateless fluid model can generalize to unseen actuation and outperform classical analytical models such as the elongated body theory. This simulation environment runs faster than real-time and can easily enable downstream learning algorithms such as reinforcement learning for target tracking, reaching a 93% success rate. Due to the simplicity and ease of use of the model and our open-source simulation environment, our results show that even simple, stateless models -- when carefully matched to physical data -- can serve as effective digital twins for soft underwater robots, opening up new directions for scalable learning and control in aquatic environments.",
          "authors": [
            "Mike Y. Michelis",
            "Nana Obayashi",
            "Josie Hughes",
            "Robert K. Katzschmann"
          ],
          "published": "2026-02-26T17:55:22Z",
          "updated": "2026-02-26T17:55:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23283v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23280v1",
          "title": "Physics Informed Viscous Value Representations",
          "summary": "Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.",
          "authors": [
            "Hrishikesh Viswanath",
            "Juanwu Lu",
            "S. Talha Bukhari",
            "Damon Conover",
            "Ziran Wang",
            "Aniket Bera"
          ],
          "published": "2026-02-26T17:53:46Z",
          "updated": "2026-02-26T17:53:46Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23280v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23259v1",
          "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
          "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
          "authors": [
            "Jiangxin Sun",
            "Feng Xue",
            "Teng Long",
            "Chang Liu",
            "Jian-Fang Hu",
            "Wei-Shi Zheng",
            "Nicu Sebe"
          ],
          "published": "2026-02-26T17:32:30Z",
          "updated": "2026-02-26T17:32:30Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23259v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23253v1",
          "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly",
          "summary": "Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.",
          "authors": [
            "Yijie Guo",
            "Iretiayo Akinola",
            "Lars Johannsmeier",
            "Hugo Hadfield",
            "Abhishek Gupta",
            "Yashraj Narang"
          ],
          "published": "2026-02-26T17:26:13Z",
          "updated": "2026-02-26T17:26:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23253v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23224v1",
          "title": "UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception",
          "summary": "We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.",
          "authors": [
            "Mohammad Mahdavian",
            "Gordon Tan",
            "Binbin Xu",
            "Yuan Ren",
            "Dongfeng Bai",
            "Bingbing Liu"
          ],
          "published": "2026-02-26T17:04:36Z",
          "updated": "2026-02-26T17:04:36Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23224v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23206v1",
          "title": "Grasp, Slide, Roll: Comparative Analysis of Contact Modes for Tactile-Based Shape Reconstruction",
          "summary": "Tactile sensing allows robots to gather detailed geometric information about objects through physical interaction, complementing vision-based approaches. However, efficiently acquiring useful tactile data remains challenging due to the time-consuming nature of physical contact and the need to strategically choose contact locations that maximize information gain while minimizing physical interactions. This paper studies how different contact modes affect object shape reconstruction using a tactile-enabled dexterous gripper. We compare three contact interaction modes: grasp-releasing, sliding induced by finger-grazing, and palm-rolling. These contact modes are combined with an information-theoretic exploration framework that guides subsequent sampling locations using a shape completion model. Our results show that the improved tactile sensing efficiency of finger-grazing and palm-rolling translates into faster convergence in shape reconstruction, requiring 34% fewer physical interactions while improving reconstruction accuracy by 55%. We validate our approach using a UR5e robot arm equipped with an Inspire-Robots Dexterous Hand, showing robust performance across primitive object geometries.",
          "authors": [
            "Chung Hee Kim",
            "Shivani Kamtikar",
            "Tye Brady",
            "Taskin Padir",
            "Joshua Migdal"
          ],
          "published": "2026-02-26T16:53:59Z",
          "updated": "2026-02-26T16:53:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23206v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23204v1",
          "title": "Motion-aware Event Suppression for Event Cameras",
          "summary": "In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the challenging EVIMO benchmark by 67\\% in segmentation accuracy while operating at a 53\\% higher inference rate. Moreover, we demonstrate significant benefits for downstream applications: our method accelerates Vision Transformer inference by 83\\% via token pruning and improves event-based visual odometry accuracy, reducing Absolute Trajectory Error (ATE) by 13\\%.",
          "authors": [
            "Roberto Pellerito",
            "Nico Messikommer",
            "Giovanni Cioffi",
            "Marco Cannici",
            "Davide Scaramuzza"
          ],
          "published": "2026-02-26T16:53:36Z",
          "updated": "2026-02-26T16:53:36Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23204v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23172v1",
          "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking",
          "summary": "Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a holistic direction. Our approach incorporates camera-based end-to-end tracking with mask-based multi-view panoptic occupancy prediction, and addresses the key challenge of efficiently aggregating multi-view information into 3D voxel grids via a novel latent Gaussian splatting approach. Specifically, we first fuse observations into 3D Gaussians that serve as a sparse point-centric latent representation of the 3D scene, and then splat the aggregated features onto a 3D voxel grid that is decoded by a mask-based segmentation head. We evaluate LaGS on the Occ3D nuScenes and Waymo datasets, achieving state-of-the-art performance for 4D panoptic occupancy tracking. We make our code available at https://lags.cs.uni-freiburg.de/.",
          "authors": [
            "Maximilian Luz",
            "Rohit Mohan",
            "Thomas Nürnberg",
            "Yakov Miron",
            "Daniele Cattaneo",
            "Abhinav Valada"
          ],
          "published": "2026-02-26T16:34:49Z",
          "updated": "2026-02-26T16:34:49Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23172v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23115v1",
          "title": "FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time",
          "summary": "Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera's heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.",
          "authors": [
            "David Dirnfeld",
            "Fabien Delattre",
            "Pedro Miraldo",
            "Erik Learned-Miller"
          ],
          "published": "2026-02-26T15:27:49Z",
          "updated": "2026-02-26T15:27:49Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.CG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23115v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23109v1",
          "title": "Towards Intelligible Human-Robot Interaction: An Active Inference Approach to Occluded Pedestrian Scenarios",
          "summary": "The sudden appearance of occluded pedestrians presents a critical safety challenge in autonomous driving. Conventional rule-based or purely data-driven approaches struggle with the inherent high uncertainty of these long-tail scenarios. To tackle this challenge, we propose a novel framework grounded in Active Inference, which endows the agent with a human-like, belief-driven mechanism. Our framework leverages a Rao-Blackwellized Particle Filter (RBPF) to efficiently estimate the pedestrian's hybrid state. To emulate human-like cognitive processes under uncertainty, we introduce a Conditional Belief Reset mechanism and a Hypothesis Injection technique to explicitly model beliefs about the pedestrian's multiple latent intentions. Planning is achieved via a Cross-Entropy Method (CEM) enhanced Model Predictive Path Integral (MPPI) controller, which synergizes the efficient, iterative search of CEM with the inherent robustness of MPPI. Simulation experiments demonstrate that our approach significantly reduces the collision rate compared to reactive, rule-based, and reinforcement learning (RL) baselines, while also exhibiting explainable and human-like driving behavior that reflects the agent's internal belief state.",
          "authors": [
            "Kai Chen",
            "Yuyao Huang",
            "Guang Chen"
          ],
          "published": "2026-02-26T15:22:07Z",
          "updated": "2026-02-26T15:22:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23109v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23058v1",
          "title": "GeoWorld: Geometric World Models",
          "summary": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
          "authors": [
            "Zeyu Zhang",
            "Danning Li",
            "Ian Reid",
            "Richard Hartley"
          ],
          "published": "2026-02-26T14:42:53Z",
          "updated": "2026-02-26T14:42:53Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23058v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23053v1",
          "title": "Marinarium: a New Arena to Bring Maritime Robotics Closer to Shore",
          "summary": "This paper presents the Marinarium, a modular and stand-alone underwater research facility designed to provide a realistic testbed for maritime and space-analog robotic experimentation in a resource-efficient manner. The Marinarium combines a fully instrumented underwater and aerial operational volume, extendable via a retractable roof for real-weather conditions, a digital twin in the SMaRCSim simulator and tight integration with a space robotics laboratory. All of these result from design choices aimed at bridging simulation, laboratory validation, and field conditions. We compare the Marinarium to similar existing infrastructures and illustrate how its design enables a set of experiments in four open research areas within field robotics. First, we exploit high-fidelity dynamics data from the tank to demonstrate the potential of learning-based system identification approaches applied to underwater vehicles. We further highlight the versatility of the multi-domain operating volume via a rendezvous mission with a heterogeneous fleet of robots across underwater, surface, and air. We then illustrate how the presented digital twin can be utilized to reduce the reality gap in underwater simulation. Finally, we demonstrate the potential of underwater surrogates for spacecraft navigation validation by executing spatiotemporally identical inspection tasks on a planar space-robot emulator and a neutrally buoyant \\gls{rov}. In this work, by sharing the insights obtained and rationale behind the design and construction of the Marinarium, we hope to provide the field robotics research community with a blueprint for bridging the gap between controlled and real offshore and space robotics experimentation.",
          "authors": [
            "Ignacio Torroba",
            "David Dorner",
            "Victor Nan Fernandez-Ayala",
            "Mart Kartasev",
            "Joris Verhagen",
            "Elias Krantz",
            "Gregorio Marchesini",
            "Carl Ljung",
            "Pedro Roque",
            "Chelsea Sidrane",
            "Linda Van der Spaa",
            "Nicola De Carli",
            "Petter Ogren",
            "Christer Fuglesang",
            "Jana Tumova",
            "Dimos V. Dimarogonas",
            "Ivan Stenius"
          ],
          "published": "2026-02-26T14:40:20Z",
          "updated": "2026-02-26T14:40:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23053v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23051v1",
          "title": "An Empirical Analysis of Cooperative Perception for Occlusion Risk Mitigation",
          "summary": "Occlusions present a significant challenge for connected and automated vehicles, as they can obscure critical road users from perception systems. Traditional risk metrics often fail to capture the cumulative nature of these threats over time adequately. In this paper, we propose a novel and universal risk assessment metric, the Risk of Tracking Loss (RTL), which aggregates instantaneous risk intensity throughout occluded periods. This provides a holistic risk profile that encompasses both high-intensity, short-term threats and prolonged exposure. Utilizing diverse and high-fidelity real-world datasets, a large-scale statistical analysis is conducted to characterize occlusion risk and validate the effectiveness of the proposed metric. The metric is applied to evaluate different vehicle-to-everything (V2X) deployment strategies. Our study shows that full V2X penetration theoretically eliminates this risk, the reduction is highly nonlinear; a substantial statistical benefit requires a high penetration threshold of 75-90%. To overcome this limitation, we propose a novel asymmetric communication framework that allows even non-connected vehicles to receive warnings. Experimental results demonstrate that this paradigm achieves better risk mitigation performance. We found that our approach at 25% penetration outperforms the traditional symmetric model at 75%, and benefits saturate at only 50% penetration. This work provides a crucial risk assessment metric and a cost-effective, strategic roadmap for accelerating the safety benefits of V2X deployment.",
          "authors": [
            "Aihong Wang",
            "Tenghui Xie",
            "Fuxi Wen",
            "Jun Li"
          ],
          "published": "2026-02-26T14:38:38Z",
          "updated": "2026-02-26T14:38:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23051v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23024v1",
          "title": "InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation",
          "summary": "Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.",
          "authors": [
            "Jiahao Liu",
            "Cui Wenbo",
            "Haoran Li",
            "Dongbin Zhao"
          ],
          "published": "2026-02-26T14:03:58Z",
          "updated": "2026-02-26T14:03:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23024v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.23017v1",
          "title": "DigiArm: An Anthropomorphic 3D-Printed Prosthetic Hand with Enhanced Dexterity for Typing Tasks",
          "summary": "Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand. Current control systems for prosthetic hands are often limited to grasping, and commercial prosthetic hands lack the precision needed for dexterous manipulation or applications that require fine finger motions. Thus, there is a critical need for accessible and replicable prosthetic designs that enable individuals to interact with electronic devices and perform precise finger pressing, such as keyboard typing or piano playing, while preserving current prosthetic capabilities. This paper presents a low-cost, lightweight, 3D-printed robotic prosthetic hand, specifically engineered for enhanced dexterity with electronic devices such as a computer keyboard or piano, as well as general object manipulation. The robotic hand features a mechanism to adjust finger abduction/adduction spacing, a 2-D wrist with the inclusion of controlled ulnar/radial deviation optimized for typing, and control of independent finger pressing. We conducted a study to demonstrate how participants can use the robotic hand to perform keyboard typing and piano playing in real time, with different levels of finger and wrist motion. This supports the notion that our proposed design can allow for the execution of key typing motions more effectively than before, aiming to enhance the functionality of prosthetic hands.",
          "authors": [
            "Dean Zadok",
            "Tom Naamani",
            "Yuval Bar-Ratson",
            "Elisha Barash",
            "Oren Salzman",
            "Alon Wolf",
            "Alex M. Bronstein",
            "Nili Krausz"
          ],
          "published": "2026-02-26T13:55:05Z",
          "updated": "2026-02-26T13:55:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.23017v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22998v1",
          "title": "A Perspective on Open Challenges in Deformable Object Manipulation",
          "summary": "Deformable object manipulation (DOM) represents a critical challenge in robotics, with applications spanning healthcare, manufacturing, food processing, and beyond. Unlike rigid objects, deformable objects exhibit infinite dimensionality, dynamic shape changes, and complex interactions with their environment, posing significant hurdles for perception, modeling, and control. This paper reviews the state of the art in DOM, focusing on key challenges such as occlusion handling, task generalization, and scalable, real-time solutions. It highlights advancements in multimodal perception systems, including the integration of multi-camera setups, active vision, and tactile sensing, which collectively address occlusion and improve adaptability in unstructured environments. Cutting-edge developments in physically informed reinforcement learning (RL) and differentiable simulations are explored, showcasing their impact on efficiency, precision, and scalability. The review also emphasizes the potential of simulated expert demonstrations and generative neural networks to standardize task specifications and bridge the simulation-to-reality gap. Finally, future directions are proposed, including the adoption of graph neural networks for high-level decision-making and the creation of comprehensive datasets to enhance DOM's real-world applicability. By addressing these challenges, DOM research can pave the way for versatile robotic systems capable of handling diverse and dynamic tasks with deformable objects.",
          "authors": [
            "Ryan Paul McKennaa",
            "John Oyekan"
          ],
          "published": "2026-02-26T13:39:30Z",
          "updated": "2026-02-26T13:39:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22998v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22952v1",
          "title": "Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy",
          "summary": "Percutaneous dilatational tracheostomy (PDT) is frequently performed on patients in intensive care units for prolonged mechanical ventilation. The needle puncture, as the most critical step of PDT, could lead to adverse consequences such as major bleeding and posterior tracheal wall perforation if performed inaccurately. Current practices of PDT puncture are all performed manually with no navigation assistance, which leads to large position and angular errors (5 mm and 30 degree). To improve the accuracy and reduce the difficulty of the PDT procedure, we propose a system that automates the needle insertion using a velocity-controlled robotic manipulator. Guided using pose data from two electromagnetic sensors, one at the needle tip and the other inside the trachea, the robotic system uses an adaptive constrained controller to adapt the uncertain kinematic parameters online and avoid collisions with the patient's body and tissues near the target. Simulations were performed to validate the controller's implementation, and then four hundred PDT punctures were performed on a mannequin to evaluate the position and angular accuracy. The absolute median puncture position error was 1.7 mm (IQR: 1.9 mm) and midline deviation was 4.13 degree (IQR: 4.55 degree), measured by the sensor inside the trachea. The small deviations from the nominal puncture in a simulated experimental setup and formal guarantees of collision-free insertions suggest the feasibility of the robotic PDT puncture.",
          "authors": [
            "Yuan Tang",
            "Bruno V. Adorno",
            "Brendan A. McGrath",
            "Andrew Weightman"
          ],
          "published": "2026-02-26T12:47:04Z",
          "updated": "2026-02-26T12:47:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22952v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22940v1",
          "title": "Considering Perspectives for Automated Driving Ethics: Collective Risk in Vehicular Motion Planning",
          "summary": "Recent automated vehicle (AV) motion planning strategies evolve around minimizing risk in road traffic. However, they exclusively consider risk from the AV's perspective and, as such, do not address the ethicality of its decisions for other road users. We argue that this does not reduce the risk of each road user, as risk may be different from the perspective of each road user. Indeed, minimizing the risk from the AV's perspective may not imply that the risk from the perspective of other road users is also being minimized; in fact, it may even increase. To test this hypothesis, we propose an AV motion planning strategy that supports switching risk minimization strategies between all road user perspectives. We find that the risk from the perspective of other road users can generally be considered different to the risk from the AV's perspective. Taking a collective risk perspective, i.e., balancing the risks of all road users, we observe an AV that minimizes overall traffic risk the best, while putting itself at slightly higher risk for the benefit of others, which is consistent with human driving behavior. In addition, adopting a collective risk minimization strategy can also be beneficial to the AV's travel efficiency by acting assertively when other road users maintain a low risk estimate of the AV. Yet, the AV drives conservatively when its planned actions are less predictable to other road users, i.e., associated with high risk. We argue that such behavior is a form of self-reflection and a natural prerequisite for socially acceptable AV behavior. We conclude that to facilitate ethicality in road traffic that includes AVs, the risk-perspective of each road user must be considered in the decision-making of AVs.",
          "authors": [
            "Leon Tolksdorf",
            "Arturo Tejada",
            "Christian Birkner",
            "Nathan van de Wouw"
          ],
          "published": "2026-02-26T12:30:44Z",
          "updated": "2026-02-26T12:30:44Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22940v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22923v1",
          "title": "WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents",
          "summary": "While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.",
          "authors": [
            "Runwei Guan",
            "Shaofeng Liang",
            "Ningwei Ouyang",
            "Weichen Fei",
            "Shanliang Yao",
            "Wei Dai",
            "Chenhao Ge",
            "Penglei Sun",
            "Xiaohui Zhu",
            "Tao Huang",
            "Ryan Wen Liu",
            "Hui Xiong"
          ],
          "published": "2026-02-26T12:12:40Z",
          "updated": "2026-02-26T12:12:40Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22923v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22922v1",
          "title": "Bayesian Preference Elicitation: Human-In-The-Loop Optimization of An Active Prosthesis",
          "summary": "Tuning active prostheses for people with amputation is time-consuming and relies on metrics that may not fully reflect user needs. We introduce a human-in-the-loop optimization (HILO) approach that leverages direct user preferences to personalize a standard four-parameter prosthesis controller efficiently. Our method employs preference-based Multiobjective Bayesian Optimization that uses a state-or-the-art acquisition function especially designed for preference learning, and includes two algorithmic variants: a discrete version (\\textit{EUBO-LineCoSpar}), and a continuous version (\\textit{BPE4Prost}). Simulation results on benchmark functions and real-application trials demonstrate efficient convergence, robust preference elicitation, and measurable biomechanical improvements, illustrating the potential of preference-driven tuning for user-centered prosthesis control.",
          "authors": [
            "Sophia Taddei",
            "Wouter Koppen",
            "Eligia Alfio",
            "Stefano Nuzzo",
            "Louis Flynn",
            "Maria Alejandra Diaz",
            "Sebastian Rojas Gonzalez",
            "Tom Dhaene",
            "Kevin De Pauw",
            "Ivo Couckuyt",
            "Tom Verstraten"
          ],
          "published": "2026-02-26T12:11:51Z",
          "updated": "2026-02-26T12:11:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22922v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22896v1",
          "title": "DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation",
          "summary": "Vision-Language-Action (VLA) models have shown remarkable success in robotic tasks like manipulation by fusing a language model's reasoning with a vision model's 3D understanding. However, their high computational cost remains a major obstacle for real-world applications that require real-time performance. We observe that the actions within a task have varying levels of importance: critical steps demand high precision, while less important ones can tolerate more variance. Leveraging this insight, we propose DySL-VLA, a novel framework that addresses computational cost by dynamically skipping VLA layers based on each action's importance. DySL-VLA categorizes its layers into two types: informative layers, which are consistently executed, and incremental layers, which can be selectively skipped. To intelligently skip layers without sacrificing accuracy, we invent a prior-post skipping guidance mechanism to determine when to initiate layer-skipping. We also propose a skip-aware two-stage knowledge distillation algorithm to efficiently train a standard VLA into a DySL-VLA. Our experiments indicate that DySL-VLA achieves 2.1% improvement in success length over Deer-VLA on the Calvin dataset, while simultaneously reducing trainable parameters by a factor of 85.7 and providing a 3.75x speedup relative to the RoboFlamingo baseline at iso-accuracy. Our code is available on https://github.com/PKU-SEC-Lab/DYSL_VLA.",
          "authors": [
            "Zebin Yang",
            "Yijiahao Qi",
            "Tong Xie",
            "Bo Yu",
            "Shaoshan Liu",
            "Meng Li"
          ],
          "published": "2026-02-26T11:34:36Z",
          "updated": "2026-02-26T11:34:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22896v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22862v1",
          "title": "GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion",
          "summary": "This paper focuses on enhancing the grasping precision and generalization of manipulation policies learned via imitation learning. Diffusion-based policy learning methods have recently become the mainstream approach for robotic manipulation tasks. As grasping is a critical subtask in manipulation, the ability of imitation-learned policies to execute precise and generalizable grasps merits particular attention. Existing imitation learning techniques for grasping often suffer from imprecise grasp executions, limited spatial generalization, and poor object generalization. To address these challenges, we incorporate grasp prior knowledge into the diffusion policy framework. In particular, we employ a latent diffusion policy to guide action chunk decoding with grasp pose prior, ensuring that generated motion trajectories adhere closely to feasible grasp configurations. Furthermore, we introduce a self-supervised reconstruction objective during diffusion to embed the graspness prior: at each reverse diffusion step, we reconstruct wrist-camera images back-projected the graspness from the intermediate representations. Both simulation and real robot experiments demonstrate that our approach significantly outperforms baseline methods and exhibits strong dynamic grasping capabilities.",
          "authors": [
            "Enda Xiang",
            "Haoxiang Ma",
            "Xinzhu Ma",
            "Zicheng Liu",
            "Di Huang"
          ],
          "published": "2026-02-26T10:56:01Z",
          "updated": "2026-02-26T10:56:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22862v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22854v1",
          "title": "Performance and Experimental Analysis of Strain-based Models for Continuum Robots",
          "summary": "Although strain-based models have been widely adopted in robotics, no comparison beyond the uniform bending test is commonly recognized to assess their performance. In addition, the increasing effort in prototyping continuum robots highlights the need to assess the applicability of these models and the necessity of comprehensive performance evaluation. To address this gap, this work investigates the shape reconstruction abilities of a third-order strain interpolation method, examining its ability to capture both individual and combined deformation effects. These results are compared and discussed against the Geometric-Variable Strain approach. Subsequently, simulation results are experimentally verified by reshaping a slender rod while recording the resulting configurations using cameras. The rod configuration is imposed using a manipulator displacing one of its tips and extracted through reflective markers, without the aid of any other external sensor -- i.e. strain gauges or wrench sensors placed along the rod. The experiments demonstrate good agreement between the model predictions and observed shapes, with average error of 0.58% of the rod length and average computational time of 0.32s per configuration, outperforming existing models.",
          "authors": [
            "Annika Delucchi",
            "Vincenzo Di Paola",
            "Andreas Müller",
            "and Matteo Zoppi"
          ],
          "published": "2026-02-26T10:46:13Z",
          "updated": "2026-02-26T10:46:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22854v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22818v1",
          "title": "LeRobot: An Open-Source Library for End-to-End Robot Learning",
          "summary": "Robotics is undergoing a significant transformation powered by advances in high-level control techniques based on machine learning, giving rise to the field of robot learning. Recent progress in robot learning has been accelerated by the increasing availability of affordable teleoperation systems, large-scale openly available datasets, and scalable learning-based methods. However, development in the field of robot learning is often slowed by fragmented, closed-source tools designed to only address specific sub-components within the robotics stack. In this paper, we present \\texttt{lerobot}, an open-source library that integrates across the entire robot learning stack, from low-level middleware communication for motor controls to large-scale dataset collection, storage and streaming. The library is designed with a strong focus on real-world robotics, supporting accessible hardware platforms while remaining extensible to new embodiments. It also supports efficient implementations for various state-of-the-art robot learning algorithms from multiple prominent paradigms, as well as a generalized asynchronous inference stack. Unlike traditional pipelines which heavily rely on hand-crafted techniques, \\texttt{lerobot} emphasizes scalable learning approaches that improve directly with more data and compute. Designed for accessibility, scalability, and openness, \\texttt{lerobot} lowers the barrier to entry for researchers and practitioners to robotics while providing a platform for reproducible, state-of-the-art robot learning.",
          "authors": [
            "Remi Cadene",
            "Simon Aliberts",
            "Francesco Capuano",
            "Michel Aractingi",
            "Adil Zouitine",
            "Pepijn Kooijmans",
            "Jade Choghari",
            "Martino Russi",
            "Caroline Pascal",
            "Steven Palma",
            "Mustafa Shukor",
            "Jess Moss",
            "Alexander Soare",
            "Dana Aubakirova",
            "Quentin Lhoest",
            "Quentin Gallouédec",
            "Thomas Wolf"
          ],
          "published": "2026-02-26T09:59:50Z",
          "updated": "2026-02-26T09:59:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22818v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22801v1",
          "title": "Unleashing the Potential of Diffusion Models for End-to-End Autonomous Driving",
          "summary": "Diffusion models have become a popular choice for decision-making tasks in robotics, and more recently, are also being considered for solving autonomous driving tasks. However, their applications and evaluations in autonomous driving remain limited to simulation-based or laboratory settings. The full strength of diffusion models for large-scale, complex real-world settings, such as End-to-End Autonomous Driving (E2E AD), remains underexplored. In this study, we conducted a systematic and large-scale investigation to unleash the potential of the diffusion models as planners for E2E AD, based on a tremendous amount of real-vehicle data and road testing. Through comprehensive and carefully controlled studies, we identify key insights into the diffusion loss space, trajectory representation, and data scaling that significantly impact E2E planning performance. Moreover, we also provide an effective reinforcement learning post-training strategy to further enhance the safety of the learned planner. The resulting diffusion-based learning framework, Hyper Diffusion Planner} (HDP), is deployed on a real-vehicle platform and evaluated across 6 urban driving scenarios and 200 km of real-world testing, achieving a notable 10x performance improvement over the base model. Our work demonstrates that diffusion models, when properly designed and trained, can serve as effective and scalable E2E AD planners for complex, real-world autonomous driving tasks.",
          "authors": [
            "Yinan Zheng",
            "Tianyi Tan",
            "Bin Huang",
            "Enguang Liu",
            "Ruiming Liang",
            "Jianlin Zhang",
            "Jianwei Cui",
            "Guang Chen",
            "Kun Ma",
            "Hangjun Ye",
            "Long Chen",
            "Ya-Qin Zhang",
            "Xianyuan Zhan",
            "Jingjing Liu"
          ],
          "published": "2026-02-26T09:37:38Z",
          "updated": "2026-02-26T09:37:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22801v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22733v1",
          "title": "Pixel2Catch: Multi-Agent Sim-to-Real Transfer for Agile Manipulation with a Single RGB Camera",
          "summary": "To catch a thrown object, a robot must be able to perceive the object's motion and generate control actions in a timely manner. Rather than explicitly estimating the object's 3D position, this work focuses on a novel approach that recognizes object motion using pixel-level visual information extracted from a single RGB image. Such visual cues capture changes in the object's position and scale, allowing the policy to reason about the object's motion. Furthermore, to achieve stable learning in a high-DoF system composed of a robot arm equipped with a multi-fingered hand, we design a heterogeneous multi-agent reinforcement learning framework that defines the arm and hand as independent agents with distinct roles. Each agent is trained cooperatively using role-specific observations and rewards, and the learned policies are successfully transferred from simulation to the real world.",
          "authors": [
            "Seongyong Kim",
            "Junhyeon Cho",
            "Kang-Won Lee",
            "Soo-Chul Lim"
          ],
          "published": "2026-02-26T08:15:38Z",
          "updated": "2026-02-26T08:15:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22733v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22731v1",
          "title": "Sapling-NeRF: Geo-Localised Sapling Reconstruction in Forests for Ecological Monitoring",
          "summary": "Saplings are key indicators of forest regeneration and overall forest health. However, their fine-scale architectural traits are difficult to capture with existing 3D sensing methods, which make quantitative evaluation difficult. Terrestrial Laser Scanners (TLS), Mobile Laser Scanners (MLS), or traditional photogrammetry approaches poorly reconstruct thin branches, dense foliage, and lack the scale consistency needed for long-term monitoring. Implicit 3D reconstruction methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are promising alternatives, but cannot recover the true scale of a scene and lack any means to be accurately geo-localised. In this paper, we present a pipeline which fuses NeRF, LiDAR SLAM, and GNSS to enable repeatable, geo-localised ecological monitoring of saplings. Our system proposes a three-level representation: (i) coarse Earth-frame localisation using GNSS, (ii) LiDAR-based SLAM for centimetre-accurate localisation and reconstruction, and (iii) NeRF-derived object-centric dense reconstruction of individual saplings. This approach enables repeatable quantitative evaluation and long-term monitoring of sapling traits. Our experiments in forest plots in Wytham Woods (Oxford, UK) and Evo (Finland) show that stem height, branching patterns, and leaf-to-wood ratios can be captured with increased accuracy as compared to TLS. We demonstrate that accurate stem skeletons and leaf distributions can be measured for saplings with heights between 0.5m and 2m in situ, giving ecologists access to richer structural and quantitative data for analysing forest dynamics.",
          "authors": [
            "Miguel Ángel Muñoz-Bañón",
            "Nived Chebrolu",
            "Sruthi M. Krishna Moorthy",
            "Yifu Tao",
            "Fernando Torres",
            "Roberto Salguero-Gómez",
            "Maurice Fallon"
          ],
          "published": "2026-02-26T08:13:47Z",
          "updated": "2026-02-26T08:13:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22731v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22714v1",
          "title": "Robust Helicopter Ship Deck Landing With Guaranteed Timing Using Shrinking-Horizon Model Predictive Control",
          "summary": "We present a runtime efficient algorithm for autonomous helicopter landings on moving ship decks based on Shrinking-Horizon Model Predictive Control (SHMPC). First, a suitable planning model capturing the relevant aspects of the full nonlinear helicopter dynamics is derived. Next, we use the SHMPC together with a touchdown controller stage to ensure a pre-specified maneuver time and an associated landing time window despite the presence of disturbances. A high disturbance rejection performance is achieved by designing an ancillary controller with disturbance feedback. Thus, given a target position and time, a safe landing with suitable terminal conditions is be guaranteed if the initial optimization problem is feasible. The efficacy of our approach is shown in simulation where all maneuvers achieve a high landing precision in strong winds while satisfying timing and operational constraints with maximum computation times in the millisecond range.",
          "authors": [
            "Philipp Schitz",
            "Paolo Mercorelli",
            "Johann C. Dauer"
          ],
          "published": "2026-02-26T07:40:57Z",
          "updated": "2026-02-26T07:40:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22714v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22707v1",
          "title": "SCOPE: Skeleton Graph-Based Computation-Efficient Framework for Autonomous UAV Exploration",
          "summary": "Autonomous exploration in unknown environments is key for mobile robots, helping them perceive, map, and make decisions in complex areas. However, current methods often rely on frequent global optimization, suffering from high computational latency and trajectory oscillation, especially on resource-constrained edge devices. To address these limitations, we propose SCOPE, a novel framework that incrementally constructs a real-time skeletal graph and introduces Implicit Unknown Region Analysis for efficient spatial reasoning. The planning layer adopts a hierarchical on-demand strategy: the Proximal Planner generates smooth, high-frequency local trajectories, while the Region-Sequence Planner is activated only when necessary to optimize global visitation order. Comparative evaluations in simulation demonstrate that SCOPE achieves competitive exploration performance comparable to state-of-the-art global planners, while reducing computational cost by an average of 86.9%. Real-world experiments further validate the system's robustness and low latency in practical scenarios.",
          "authors": [
            "Kai Li",
            "Shengtao Zheng",
            "Linkun Xiu",
            "Yuze Sheng",
            "Xiao-Ping Zhang",
            "Dongyue Huang",
            "Xinlei Chen"
          ],
          "published": "2026-02-26T07:31:29Z",
          "updated": "2026-02-26T07:31:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22707v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22671v1",
          "title": "Does the testing environment matter? Carsickness across on-road, test-track, and driving simulator conditions",
          "summary": "Carsickness has gained significant attention with the rise of automated vehicles, prompting extensive research across on-road, test-track, and driving simulator environments to understand its occurrence and develop mitigation strategies. However, the lack of carsickness standardization complicates comparisons across studies and environments. Previous works demonstrate measurement validity between two setups at most (e.g., on-road vs. driving simulator), leaving gaps in multi-environment comparisons. This study investigates the recreation of an on-road motion sickness exposure - previously replicated on a test track - using a motion-based driving simulator. Twenty-eight participants performed an eyes-off-road non-driving task while reporting motion sickness using the Misery Scale during the experiment and the Motion Sickness Assessment Questionnaire afterward. Psychological factors known to influence motion sickness were also assessed. The results present subjective and objective measurements for motion sickness across the considered environments. In this paper, acceleration measurements, objective metrics and subjective motion sickness ratings across environments are compared, highlighting key differences in sickness occurrence for simulator-based research validity. Significantly lower motion sickness scores are reported in the simulator compared to on-road and test-track conditions, due to its limited working envelope to reproduce low-frequency (<0.5 Hz) motions, which are the most provocative for motion sickness.",
          "authors": [
            "Georgios Papaioannou",
            "Barys Shyrokau"
          ],
          "published": "2026-02-26T06:42:25Z",
          "updated": "2026-02-26T06:42:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.ET"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22671v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22663v1",
          "title": "Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline",
          "summary": "Vision-Language-Action (VLA) models have emerged as a generalist robotic agent. However, existing VLAs are hindered by excessive parameter scales, prohibitive pre-training requirements, and limited applicability to diverse embodiments. To improve the practicality of VLAs, we propose a comprehensive benchmark and an improved baseline. First, we propose CEBench, a new benchmark spanning diverse embodiments in both simulation and the real world with consideration of domain randomization. We collect 14.4k simulated trajectories and 1.6k real-world expert-curated trajectories to support training on CEBench. Second, using CEBench as our testbed, we study three critical aspects of VLAs' practicality and offer several key findings. Informed by these findings, we introduce LLaVA-VLA, a lightweight yet powerful VLA designed for practical deployment on consumer-grade GPUs. Architecturally, it integrates a compact VLM backbone with multi-view perception, proprioceptive tokenization, and action chunking. To eliminate reliance on costly pre-training, LLaVA-VLA adopts a two-stage training paradigm including post-training and fine-tuning. Furthermore, LLaVA-VLA extends the action space to unify navigation and manipulation. Experiments across embodiments demonstrate the capabilities of generalization and versatility of LLaVA-VLA , while real-world mobile manipulation experiments establish it as the first end-to-end VLA model for mobile manipulation. We will open-source all datasets, codes, and checkpoints upon acceptance to foster reproducibility and future research.",
          "authors": [
            "Wenxuan Song",
            "Jiayi Chen",
            "Xiaoquan Sun",
            "Huashuo Lei",
            "Yikai Qin",
            "Wei Zhao",
            "Pengxiang Ding",
            "Han Zhao",
            "Tongxin Wang",
            "Pengxu Hou",
            "Zhide Zhong",
            "Haodong Yan",
            "Donglin Wang",
            "Jun Ma",
            "Haoang Li"
          ],
          "published": "2026-02-26T06:27:37Z",
          "updated": "2026-02-26T06:27:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22663v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22628v1",
          "title": "Designing Robots for Families: In-Situ Prototyping for Contextual Reminders on Family Routines",
          "summary": "Robots are increasingly entering the daily lives of families, yet their successful integration into domestic life remains a challenge. We explore family routines as a critical entry point for understanding how robots might find a sustainable role in everyday family settings. Together with each of the ten families, we co-designed robot interactions and behaviors, and a plan for the robot to support their chosen routines, accounting for contextual factors such as timing, participants, locations, and the activities in the environment. We then designed, prototyped, and deployed a mobile social robot as a four-day, in-home user study. Families welcomed the robot's reminders, with parents especially appreciating the offloading of some reminding tasks. At the same time, interviews revealed tensions around timing, authority, and family dynamics, highlighting the complexity of integrating robots into households beyond the immediate task of reminders. Based on these insights, we offer design implications for robot-facilitated contextual reminders and discuss broader considerations for designing robots for family settings.",
          "authors": [
            "Michael F. Xu",
            "Enhui Zhao",
            "Yawen Zhang",
            "Joseph E. Michaelis",
            "Sarah Sebo",
            "Bilge Mutlu"
          ],
          "published": "2026-02-26T05:03:17Z",
          "updated": "2026-02-26T05:03:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22628v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22579v1",
          "title": "Metamorphic Testing of Vision-Language Action-Enabled Robots",
          "summary": "Vision-Language-Action (VLA) models are multimodal robotic task controllers that, given an instruction and visual inputs, produce a sequence of low-level control actions (or motor commands) enabling a robot to execute the requested task in the physical environment. These systems face the test oracle problem from multiple perspectives. On the one hand, a test oracle must be defined for each instruction prompt, which is a complex and non-generalizable approach. On the other hand, current state-of-the-art oracles typically capture symbolic representations of the world (e.g., robot and object states), enabling the correctness evaluation of a task, but fail to assess other critical aspects, such as the quality with which VLA-enabled robots perform a task. In this paper, we explore whether Metamorphic Testing (MT) can alleviate the test oracle problem in this context. To do so, we propose two metamorphic relation patterns and five metamorphic relations to assess whether changes to the test inputs impact the original trajectory of the VLA-enabled robots. An empirical study involving five VLA models, two simulated robots, and four robotic tasks shows that MT can effectively alleviate the test oracle problem by automatically detecting diverse types of failures, including, but not limited to, uncompleted tasks. More importantly, the proposed MRs are generalizable, making the proposed approach applicable across different VLA models, robots, and tasks, even in the absence of test oracles.",
          "authors": [
            "Pablo Valle",
            "Sergio Segura",
            "Shaukat Ali",
            "Aitor Arrieta"
          ],
          "published": "2026-02-26T03:32:43Z",
          "updated": "2026-02-26T03:32:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.SE"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22579v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22542v1",
          "title": "Relational Appliances: A Robot in the Refrigerator for Home-Based Health Promotion",
          "summary": "Kitchen appliances are frequently used domestic artifacts situated at the point of everyday dietary decision making, making them a promising but underexplored site for health promotion. We explore the concept of relational appliances: everyday household devices designed as embodied social actors that engage users through ongoing, personalized interaction. We focus on the refrigerator, whose unique affordances, including a fixed, sensor-rich environment, private interaction space, and close coupling to food items, support contextualized, conversational engagement during snack choices. We present an initial exploration of this concept through a pilot study deploying an anthropomorphic robotic head inside a household refrigerator. In a home-lab apartment, participants repeatedly retrieved snacks during simulated TV \"commercial breaks\" while interacting with a human-sized robotic head. Participants were randomized to either a health-promotion condition, in which the robot made healthy snack recommendations, or a social-chat control condition. Outcomes included compliance with recommendations, nutritional quality of selected snacks, and psychosocial measures related to acceptance of the robot. Results suggest that participants found the robot persuasive, socially engaging, and increasingly natural over time, often describing it as helpful, aware, and companionable. Most participants reported greater awareness of their snack decisions and expressed interest in having such a robot in their own home. We discuss implications for designing relational appliances that leverage anthropomorphism, trust, and long-term human-technology relationships for home-based health promotion.",
          "authors": [
            "Timothy Bickmore",
            "Mehdi Arjmand",
            "Yunus Terzioglu"
          ],
          "published": "2026-02-26T02:30:49Z",
          "updated": "2026-02-26T02:30:49Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22542v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22514v1",
          "title": "SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation",
          "summary": "We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction. In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction. Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.",
          "authors": [
            "Xinyu Tan",
            "Ningwei Bai",
            "Harry Gardener",
            "Zhengyang Zhong",
            "Luoyu Zhang",
            "Liuhaichen Yang",
            "Zhekai Duan",
            "Monkgogi Galeitsiwe",
            "Zezhi Tang"
          ],
          "published": "2026-02-26T01:16:27Z",
          "updated": "2026-02-26T01:16:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22514v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22474v1",
          "title": "When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering",
          "summary": "Policy steering is an emerging way to adapt robot behaviors at deployment-time: a learned verifier analyzes low-level action samples proposed by a pre-trained policy (e.g., diffusion policy) and selects only those aligned with the task. While Vision-Language Models (VLMs) are promising general-purpose verifiers due to their reasoning capabilities, existing frameworks often assume these models are well-calibrated. In practice, the overconfident judgment from VLM can degrade the steering performance under both high-level semantic uncertainty in task specifications and low-level action uncertainty or incapability of the pre-trained policy. We propose uncertainty-aware policy steering (UPS), a framework that jointly reasons about semantic task uncertainty and low-level action feasibility, and selects an uncertainty resolution strategy: execute a high-confidence action, clarify task ambiguity via natural language queries, or ask for action interventions to correct the low-level policy when it is deemed incapable at the task. We leverage conformal prediction to calibrate the composition of the VLM and the pre-trained base policy, providing statistical assurances that the verifier selects the correct strategy. After collecting interventions during deployment, we employ residual learning to improve the capability of the pre-trained policy, enabling the system to learn continually but with minimal expensive human feedback. We demonstrate our framework through experiments in simulation and on hardware, showing that UPS can disentangle confident, ambiguous, and incapable scenarios and minimizes expensive user interventions compared to uncalibrated baselines and prior human- or robot-gated continual learning approaches. Videos can be found at https://jessie-yuan.github.io/ups/",
          "authors": [
            "Jessie Yuan",
            "Yilin Wu",
            "Andrea Bajcsy"
          ],
          "published": "2026-02-25T23:23:22Z",
          "updated": "2026-02-25T23:23:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22474v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22461v1",
          "title": "EgoAVFlow: Robot Policy Learning with Active Vision from Human Egocentric Videos via 3D Flow",
          "summary": "Egocentric human videos provide a scalable source of manipulation demonstrations; however, deploying them on robots requires active viewpoint control to maintain task-critical visibility, which human viewpoint imitation often fails to provide due to human-specific priors. We propose EgoAVFlow, which learns manipulation and active vision from egocentric videos through a shared 3D flow representation that supports geometric visibility reasoning and transfers without robot demonstrations. EgoAVFlow uses diffusion models to predict robot actions, future 3D flow, and camera trajectories, and refines viewpoints at test time with reward-maximizing denoising under a visibility-aware reward computed from predicted motion and scene geometry. Real-world experiments under actively changing viewpoints show that EgoAVFlow consistently outperforms prior human-demo-based baselines, demonstrating effective visibility maintenance and robust manipulation without robot demonstrations.",
          "authors": [
            "Daesol Cho",
            "Youngseok Jang",
            "Danfei Xu",
            "Sehoon Ha"
          ],
          "published": "2026-02-25T22:50:51Z",
          "updated": "2026-02-25T22:50:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22461v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22459v1",
          "title": "Hierarchical Trajectory Planning of Floating-Base Multi-Link Robot for Maneuvering in Confined Environments",
          "summary": "Floating-base multi-link robots can change their shape during flight, making them well-suited for applications in confined environments such as autonomous inspection and search and rescue. However, trajectory planning for such systems remains an open challenge because the problem lies in a high-dimensional, constraint-rich space where collision avoidance must be addressed together with kinematic limits and dynamic feasibility. This work introduces a hierarchical trajectory planning framework that integrates global guidance with configuration-aware local optimization. First, we exploit the dual nature of these robots - the root link as a rigid body for guidance and the articulated joints for flexibility - to generate global anchor states that decompose the planning problem into tractable segments. Second, we design a local trajectory planner that optimizes each segment in parallel with differentiable objectives and constraints, systematically enforcing kinematic feasibility and maintaining dynamic feasibility by avoiding control singularities. Third, we implement a complete system that directly processes point-cloud data, eliminating the need for handcrafted obstacle models. Extensive simulations and real-world experiments confirm that this framework enables an articulated aerial robot to exploit its morphology for maneuvering that rigid robots cannot achieve. To the best of our knowledge, this is the first planning framework for floating-base multi-link robots that has been demonstrated on a real robot to generate continuous, collision-free, and dynamically feasible trajectories directly from raw point-cloud inputs, without relying on handcrafted obstacle models.",
          "authors": [
            "Yicheng Chen",
            "Jinjie Li",
            "Haokun Liu",
            "Zicheng Luo",
            "Kotaro Kaneko",
            "Moju Zhao"
          ],
          "published": "2026-02-25T22:49:54Z",
          "updated": "2026-02-25T22:49:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22459v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22452v1",
          "title": "CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines",
          "summary": "A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.",
          "authors": [
            "Chayan Banerjee"
          ],
          "published": "2026-02-25T22:27:30Z",
          "updated": "2026-02-25T22:27:30Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22452v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22346v1",
          "title": "Detection and Recognition: A Pairwise Interaction Framework for Mobile Service Robots",
          "summary": "Autonomous mobile service robots, like lawnmowers or cleaning robots, operating in human-populated environments need to reason about local human-human interactions to support safe and socially aware navigation while fulfilling their tasks. For such robots, interaction understanding is not primarily a fine-grained recognition problem, but a perception problem under limited sensing quality and computational resources. Many existing approaches focus on holistic group activity recognition, which often requires complex and large models which may not be necessary for mobile service robots. Others use pairwise interaction methods which commonly rely on skeletal representations but their use in outdoor environments remains challenging. In this work, we argue that pairwise human interaction constitute a minimal yet sufficient perceptual unit for robot-centric social understanding. We study the problem of identifying interacting person pairs and classifying coarse-grained interaction behaviors sufficient for downstream group-level reasoning and service robot decision-making. To this end, we adopt a two-stage framework in which candidate interacting pairs are first identified based on lightweight geometric and motion cues, and interaction types are subsequently classified using a relation network. We evaluate the proposed approach on the JRDB dataset, where it achieves sufficient accuracy with reduced computational cost and model size compared to appearance-based methods. Additional experiments on the Collective Activity Dataset and zero shot test on a lawnmower-collected dataset further illustrate the generality of the proposed framework. These results suggest that pairwise geometric and motion cues provide a practical basis for interaction perception on mobile service robot providing a promising method for integration into mobile robot navigation stacks in future work. Code will be released soon",
          "authors": [
            "Mengyu Liang",
            "Sarah Gillet Schlegel",
            "Iolanda Leite"
          ],
          "published": "2026-02-25T19:12:07Z",
          "updated": "2026-02-25T19:12:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22346v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22154v1",
          "title": "Position-Based Flocking for Persistent Alignment without Velocity Sensing",
          "summary": "Coordinated collective motion in bird flocks and fish schools inspires algorithms for cohesive swarm robotics. This paper presents a position-based flocking model that achieves persistent velocity alignment without velocity sensing. By approximating relative velocity differences from changes between current and initial relative positions and incorporating a time- and density-dependent alignment gain with a non-zero minimum threshold to maintain persistent alignment, the model sustains coherent collective motion over extended periods. Simulations with a collective of 50 agents demonstrate that the position-based flocking model attains faster and more sustained directional alignment and results in more compact formations than a velocity-alignment-based baseline. This position-based flocking model is particularly well-suited for real-world robotic swarms, where velocity measurements are unreliable, noisy, or unavailable. Experimental results using a team of nine real wheeled mobile robots are also presented.",
          "authors": [
            "Hossein B. Jond",
            "Veli Bakırcıoğlu",
            "Logan E. Beaver",
            "Nejat Tükenmez",
            "Adel Akbarimajd",
            "Martin Saska"
          ],
          "published": "2026-02-25T18:01:00Z",
          "updated": "2026-02-25T18:01:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22154v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22118v1",
          "title": "System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot",
          "summary": "Trials cyclists and mountain bike riders can hop, jump, balance, and drive on one or both wheels. This versatility allows them to achieve speed and energy-efficiency on smooth terrain and agility over rough terrain. Inspired by these athletes, we present the design and control of a robotic platform, Ultra Mobility Vehicle (UMV), which combines a bicycle and a reaction mass to move dynamically with minimal actuated degrees of freedom. We employ a simulation-driven design optimization process to synthesize a spatial linkage topology with a focus on vertical jump height and momentum-based balancing on a single wheel contact. Using a constrained Reinforcement Learning (RL) framework, we demonstrate zero-shot transfer of diverse athletic behaviors, including track-stands, jumps, wheelies, rear wheel hopping, and front flips. This 23.5 kg robot is capable of high speeds (8 m/s) and jumping on and over large obstacles (1 m tall, or 130% of the robot's nominal height).",
          "authors": [
            "Benjamin Bokser",
            "Daniel Gonzalez",
            "Surya Singh",
            "Aaron Preston",
            "Alex Bahner",
            "Annika Wollschläger",
            "Arianna Ilvonen",
            "Asa Eckert-Erdheim",
            "Ashwin Khadke",
            "Bilal Hammoud",
            "Dean Molinaro",
            "Fabian Jenelten",
            "Henry Mayne",
            "Howie Choset",
            "Igor Bogoslavskyi",
            "Itic Tinman",
            "James Tigue",
            "Jan Preisig",
            "Kaiyu Zheng",
            "Kenny Sharma",
            "Kim Ang",
            "Laura Lee",
            "Liana Margolese",
            "Nicole Lin",
            "Oscar Frias",
            "Paul Drews",
            "Ravi Boggavarapu",
            "Rick Burnham",
            "Samuel Zapolsky",
            "Sangbae Kim",
            "Scott Biddlestone",
            "Sean Mayorga",
            "Shamel Fahmi",
            "Tyler McCollum",
            "Velin Dimitrov",
            "William Moyne",
            "Yu-Ming Chen",
            "Farbod Farshidian",
            "Marco Hutter",
            "David Perry",
            "Al Rizzi",
            "Gabe Nelson"
          ],
          "published": "2026-02-25T17:05:09Z",
          "updated": "2026-02-25T17:05:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22118v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22100v1",
          "title": "Behavioral Cloning for Robotic Connector Assembly: An Empirical Study",
          "summary": "Automating the assembly of wire harnesses is challenging in automotive, electrical cabinet, and aircraft production, particularly due to deformable cables and a high variance in connector geometries. In addition, connectors must be inserted with limited force to avoid damage, while their poses can vary significantly. While humans can do this task intuitively by combining visual and haptic feedback, programming an industrial robot for such a task in an adaptable manner remains difficult. This work presents an empirical study investigating the suitability of behavioral cloning for learning an action prediction model for connector insertion that fuses force-torque sensing with a fixed position camera. We compare several network architectures and other design choices using a dataset of up to 300 successful human demonstrations collected via teleoperation of a UR5e robot with a SpaceMouse under varying connector poses. The resulting system is then evaluated against five different connector geometries under varying connector poses, achieving an overall insertion success rate of over 90 %.",
          "authors": [
            "Andreas Kernbach",
            "Daniel Bargmann",
            "Werner Kraus",
            "Marco F. Huber"
          ],
          "published": "2026-02-25T16:47:08Z",
          "updated": "2026-02-25T16:47:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22100v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22088v1",
          "title": "Force Policy: Learning Hybrid Force-Position Control Policy under Interaction Frame for Contact-Rich Manipulation",
          "summary": "Contact-rich manipulation demands human-like integration of perception and force feedback: vision should guide task progress, while high-frequency interaction control must stabilize contact under uncertainty. Existing learning-based policies often entangle these roles in a monolithic network, trading off global generalization against stable local refinement, while control-centric approaches typically assume a known task structure or learn only controller parameters rather than the structure itself. In this paper, we formalize a physically grounded interaction frame, an instantaneous local basis that decouples force regulation from motion execution, and propose a method to recover it from demonstrations. Based on this, we address both issues by proposing Force Policy, a global-local vision-force policy in which a global policy guides free-space actions using vision, and upon contact, a high-frequency local policy with force feedback estimates the interaction frame and executes hybrid force-position control for stable interaction. Real-world experiments across diverse contact-rich tasks show consistent gains over strong baselines, with more robust contact establishment, more accurate force regulation, and reliable generalization to novel objects with varied geometries and physical properties, ultimately improving both contact stability and execution quality. Project page: https://force-policy.github.io/",
          "authors": [
            "Hongjie Fang",
            "Shirun Tang",
            "Mingyu Mei",
            "Haoxiang Qin",
            "Zihao He",
            "Jingjing Chen",
            "Ying Feng",
            "Chenxi Wang",
            "Wanxi Liu",
            "Zaixing He",
            "Cewu Lu",
            "Shiquan Wang"
          ],
          "published": "2026-02-25T16:35:24Z",
          "updated": "2026-02-25T16:35:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22088v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22056v1",
          "title": "FlowCorrect: Efficient Interactive Correction of Generative Flow Policies for Robotic Manipulation",
          "summary": "Generative manipulation policies can fail catastrophically under deployment-time distribution shift, yet many failures are near-misses: the robot reaches almost-correct poses and would succeed with a small corrective motion. We present FlowCorrect, a deployment-time correction framework that converts near-miss failures into successes using sparse human nudges, without full policy retraining. During execution, a human provides brief corrective pose nudges via a lightweight VR interface. FlowCorrect uses these sparse corrections to locally adapt the policy, improving actions without retraining the backbone while preserving the model performance on previously learned scenarios. We evaluate on a real-world robot across three tabletop tasks: pick-and-place, pouring, and cup uprighting. With a low correction budget, FlowCorrect improves success on hard cases by 85\\% while preserving performance on previously solved scenarios. The results demonstrate clearly that FlowCorrect learns only with very few demonstrations and enables fast and sample-efficient incremental, human-in-the-loop corrections of generative visuomotor policies at deployment time in real-world robotics.",
          "authors": [
            "Edgar Welte",
            "Yitian Shi",
            "Rosa Wolf",
            "Maximillian Gilles",
            "Rania Rayyes"
          ],
          "published": "2026-02-25T16:06:49Z",
          "updated": "2026-02-25T16:06:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22056v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22010v1",
          "title": "World Guidance: World Modeling in Condition Space for Action Generation",
          "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
          "authors": [
            "Yue Su",
            "Sijin Chen",
            "Haixin Shi",
            "Mingyu Liu",
            "Zhengshen Zhang",
            "Ningyuan Huang",
            "Weiheng Zhong",
            "Zhengbang Zhu",
            "Yuxiao Liu",
            "Xihui Liu"
          ],
          "published": "2026-02-25T15:27:09Z",
          "updated": "2026-02-25T15:27:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22010v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22006v1",
          "title": "Parallel Continuous-Time Relative Localization with Augmented Clamped Non-Uniform B-Splines",
          "summary": "Accurate relative localization is critical for multi-robot cooperation. In robot swarms, measurements from different robots arrive asynchronously and with clock time-offsets. Although Continuous-Time (CT) formulations have proved effective for handling asynchronous measurements in single-robot SLAM and calibration, extending CT methods to multi-robot settings faces great challenges to achieve high-accuracy, low-latency, and high-frequency performance. Especially, existing CT methods suffer from the inherent query-time delay of unclamped B-splines and high computational cost. This paper proposes CT-RIO, a novel Continuous-Time Relative-Inertial Odometry framework. We employ Clamped Non-Uniform B-splines (C-NUBS) to represent robot states for the first time, eliminating the query-time delay. We further augment C-NUBS with closed-form extension and shrinkage operations that preserve the spline shape, making it suitable for online estimation and enabling flexible knot management. This flexibility leads to the concept of knot-keyknot strategy, which supports spline extension at high-frequency while retaining sparse keyknots for adaptive relative-motion modeling. We then formulate a sliding-window relative localization problem that operates purely on relative kinematics and inter-robot constraints. To meet the demanding computation required at swarm scale, we decompose the tightly-coupled optimization into robot-wise sub-problems and solve them in parallel using incremental asynchronous block coordinate descent. Extensive experiments show that CT-RIO converges from time-offsets as large as 263 ms to sub-millisecond within 3 s, and achieves RMSEs of 0.046 m and 1.8 °. It consistently outperforms state-of-the-art methods, with improvements of up to 60% under high-speed motion.",
          "authors": [
            "Jiadong Lu",
            "Zhehan Li",
            "Tao Han",
            "Miao Xu",
            "Chao Xu",
            "Yanjun Cao"
          ],
          "published": "2026-02-25T15:23:18Z",
          "updated": "2026-02-25T15:23:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22006v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22001v1",
          "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
          "summary": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology.",
          "authors": [
            "Freek Stulp",
            "Samuel Bustamante",
            "João Silvério",
            "Alin Albu-Schäffer",
            "Jeannette Bohg",
            "Shuran Song"
          ],
          "published": "2026-02-25T15:19:44Z",
          "updated": "2026-02-25T15:19:44Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22001v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21983v1",
          "title": "Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots",
          "summary": "Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions.",
          "authors": [
            "Jingchao Wei",
            "Jingkai Qin",
            "Yuxiao Cao",
            "Jingcheng Huang",
            "Xiangrui Zeng",
            "Min Li",
            "Zhouping Yin"
          ],
          "published": "2026-02-25T15:02:11Z",
          "updated": "2026-02-25T15:02:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21983v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21967v1",
          "title": "Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments",
          "summary": "In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance.",
          "authors": [
            "Xiangqi Meng",
            "Pengxu Hou",
            "Zhenjun Zhao",
            "Javier Civera",
            "Daniel Cremers",
            "Hesheng Wang",
            "Haoang Li"
          ],
          "published": "2026-02-25T14:48:49Z",
          "updated": "2026-02-25T14:48:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21967v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21954v1",
          "title": "The Swarm Intelligence Freeway-Urban Trajectories (SWIFTraj) Dataset - Part II: A Graph-Based Approach for Trajectory Connection",
          "summary": "In Part I of this companion paper series, we introduced SWIFTraj, a new open-source vehicle trajectory dataset collected using a unmanned aerial vehicle (UAV) swarm. The dataset has two distinctive features. First, by connecting trajectories across consecutive UAV videos, it provides long-distance continuous trajectories, with the longest exceeding 4.5 km. Second, it covers an integrated traffic network consisting of both freeways and their connected urban roads. Obtaining such long-distance continuous trajectories from a UAV swarm is challenging, due to the need for accurate time alignment across multiple videos and the irregular spatial distribution of UAVs. To address these challenges, this paper proposes a novel graph-based approach for connecting vehicle trajectories captured by a UAV swarm. An undirected graph is constructed to represent flexible UAV layouts, and an automatic time alignment method based on trajectory matching cost minimization is developed to estimate optimal time offsets across videos. To associate trajectories of the same vehicle observed in different videos, a vehicle matching table is established using the Hungarian algorithm. The proposed approach is evaluated using both simulated and real-world data. Results from real-world experiments show that the time alignment error is within three video frames, corresponding to approximately 0.1 s, and that the vehicle matching achieves an F1-score of about 0.99. These results demonstrate the effectiveness of the proposed method in addressing key challenges in UAV-based trajectory connection and highlight its potential for large-scale vehicle trajectory collection.",
          "authors": [
            "Xinkai Ji",
            "Pan Liu",
            "Yu Han"
          ],
          "published": "2026-02-25T14:35:24Z",
          "updated": "2026-02-25T14:35:24Z",
          "primary_category": "physics.soc-ph",
          "categories": [
            "physics.soc-ph",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21954v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21904v1",
          "title": "UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing",
          "summary": "Accurate cone localization in 3D space is essential in autonomous racing for precise navigation around the track. Approaches that rely on traditional computer vision algorithms are sensitive to environmental variations, and neural networks are often trained on limited data and are infeasible to run in real time. We present a UNet-based neural network for keypoint detection on cones, leveraging the largest custom-labeled dataset we have assembled. Our approach enables accurate cone position estimation and the potential for color prediction. Our model achieves substantial improvements in keypoint accuracy over conventional methods. Furthermore, we leverage our predicted keypoints in the perception pipeline and evaluate the end-to-end autonomous system. Our results show high-quality performance across all metrics, highlighting the effectiveness of this approach and its potential for adoption in competitive autonomous racing systems.",
          "authors": [
            "Mariia Baidachna",
            "James Carty",
            "Aidan Ferguson",
            "Joseph Agrane",
            "Varad Kulkarni",
            "Aubrey Agub",
            "Michael Baxendale",
            "Aaron David",
            "Rachel Horton",
            "Elliott Atkinson"
          ],
          "published": "2026-02-25T13:34:56Z",
          "updated": "2026-02-25T13:34:56Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21904v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21899v1",
          "title": "Enhancing Cellular-enabled Collaborative Robots Planning through GNSS data for SAR Scenarios",
          "summary": "Cellular-enabled collaborative robots are becoming paramount in Search-and-Rescue (SAR) and emergency response. Crucially dependent on resilient mobile network connectivity, they serve as invaluable assets for tasks like rapid victim localization and the exploration of hazardous, otherwise unreachable areas. However, their reliance on battery power and the need for persistent, low-latency communication limit operational time and mobility. To address this, and considering the evolving capabilities of 5G/6G networks, we propose a novel SAR framework that includes Mission Planning and Mission Execution phases and that optimizes robot deployment. By considering parameters such as the exploration area size, terrain elevation, robot fleet size, communication-influenced energy profiles, desired exploration rate, and target response time, our framework determines the minimum number of robots required and their optimal paths to ensure effective coverage and timely data backhaul over mobile networks. Our results demonstrate the trade-offs between number of robots, explored area, and response time for wheeled and quadruped robots. Further, we quantify the impact of terrain elevation data on mission time and energy consumption, showing the benefits of incorporating real-world environmental factors that might also affect mobile signal propagation and connectivity into SAR planning. This framework provides critical insights for leveraging next-generation mobile networks to enhance autonomous SAR operations.",
          "authors": [
            "Arnau Romero",
            "Carmen Delgado",
            "Jana Baguer",
            "Raúl Suárez",
            "Xavier Costa-Pérez"
          ],
          "published": "2026-02-25T13:29:38Z",
          "updated": "2026-02-25T13:29:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.NI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21899v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21816v1",
          "title": "Self-Curriculum Model-based Reinforcement Learning for Shape Control of Deformable Linear Objects",
          "summary": "Precise shape control of Deformable Linear Objects (DLOs) is crucial in robotic applications such as industrial and medical fields. However, existing methods face challenges in handling complex large deformation tasks, especially those involving opposite curvatures, and lack efficiency and precision. To address this, we propose a two-stage framework combining Reinforcement Learning (RL) and online visual servoing. In the large-deformation stage, a model-based reinforcement learning approach using an ensemble of dynamics models is introduced to significantly improve sample efficiency. Additionally, we design a self-curriculum goal generation mechanism that dynamically selects intermediate-difficulty goals with high diversity through imagined evaluations, thereby optimizing the policy learning process. In the small-deformation stage, a Jacobian-based visual servo controller is deployed to ensure high-precision convergence. Simulation results show that the proposed method enables efficient policy learning and significantly outperforms mainstream baselines in shape control success rate and precision. Furthermore, the framework effectively transfers the policy trained in simulation to real-world tasks with zero-shot adaptation. It successfully completes all 30 cases with diverse initial and target shapes across DLOs of different sizes and materials. The project website is available at: https://anonymous.4open.science/w/sc-mbrl-dlo-EB48/",
          "authors": [
            "Zhaowei Liang",
            "Song Wang",
            "Zhao Jin",
            "Shirui Wu",
            "Dan Wu"
          ],
          "published": "2026-02-25T11:44:15Z",
          "updated": "2026-02-25T11:44:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21816v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21811v1",
          "title": "DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations",
          "summary": "Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.",
          "authors": [
            "Qingtao Liu",
            "Zhengnan Sun",
            "Yu Cui",
            "Haoming Li",
            "Gaofeng Li",
            "Lin Shao",
            "Jiming Chen",
            "Qi Ye"
          ],
          "published": "2026-02-25T11:38:07Z",
          "updated": "2026-02-25T11:38:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21811v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21783v1",
          "title": "Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control",
          "summary": "Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy.",
          "authors": [
            "Beatrice Luciani",
            "Alex van den Berg",
            "Matti Lang",
            "Alexandre L. Ratschat",
            "Laura Marchal-Crespo"
          ],
          "published": "2026-02-25T11:04:49Z",
          "updated": "2026-02-25T11:04:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21783v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21736v1",
          "title": "Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild",
          "summary": "Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.",
          "authors": [
            "Hao Luo",
            "Ye Wang",
            "Wanpeng Zhang",
            "Haoqi Yuan",
            "Yicheng Feng",
            "Haiweng Xu",
            "Sipeng Zheng",
            "Zongqing Lu"
          ],
          "published": "2026-02-25T09:46:42Z",
          "updated": "2026-02-25T09:46:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21736v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21723v1",
          "title": "LessMimic: Long-Horizon Humanoid Interaction with Unified Distance Field Representations",
          "summary": "Humanoid robots that autonomously interact with physical environments over extended horizons represent a central goal of embodied intelligence. Existing approaches rely on reference motions or task-specific rewards, tightly coupling policies to particular object geometries and precluding multi-skill generalization within a single framework. A unified interaction representation enabling reference-free inference, geometric generalization, and long-horizon skill composition within one policy remains an open challenge. Here we show that Distance Field (DF) provides such a representation: LessMimic conditions a single whole-body policy on DF-derived geometric cues--surface distances, gradients, and velocity decompositions--removing the need for motion references, with interaction latents encoded via a Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). Through DAgger-style distillation that aligns DF latents with egocentric depth features, LessMimic further transfers seamlessly to vision-only deployment without motion capture (MoCap) infrastructure. A single LessMimic policy achieves 80--100% success across object scales from 0.4x to 1.6x on PickUp and SitStand where baselines degrade sharply, attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. By grounding interaction in local geometry rather than demonstrations, LessMimic offers a scalable path toward humanoid robots that generalize, compose skills, and recover from failures in unstructured environments.",
          "authors": [
            "Yutang Lin",
            "Jieming Cui",
            "Yixuan Li",
            "Baoxiong Jia",
            "Yixin Zhu",
            "Siyuan Huang"
          ],
          "published": "2026-02-25T09:31:28Z",
          "updated": "2026-02-25T09:31:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21723v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21696v1",
          "title": "Dual-Regime Hybrid Aerodynamic Modeling of Winged Blimps With Neural Mixing",
          "summary": "Winged blimps operate across distinct aerodynamic regimes that cannot be adequately captured by a single model. At high speeds and small angles of attack, their dynamics exhibit strong coupling between lift and attitude, resembling fixed-wing aircraft behavior. At low speeds or large angles of attack, viscous effects and flow separation dominate, leading to drag-driven and damping-dominated dynamics. Accurately representing transitions between these regimes remains a fundamental challenge. This paper presents a hybrid aerodynamic modeling framework that integrates a fixed-wing Aerodynamic Coupling Model (ACM) and a Generalized Drag Model (GDM) using a learned neural network mixer with explicit physics-based regularization. The mixer enables smooth transitions between regimes while retaining explicit, physics-based aerodynamic representation. Model parameters are identified through a structured three-phase pipeline tailored for hybrid aerodynamic modeling. The proposed approach is validated on the RGBlimp platform through a large-scale experimental campaign comprising 1,320 real-world flight trajectories across 330 thruster and moving mass configurations, spanning a wide range of speeds and angles of attack. Experimental results demonstrate that the proposed hybrid model consistently outperforms single-model and predefined-mixer baselines, establishing a practical and robust aerodynamic modeling solution for winged blimps.",
          "authors": [
            "Xiaorui Wang",
            "Hongwu Wang",
            "Yue Fan",
            "Hao Cheng",
            "Feitian Zhang"
          ],
          "published": "2026-02-25T08:59:50Z",
          "updated": "2026-02-25T08:59:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21696v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21691v1",
          "title": "Trajectory Generation with Endpoint Regulation and Momentum-Aware Dynamics for Visually Impaired Scenarios",
          "summary": "Trajectory generation for visually impaired scenarios requires smooth and temporally consistent state in structured, low-speed dynamic environments. However, traditional jerk-based heuristic trajectory sampling with independent segment generation and conventional smoothness penalties often lead to unstable terminal behavior and state discontinuities under frequent regenerating. This paper proposes a trajectory generation approach that integrates endpoint regulation to stabilize terminal states within each segment and momentum-aware dynamics to regularize the evolution of velocity and acceleration for segment consistency. Endpoint regulation is incorporated into trajectory sampling to stabilize terminal behavior, while a momentum-aware dynamics enforces consistent velocity and acceleration evolution across consecutive trajectory segments. Experimental results demonstrate reduced acceleration peaks and lower jerk levels with decreased dispersion, smoother velocity and acceleration profiles, more stable endpoint distributions, and fewer infeasible trajectory candidates compared with a baseline planner.",
          "authors": [
            "Yuting Zeng",
            "Manping Fan",
            "You Zhou",
            "Yongbin Yu",
            "Zhiwen Zheng",
            "Jingtao Zhang",
            "Liyong Ren",
            "Zhenglin Yang"
          ],
          "published": "2026-02-25T08:47:08Z",
          "updated": "2026-02-25T08:47:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21691v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21684v1",
          "title": "Primary-Fine Decoupling for Action Generation in Robotic Imitation",
          "summary": "Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.",
          "authors": [
            "Xiaohan Lei",
            "Min Wang",
            "Wengang Zhou",
            "Xingyu Lu",
            "Houqiang Li"
          ],
          "published": "2026-02-25T08:36:45Z",
          "updated": "2026-02-25T08:36:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21684v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21682v1",
          "title": "SunnyParking: Multi-Shot Trajectory Generation and Motion State Awareness for Human-like Parking",
          "summary": "Autonomous parking fundamentally differs from on-road driving due to its frequent direction changes and complex maneuvering requirements. However, existing End-to-End (E2E) planning methods often simplify the parking task into a geometric path regression problem, neglecting explicit modeling of the vehicle's kinematic state. This \"dimensionality deficiency\" easily leads to physically infeasible trajectories and deviates from real human driving behavior, particularly at critical gear-shift points in multi-shot parking scenarios. In this paper, we propose SunnyParking, a novel dual-branch E2E architecture that achieves motion state awareness by jointly predicting spatial trajectories and discrete motion state sequences (e.g., forward/reverse). Additionally, we introduce a Fourier feature-based representation of target parking slots to overcome the resolution limitations of traditional bird's-eye view (BEV) approaches, enabling high-precision target interactions. Experimental results demonstrate that our framework generates more robust and human-like trajectories in complex multi-shot parking scenarios, while significantly improving gear-shift point localization accuracy compared to state-of-the-art methods. We open-source a new parking dataset of the CARLA simulator, specifically designed to evaluate full prediction capabilities under complex maneuvers.",
          "authors": [
            "Jishu Miao",
            "Han Chen",
            "Jiankun Zhai",
            "Qi Liu",
            "Tsubasa Hirakawa",
            "Takayoshi Yamashita",
            "Hironobu Fujiyoshi"
          ],
          "published": "2026-02-25T08:35:58Z",
          "updated": "2026-02-25T08:35:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21682v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21670v1",
          "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
          "summary": "Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate.",
          "authors": [
            "Tomoya Kawabe",
            "Rin Takano"
          ],
          "published": "2026-02-25T08:08:26Z",
          "updated": "2026-02-25T08:08:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21670v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21670v2",
          "title": "Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning",
          "summary": "Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate.",
          "authors": [
            "Tomoya Kawabe",
            "Rin Takano"
          ],
          "published": "2026-02-25T08:08:26Z",
          "updated": "2026-02-26T02:28:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21670v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21666v1",
          "title": "Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits",
          "summary": "It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper.",
          "authors": [
            "Luying Feng",
            "Yaochu Jin",
            "Hanze Hu",
            "Wei Chen"
          ],
          "published": "2026-02-25T07:59:16Z",
          "updated": "2026-02-25T07:59:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21666v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21644v1",
          "title": "DAGS-SLAM: Dynamic-Aware 3DGS SLAM via Spatiotemporal Motion Probability and Uncertainty-Aware Scheduling",
          "summary": "Mobile robots and IoT devices demand real-time localization and dense reconstruction under tight compute and energy budgets. While 3D Gaussian Splatting (3DGS) enables efficient dense SLAM, dynamic objects and occlusions still degrade tracking and mapping. Existing dynamic 3DGS-SLAM often relies on heavy optical flow and per-frame segmentation, which is costly for mobile deployment and brittle under challenging illumination. We present DAGS-SLAM, a dynamic-aware 3DGS-SLAM system that maintains a spatiotemporal motion probability (MP) state per Gaussian and triggers semantics on demand via an uncertainty-aware scheduler. DAGS-SLAM fuses lightweight YOLO instance priors with geometric cues to estimate and temporally update MP, propagates MP to the front-end for dynamic-aware correspondence selection, and suppresses dynamic artifacts in the back-end via MP-guided optimization. Experiments on public dynamic RGB-D benchmarks show improved reconstruction and robust tracking while sustaining real-time throughput on a commodity GPU, demonstrating a practical speed-accuracy tradeoff with reduced semantic invocations toward mobile deployment.",
          "authors": [
            "Li Zhang",
            "Yu-An Liu",
            "Xijia Jiang",
            "Conghao Huang",
            "Danyang Li",
            "Yanyong Zhang"
          ],
          "published": "2026-02-25T07:14:07Z",
          "updated": "2026-02-25T07:14:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21644v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21633v1",
          "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
          "summary": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.",
          "authors": [
            "Chenyv Liu",
            "Wentao Tan",
            "Lei Zhu",
            "Fengling Li",
            "Jingjing Li",
            "Guoli Yang",
            "Heng Tao Shen"
          ],
          "published": "2026-02-25T06:58:06Z",
          "updated": "2026-02-25T06:58:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21633v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21625v1",
          "title": "Tacmap: Bridging the Tactile Sim-to-Real Gap via Geometry-Consistent Penetration Depth Map",
          "summary": "Vision-Based Tactile Sensors (VBTS) are essential for achieving dexterous robotic manipulation, yet the tactile sim-to-real gap remains a fundamental bottleneck. Current tactile simulations suffer from a persistent dilemma: simplified geometric projections lack physical authenticity, while high-fidelity Finite Element Methods (FEM) are too computationally prohibitive for large-scale reinforcement learning. In this work, we present Tacmap, a high-fidelity, computationally efficient tactile simulation framework anchored in volumetric penetration depth. Our key insight is to bridge the tactile sim-to-real gap by unifying both domains through a shared deform map representation. Specifically, we compute 3D intersection volumes as depth maps in simulation, while in the real world, we employ an automated data-collection rig to learn a robust mapping from raw tactile images to ground-truth depth maps. By aligning simulation and real-world in this unified geometric space, Tacmap minimizes domain shift while maintaining physical consistency. Quantitative evaluations across diverse contact scenarios demonstrate that Tacmap's deform maps closely mirror real-world measurements. Moreover, we validate the utility of Tacmap through an in-hand rotation task, where a policy trained exclusively in simulation achieves zero-shot transfer to a physical robot.",
          "authors": [
            "Lei Su",
            "Zhijie Peng",
            "Renyuan Ren",
            "Shengping Mao",
            "Juan Du",
            "Kaifeng Zhang",
            "Xuezhou Zhu"
          ],
          "published": "2026-02-25T06:40:59Z",
          "updated": "2026-02-25T06:40:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21625v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21622v1",
          "title": "ADM-DP: Adaptive Dynamic Modality Diffusion Policy through Vision-Tactile-Graph Fusion for Multi-Agent Manipulation",
          "summary": "Multi-agent robotic manipulation remains challenging due to the combined demands of coordination, grasp stability, and collision avoidance in shared workspaces. To address these challenges, we propose the Adaptive Dynamic Modality Diffusion Policy (ADM-DP), a framework that integrates vision, tactile, and graph-based (multi-agent pose) modalities for coordinated control. ADM-DP introduces four key innovations. First, an enhanced visual encoder merges RGB and point-cloud features via Feature-wise Linear Modulation (FiLM) modulation to enrich perception. Second, a tactile-guided grasping strategy uses Force-Sensitive Resistor (FSR) feedback to detect insufficient contact and trigger corrective grasp refinement, improving grasp stability. Third, a graph-based collision encoder leverages shared tool center point (TCP) positions of multiple agents as structured kinematic context to maintain spatial awareness and reduce inter-agent interference. Fourth, an Adaptive Modality Attention Mechanism (AMAM) dynamically re-weights modalities according to task context, enabling flexible fusion. For scalability and modularity, a decoupled training paradigm is employed in which agents learn independent policies while sharing spatial information. This maintains low interdependence between agents while retaining collective awareness. Across seven multi-agent tasks, ADM-DP achieves 12-25% performance gains over state-of-the-art baselines. Ablation studies show the greatest improvements in tasks requiring multiple sensory modalities, validating our adaptive fusion strategy and demonstrating its robustness for diverse manipulation scenarios.",
          "authors": [
            "Enyi Wang",
            "Wen Fan",
            "Dandan Zhang"
          ],
          "published": "2026-02-25T06:35:19Z",
          "updated": "2026-02-25T06:35:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21622v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21612v1",
          "title": "Jumping Control for a Quadrupedal Wheeled-Legged Robot via NMPC and DE Optimization",
          "summary": "Quadrupedal wheeled-legged robots combine the advantages of legged and wheeled locomotion to achieve superior mobility, but executing dynamic jumps remains a significant challenge due to the additional degrees of freedom introduced by wheeled legs. This paper develops a mini-sized wheeled-legged robot for agile motion and presents a novel motion control framework that integrates the Nonlinear Model Predictive Control (NMPC) for locomotion and the Differential Evolution (DE) based trajectory optimization for jumping in quadrupedal wheeled-legged robots. The proposed controller utilizes wheel motion and locomotion to enhance jumping performance, achieving versatile maneuvers such as vertical jumping, forward jumping, and backflips. Extensive simulations and real-world experiments validate the effectiveness of the framework, demonstrating a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m.",
          "authors": [
            "Xuanqi Zeng",
            "Lingwei Zhang",
            "Linzhu Yue",
            "Zhitao Song",
            "Hongbo Zhang",
            "Tianlin Zhang",
            "Yun-Hui Liu"
          ],
          "published": "2026-02-25T06:13:59Z",
          "updated": "2026-02-25T06:13:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21612v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21599v1",
          "title": "Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control",
          "summary": "Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework.",
          "authors": [
            "Weisheng Xu",
            "Qiwei Wu",
            "Jiaxi Zhang",
            "Tan Jing",
            "Yangfan Li",
            "Yuetong Fang",
            "Jiaqi Xiong",
            "Kai Wu",
            "Rong Ou",
            "Renjing Xu"
          ],
          "published": "2026-02-25T05:52:37Z",
          "updated": "2026-02-25T05:52:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21599v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21595v1",
          "title": "SPOC: Safety-Aware Planning Under Partial Observability And Physical Constraints",
          "summary": "Embodied Task Planning with large language models faces safety challenges in real-world environments, where partial observability and physical constraints must be respected. Existing benchmarks often overlook these critical factors, limiting their ability to evaluate both feasibility and safety. We introduce SPOC, a benchmark for safety-aware embodied task planning, which integrates strict partial observability, physical constraints, step-by-step planning, and goal-condition-based evaluation. Covering diverse household hazards such as fire, fluid, injury, object damage, and pollution, SPOC enables rigorous assessment through both state and constraint-based online metrics. Experiments with state-of-the-art LLMs reveal that current models struggle to ensure safety-aware planning, particularly under implicit constraints. Code and dataset are available at https://github.com/khm159/SPOC",
          "authors": [
            "Hyungmin Kim",
            "Hobeom Jeon",
            "Dohyung Kim",
            "Minsu Jang",
            "Jeahong Kim"
          ],
          "published": "2026-02-25T05:44:21Z",
          "updated": "2026-02-25T05:44:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21595v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21583v1",
          "title": "Learning Agile and Robust Omnidirectional Aerial Motion on Overactuated Tiltable-Quadrotors",
          "summary": "Tilt-rotor aerial robots enable omnidirectional maneuvering through thrust vectoring, but introduce significant control challenges due to the strong coupling between joint and rotor dynamics. While model-based controllers can achieve high motion accuracy under nominal conditions, their robustness and responsiveness often degrade in the presence of disturbances and modeling uncertainties. This work investigates reinforcement learning for omnidirectional aerial motion control on over-actuated tiltable quadrotors that prioritizes robustness and agility. We present a learning-based control framework that enables efficient acquisition of coordinated rotor-joint behaviors for reaching target poses in the $SE(3)$ space. To achieve reliable sim-to-real transfer while preserving motion accuracy, we integrate system identification with minimal and physically consistent domain randomization. Compared with a state-of-the-art NMPC controller, the proposed method achieves comparable six-degree-of-freedom pose tracking accuracy, while demonstrating superior robustness and generalization across diverse tasks, enabling zero-shot deployment on real hardware.",
          "authors": [
            "Wentao Zhang",
            "Zhaoqi Ma",
            "Jinjie Li",
            "Huayi Wang",
            "Haokun Liu",
            "Junichiro Sugihara",
            "Chen Chen",
            "Yicheng Chen",
            "Moju Zhao"
          ],
          "published": "2026-02-25T05:13:25Z",
          "updated": "2026-02-25T05:13:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21583v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21531v1",
          "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies",
          "summary": "General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.",
          "authors": [
            "Yue Yang",
            "Shuo Cheng",
            "Yu Fang",
            "Homanga Bharadhwaj",
            "Mingyu Ding",
            "Gedas Bertasius",
            "Daniel Szafir"
          ],
          "published": "2026-02-25T03:33:39Z",
          "updated": "2026-02-25T03:33:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21531v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21450v1",
          "title": "Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups",
          "summary": "This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object's mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness.",
          "authors": [
            "Felipe Bartelt",
            "Vinicius M. Gonçalves",
            "Luciano C. A. Pimenta"
          ],
          "published": "2026-02-25T00:06:08Z",
          "updated": "2026-02-25T00:06:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21450v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21445v1",
          "title": "VLA Knows Its Limits",
          "summary": "Action chunking has recently emerged as a standard practice in flow-based Vision-Language-Action (VLA) models. However, the effect and choice of the execution horizon - the number of actions to be executed from each predicted chunk - remains underexplored. In this work, we first show that varying the execution horizon leads to substantial performance deviations, with performance initially improving and then declining as the horizon increases. To uncover the reasons, we analyze the cross- and self-attention weights in flow-based VLAs and reveal two key phenomena: (i) intra-chunk actions attend invariantly to vision-language tokens, limiting adaptability to environmental changes; and (ii) the initial and terminal action tokens serve as stable anchors, forming latent centers around which intermediate actions are organized. Motivated by these insights, we interpret action self-attention weights as a proxy for the model's predictive limit and propose AutoHorizon, the first test-time method that dynamically estimates the execution horizon for each predicted action chunk to adapt to changing perceptual conditions. Across simulated and real-world robotic manipulation tasks, AutoHorizon is performant, incurs negligible computational overhead, and generalizes across diverse tasks and flow-based models.",
          "authors": [
            "Haoxuan Wang",
            "Gengyu Zhang",
            "Yan Yan",
            "Ramana Rao Kompella",
            "Gaowen Liu"
          ],
          "published": "2026-02-24T23:48:48Z",
          "updated": "2026-02-24T23:48:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21445v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21418v1",
          "title": "Event-Driven On-Sensor Locomotion Mode Recognition Using a Shank-Mounted IMU with Embedded Machine Learning for Exoskeleton Control",
          "summary": "This work presents a wearable human activity recognition (HAR) system that performs real-time inference directly inside a shank-mounted inertial measurement unit (IMU) to support low-latency control of a lower-limb exoskeleton. Unlike conventional approaches that continuously stream raw inertial data to a microcontroller for classification, the proposed system executes activity recognition at the sensor level using the embedded Machine Learning Core (MLC) of the STMicroelectronics LSM6DSV16X IMU, allowing the host microcontroller to remain in a low-power state and read only the recognized activity label from IMU registers. While the system generalizes to multiple human activities, this paper focuses on three representative locomotion modes - stance, level walking, and stair ascent - using data collected from adult participants. A lightweight decision-tree model was configured and deployed for on-sensor execution using ST MEMS Studio, enabling continuous operation without custom machine learning code on the microcontroller. During operation, the IMU asserts an interrupt when motion or a new classification is detected; the microcontroller wakes, reads the MLC output registers, and forwards the inferred mode to the exoskeleton controller. This interrupt-driven, on-sensor inference architecture reduces computation and communication overhead while preserving battery energy and improving robustness in distinguishing level walking from stair ascent for torque-assist control.",
          "authors": [
            "Mohammadsaleh Razmi",
            "Iman Shojaei"
          ],
          "published": "2026-02-24T22:45:01Z",
          "updated": "2026-02-24T22:45:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21418v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21389v1",
          "title": "Autonomous Sea Turtle Robot for Marine Fieldwork",
          "summary": "Autonomous robots can transform how we observe marine ecosystems, but close-range operation in reefs and other cluttered habitats remains difficult. Vehicles must maneuver safely near animals and fragile structures while coping with currents, variable illumination and limited sensing. Previous approaches simplify these problems by leveraging soft materials and bioinspired swimming designs, but such platforms remain limited in terms of deployable autonomy. Here we present a sea turtle-inspired autonomous underwater robot that closed the gap between bioinspired locomotion and field-ready autonomy through a tightly integrated, vision-driven control stack. The robot combines robust depth-heading stabilization with obstacle avoidance and target-centric control, enabling it to track and interact with moving objects in complex terrain. We validate the robot in controlled pool experiments and in a live coral reef exhibit at the New England Aquarium, demonstrating stable operation and reliable tracking of fast-moving marine animals and human divers. To the best of our knowledge, this is the first integrated biomimetic robotic system, combining novel hardware, control, and field experiments, deployed to track and monitor real marine animals in their natural environment. During off-tether experiments, we demonstrate safe navigation around obstacles (91\\% success rate in the aquarium exhibit) and introduce a low-compute onboard tracking mode. Together, these results establish a practical route toward soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive ecosystems.",
          "authors": [
            "Zach J. Patterson",
            "Emily Sologuren",
            "Levi Cai",
            "Daniel Kim",
            "Alaa Maalouf",
            "Pascal Spino",
            "Daniela Rus"
          ],
          "published": "2026-02-24T21:36:19Z",
          "updated": "2026-02-24T21:36:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21389v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21366v1",
          "title": "Environment-Aware Learning of Smooth GNSS Covariance Dynamics for Autonomous Racing",
          "summary": "Ensuring accurate and stable state estimation is a challenging task crucial to safety-critical domains such as high-speed autonomous racing, where measurement uncertainty must be both adaptive to the environment and temporally smooth for control. In this work, we develop a learning-based framework, LACE, capable of directly modeling the temporal dynamics of GNSS measurement covariance. We model the covariance evolution as an exponentially stable dynamical system where a deep neural network (DNN) learns to predict the system's process noise from environmental features through an attention mechanism. By using contraction-based stability and systematically imposing spectral constraints, we formally provide guarantees of exponential stability and smoothness for the resulting covariance dynamics. We validate our approach on an AV-24 autonomous racecar, demonstrating improved localization performance and smoother covariance estimates in challenging, GNSS-degraded environments. Our results highlight the promise of dynamically modeling the perceived uncertainty in state estimation problems that are tightly coupled with control sensitivity.",
          "authors": [
            "Y. Deemo Chen",
            "Arion Zimmermann",
            "Thomas A. Berrueta",
            "Soon-Jo Chung"
          ],
          "published": "2026-02-24T20:59:09Z",
          "updated": "2026-02-24T20:59:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21366v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21331v1",
          "title": "CableRobotGraphSim: A Graph Neural Network for Modeling Partially Observable Cable-Driven Robot Dynamics",
          "summary": "General-purpose simulators have accelerated the development of robots. Traditional simulators based on first-principles, however, typically require full-state observability or depend on parameter search for system identification. This work presents \\texttt{CableRobotGraphSim}, a novel Graph Neural Network (GNN) model for cable-driven robots that aims to address shortcomings of prior simulation solutions. By representing cable-driven robots as graphs, with the rigid-bodies as nodes and the cables and contacts as edges, this model can quickly and accurately match the properties of other simulation models and real robots, while ingesting only partially observable inputs. Accompanying the GNN model is a sim-and-real co-training procedure that promotes generalization and robustness to noisy real data. This model is further integrated with a Model Predictive Path Integral (MPPI) controller for closed-loop navigation, which showcases the model's speed and accuracy.",
          "authors": [
            "Nelson Chen",
            "William R. Johnson",
            "Rebecca Kramer-Bottiglio",
            "Kostas Bekris",
            "Mridul Aanjaneya"
          ],
          "published": "2026-02-24T20:01:29Z",
          "updated": "2026-02-24T20:01:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21331v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21319v1",
          "title": "Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling",
          "summary": "Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings. This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction.",
          "authors": [
            "Marion Neumeier",
            "Niklas Roßberg",
            "Michael Botsch",
            "Wolfgang Utschick"
          ],
          "published": "2026-02-24T19:40:37Z",
          "updated": "2026-02-24T19:40:37Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21319v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21316v1",
          "title": "Unified Complementarity-Based Contact Modeling and Planning for Soft Robots",
          "summary": "Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics.",
          "authors": [
            "Milad Azizkhani",
            "Yue Chen"
          ],
          "published": "2026-02-24T19:37:36Z",
          "updated": "2026-02-24T19:37:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21316v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21302v1",
          "title": "Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control",
          "summary": "Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io",
          "authors": [
            "Krishna Suresh",
            "Chris Atkeson"
          ],
          "published": "2026-02-24T19:13:25Z",
          "updated": "2026-02-24T19:13:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21302v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21203v1",
          "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics",
          "summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.",
          "authors": [
            "Abdulaziz Almuzairee",
            "Henrik I. Christensen"
          ],
          "published": "2026-02-24T18:58:11Z",
          "updated": "2026-02-24T18:58:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21203v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21198v1",
          "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
          "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
          "authors": [
            "Yining Hong",
            "Huang Huang",
            "Manling Li",
            "Li Fei-Fei",
            "Jiajun Wu",
            "Yejin Choi"
          ],
          "published": "2026-02-24T18:55:18Z",
          "updated": "2026-02-24T18:55:18Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21198v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21174v1",
          "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids",
          "summary": "Hierarchical, multi-resolution volumetric mapping approaches are widely used to represent large and complex environments as they can efficiently capture their occupancy and connectivity information. Yet widely used path planning methods such as sampling and trajectory optimization do not exploit this explicit connectivity information, and search-based methods such as A* suffer from scalability issues in large-scale high-resolution maps. In many applications, Euclidean shortest paths form the underpinning of the navigation system. For such applications, any-angle planning methods, which find optimal paths by connecting corners of obstacles with straight-line segments, provide a simple and efficient solution. In this paper, we present a method that has the optimality and completeness properties of any-angle planners while overcoming computational tractability issues common to search-based methods by exploiting multi-resolution representations. Extensive experiments on real and synthetic environments demonstrate the proposed approach's solution quality and speed, outperforming even sampling-based methods. The framework is open-sourced to allow the robotics and planning community to build on our research.",
          "authors": [
            "Victor Reijgwart",
            "Cesar Cadena",
            "Roland Siegwart",
            "Lionel Ott"
          ],
          "published": "2026-02-24T18:18:36Z",
          "updated": "2026-02-24T18:18:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21174v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21161v1",
          "title": "ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking",
          "summary": "Classical robotic systems typically rely on custom planners designed for constrained environments. While effective in restricted settings, these systems lack generalization capabilities, limiting the scalability of embodied AI and general-purpose robots. Recent data-driven Vision-Language-Action (VLA) approaches aim to learn policies from large-scale simulation and real-world data. However, the continuous action space of the physical world significantly exceeds the representational capacity of linguistic tokens, making it unclear if scaling data alone can yield general robotic intelligence. To address this gap, we propose ActionReasoning, an LLM-driven framework that performs explicit action reasoning to produce physics-consistent, prior-guided decisions for robotic manipulation. ActionReasoning leverages the physical priors and real-world knowledge already encoded in Large Language Models (LLMs) and structures them within a multi-agent architecture. We instantiate this framework on a tractable case study of brick stacking, where the environment states are assumed to be already accurately measured. The environmental states are then serialized and passed to a multi-agent LLM framework that generates physics-aware action plans. The experiments demonstrate that the proposed multi-agent LLM framework enables stable brick placement while shifting effort from low-level domain-specific coding to high-level tool invocation and prompting, highlighting its potential for broader generalization. This work introduces a promising approach to bridging perception and execution in robotic manipulation by integrating physical reasoning with LLMs.",
          "authors": [
            "Guangming Wang",
            "Qizhen Ying",
            "Yixiong Jing",
            "Olaf Wysocki",
            "Brian Sheil"
          ],
          "published": "2026-02-24T18:07:06Z",
          "updated": "2026-02-24T18:07:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21161v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21157v1",
          "title": "HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning",
          "summary": "Vision-Language-Action (VLA) models have shown strong performance in robotic manipulation, but often struggle in long-horizon or out-of-distribution scenarios due to the lack of explicit mechanisms for multimodal reasoning and anticipating how the world will evolve under action. Recent works introduce textual chain-of-thought or visual subgoal prediction within VLA models to reason, but still fail to offer a unified human-like reasoning framework for joint textual reasoning, visual foresight, and action prediction. To this end, we propose HALO, a unified VLA model that enables embodied multimodal chain-of-thought (EM-CoT) reasoning through a sequential process of textual task reasoning, visual subgoal prediction for fine-grained guidance, and EM-CoT-augmented action prediction. We instantiate HALO with a Mixture-of-Transformers (MoT) architecture that decouples semantic reasoning, visual foresight, and action prediction into specialized experts while allowing seamless cross-expert collaboration. To enable HALO learning at scale, we introduce an automated pipeline to synthesize EM-CoT training data along with a carefully crafted training recipe. Extensive experiments demonstrate that: (1) HALO achieves superior performance in both simulated and real-world environments, surpassing baseline policy pi_0 by 34.1% on RoboTwin benchmark; (2) all proposed components of the training recipe and EM-CoT design help improve task success rate; and (3) HALO exhibits strong generalization capabilities under aggressive unseen environmental randomization with our proposed EM-CoT reasoning.",
          "authors": [
            "Quanxin Shou",
            "Fangqi Zhu",
            "Shawn Chen",
            "Puxin Yan",
            "Zhengyang Yan",
            "Yikun Miao",
            "Xiaoyi Pang",
            "Zicong Hong",
            "Ruikai Shi",
            "Hao Huang",
            "Jie Zhang",
            "Song Guo"
          ],
          "published": "2026-02-24T18:04:31Z",
          "updated": "2026-02-24T18:04:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21157v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21148v1",
          "title": "A Micro-Macro Model of Encounter-Driven Information Diffusion in Robot Swarms",
          "summary": "In this paper, we propose the problem of Encounter-Driven Information Diffusion (EDID). In EDID, robots are allowed to exchange information only upon meeting. Crucially, EDID assumes that the robots are not allowed to schedule their meetings. As such, the robots have no means to anticipate when, where, and who they will meet. As a step towards the design of storage and routing algorithms for EDID, in this paper we propose a model of information diffusion that captures the essential dynamics of EDID. The model is derived from first principles and is composed of two levels: a micro model, based on a generalization of the concept of `mean free path'; and a macro model, which captures the global dynamics of information diffusion. We validate the model through extensive robot simulations, in which we consider swarm size, communication range, environment size, and different random motion regimes. We conclude the paper with a discussion of the implications of this model on the algorithms that best support information diffusion according to the parameters of interest.",
          "authors": [
            "Davis S. Catherman",
            "Carlo Pinciroli"
          ],
          "published": "2026-02-24T17:49:56Z",
          "updated": "2026-02-24T17:49:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21148v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21119v1",
          "title": "Cooperative-Competitive Team Play of Real-World Craft Robots",
          "summary": "Multi-agent deep Reinforcement Learning (RL) has made significant progress in developing intelligent game-playing agents in recent years. However, the efficient training of collective robots using multi-agent RL and the transfer of learned policies to real-world applications remain open research questions. In this work, we first develop a comprehensive robotic system, including simulation, distributed learning framework, and physical robot components. We then propose and evaluate reinforcement learning techniques designed for efficient training of cooperative and competitive policies on this platform. To address the challenges of multi-agent sim-to-real transfer, we introduce Out of Distribution State Initialization (OODSI) to mitigate the impact of the sim-to-real gap. In the experiments, OODSI improves the Sim2Real performance by 20%. We demonstrate the effectiveness of our approach through experiments with a multi-robot car competitive game and a cooperative task in real-world settings.",
          "authors": [
            "Rui Zhao",
            "Xihui Li",
            "Yizheng Zhang",
            "Yuzhen Liu",
            "Zhong Zhang",
            "Yufeng Zhang",
            "Cheng Zhou",
            "Zhengyou Zhang",
            "Lei Han"
          ],
          "published": "2026-02-24T17:15:37Z",
          "updated": "2026-02-24T17:15:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21119v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21101v1",
          "title": "Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones",
          "summary": "Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.",
          "authors": [
            "Rong Zou",
            "Marco Cannici",
            "Davide Scaramuzza"
          ],
          "published": "2026-02-24T17:02:56Z",
          "updated": "2026-02-24T17:02:56Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21101v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21101v2",
          "title": "Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones",
          "summary": "Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.",
          "authors": [
            "Rong Zou",
            "Marco Cannici",
            "Davide Scaramuzza"
          ],
          "published": "2026-02-24T17:02:56Z",
          "updated": "2026-02-26T16:35:02Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21101v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21072v1",
          "title": "Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning",
          "summary": "Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.",
          "authors": [
            "Zhangjie Xia",
            "Yu Yang",
            "Pan Xu"
          ],
          "published": "2026-02-24T16:32:50Z",
          "updated": "2026-02-24T16:32:50Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21072v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21028v1",
          "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing",
          "summary": "There is a growing need for soft robotic platforms that perform gentle, precise handling of a wide variety of objects. Existing surface-based manipulation systems, however, lack the compliance and tactile feedback needed for delicate handling. This work introduces the COmpliant Porous-Elastic Soft Sensing (COPESS) integrated with inductive sensors for adaptive object manipulation and localised sensing. The design features a tunable lattice layer that simultaneously modulates mechanical compliance and sensing performance. By adjusting lattice geometry, both stiffness and sensor response can be tailored to handle objects with varying mechanical properties. Experiments demonstrate that by easily adjusting one parameter, the lattice density, from 7 % to 20 %, it is possible to significantly alter the sensitivity and operational force range (about -23x and 9x, respectively). This approach establishes a blueprint for creating adaptive, sensorized surfaces where mechanical and sensory properties are co-optimized, enabling passive, yet programmable, delicate manipulation.",
          "authors": [
            "Gayatri Indukumar",
            "Muhammad Awais",
            "Diana Cafiso",
            "Matteo Lo Preti",
            "Lucia Beccai"
          ],
          "published": "2026-02-24T15:51:10Z",
          "updated": "2026-02-24T15:51:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21028v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21013v1",
          "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks",
          "summary": "Many dexterous manipulation tasks are non-markovian in nature, yet little attention has been paid to this fact in the recent upsurge of the vision-language-action (VLA) paradigm. Although they are successful in bringing internet-scale semantic understanding to robotics, existing VLAs are primarily \"stateless\" and struggle with memory-dependent long horizon tasks. In this work, we explore a way to impart both spatial and temporal memory to a VLA by incorporating a language scratchpad. The scratchpad makes it possible to memorize task-specific information, such as object positions, and it allows the model to keep track of a plan and progress towards subgoals within that plan. We evaluate this approach on a split of memory-dependent tasks from the ClevrSkills environment, on MemoryBench, as well as on a challenging real-world pick-and-place task. We show that incorporating a language scratchpad significantly improves generalization on these tasks for both non-recurrent and recurrent models.",
          "authors": [
            "Sanjay Haresh",
            "Daniel Dijkman",
            "Apratim Bhattacharyya",
            "Roland Memisevic"
          ],
          "published": "2026-02-24T15:30:55Z",
          "updated": "2026-02-24T15:30:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21013v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20963v1",
          "title": "A Robotic Testing Platform for Pipelined Discovery of Resilient Soft Actuators",
          "summary": "Short lifetime under high electrical fields hinders the widespread robotic application of linear dielectric elastomer actuators (DEAs). Systematic scanning is difficult due to time-consuming per-sample testing and the high-dimensional parameter space affecting performance. To address this, we propose an optimization pipeline enabled by a novel testing robot capable of scanning DEA lifetime. The robot integrates electro-mechanical property measurement, programmable voltage input, and multi-channel testing capacity. Using it, we scanned the lifetime of Elastosil-based linear actuators across parameters including input voltage magnitude, frequency, electrode material concentration, and electrical connection filler. The optimal parameter combinations improved operational lifetime under boundary operating conditions by up to 100% and were subsequently scaled up to achieve higher force and displacement output. The final product demonstrated resilience on a modular, scalable quadruped walking robot with payload carrying capacity (>100% of its untethered body weight, and >700% of combined actuator weight). This work is the first to introduce a self-driving lab approach into robotic actuator design.",
          "authors": [
            "Ang",
            "Li",
            "Alexander Yin",
            "Alexander White",
            "Sahib Sandhu",
            "Matthew Francoeur",
            "Victor Jimenez-Santiago",
            "Van Remenar",
            "Codrin Tugui",
            "Mihai Duduta"
          ],
          "published": "2026-02-24T14:41:28Z",
          "updated": "2026-02-24T14:41:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20963v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20958v1",
          "title": "EKF-Based Depth Camera and Deep Learning Fusion for UAV-Person Distance Estimation and Following in SAR Operations",
          "summary": "Search and rescue (SAR) operations require rapid responses to save lives or property. Unmanned Aerial Vehicles (UAVs) equipped with vision-based systems support these missions through prior terrain investigation or real-time assistance during the mission itself. Vision-based UAV frameworks aid human search tasks by detecting and recognizing specific individuals, then tracking and following them while maintaining a safe distance. A key safety requirement for UAV following is the accurate estimation of the distance between camera and target object under real-world conditions, achieved by fusing multiple image modalities. UAVs with deep learning-based vision systems offer a new approach to the planning and execution of SAR operations. As part of the system for automatic people detection and face recognition using deep learning, in this paper we present the fusion of depth camera measurements and monocular camera-to-body distance estimation for robust tracking and following. Deep learning-based filtering of depth camera data and estimation of camera-to-body distance from a monocular camera are achieved with YOLO-pose, enabling real-time fusion of depth information using the Extended Kalman Filter (EKF) algorithm. The proposed subsystem, designed for use in drones, estimates and measures the distance between the depth camera and the human body keypoints, to maintain the safe distance between the drone and the human target. Our system provides an accurate estimated distance, which has been validated against motion capture ground truth data. The system has been tested in real time indoors, where it reduces the average errors, root mean square error (RMSE) and standard deviations of distance estimation up to 15,3\\% in three tested scenarios.",
          "authors": [
            "Luka Šiktar",
            "Branimir Ćaran",
            "Bojan Šekoranja",
            "Marko Švaco"
          ],
          "published": "2026-02-24T14:37:36Z",
          "updated": "2026-02-24T14:37:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20958v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20925v1",
          "title": "LST-SLAM: A Stereo Thermal SLAM System for Kilometer-Scale Dynamic Environments",
          "summary": "Thermal cameras offer strong potential for robot perception under challenging illumination and weather conditions. However, thermal Simultaneous Localization and Mapping (SLAM) remains difficult due to unreliable feature extraction, unstable motion tracking, and inconsistent global pose and map construction, particularly in dynamic large-scale outdoor environments. To address these challenges, we propose LST-SLAM, a novel large-scale stereo thermal SLAM system that achieves robust performance in complex, dynamic scenes. Our approach combines self-supervised thermal feature learning, stereo dual-level motion tracking, and geometric pose optimization. We also introduce a semantic-geometric hybrid constraint that suppresses potentially dynamic features lacking strong inter-frame geometric consistency. Furthermore, we develop an online incremental bag-of-words model for loop closure detection, coupled with global pose optimization to mitigate accumulated drift. Extensive experiments on kilometer-scale dynamic thermal datasets show that LST-SLAM significantly outperforms recent representative SLAM systems, including AirSLAM and DROID-SLAM, in both robustness and accuracy.",
          "authors": [
            "Zeyu Jiang",
            "Kuan Xu",
            "Changhao Chen"
          ],
          "published": "2026-02-24T14:04:54Z",
          "updated": "2026-02-24T14:04:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20925v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20923v1",
          "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models",
          "summary": "Automated parking is a challenging operational domain for advanced driver assistance systems, requiring robust scene understanding and interaction reasoning. The key challenge is twofold: (i) predict multiple plausible ego intentions according to context and (ii) for each intention, predict the joint responses of surrounding agents, enabling effective what-if decision-making. However, existing methods often fall short, typically treating these interdependent problems in isolation. We propose ParkDiffusion++, which jointly learns a multi-modal ego intention predictor and an ego-conditioned multi-agent joint trajectory predictor for automated parking. Our approach makes several key contributions. First, we introduce an ego intention tokenizer that predicts a small set of discrete endpoint intentions from agent histories and vectorized map polylines. Second, we perform ego-intention-conditioned joint prediction, yielding socially consistent predictions of the surrounding agents for each possible ego intention. Third, we employ a lightweight safety-guided denoiser with different constraints to refine joint scenes during training, thus improving accuracy and safety. Fourth, we propose counterfactual knowledge distillation, where an EMA teacher refined by a frozen safety-guided denoiser provides pseudo-targets that capture how agents react to alternative ego intentions. Extensive evaluations demonstrate that ParkDiffusion++ achieves state-of-the-art performance on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Importantly, qualitative what-if visualizations show that other agents react appropriately to different ego intentions.",
          "authors": [
            "Jiarong Wei",
            "Anna Rehr",
            "Christian Feist",
            "Abhinav Valada"
          ],
          "published": "2026-02-24T14:01:33Z",
          "updated": "2026-02-24T14:01:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20923v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20920v1",
          "title": "Computer-Aided Design of Rational Motions for 4R and 6R Spatial Mechanism Synthesis",
          "summary": "This paper focuses on geometric methods for generating rational motions used in the design of single-loop rational linkages, 1-degree-of-freedom mechanisms that can execute prescribed spatial tasks. Building on established rational motion synthesis methods, we introduce a new interpolation scheme for seven 3D points based on cubic quaternionic Bezier curves. The resulting motion admits factorization, i.e. the synthesis of a spatial six-bar mechanism whose tool frame passes the specified seven points. To support engineering practice, we provide open-source CAD tools that implement also the other methods and provide fast visual evaluation of motion generation and mechanism synthesis.",
          "authors": [
            "Daniel Huczala",
            "Severinas Zube",
            "Martin Pfurner",
            "Johannes Siegele",
            "Frank C. Park"
          ],
          "published": "2026-02-24T13:58:36Z",
          "updated": "2026-02-24T13:58:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20920v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20915v1",
          "title": "Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning",
          "summary": "In this paper, we address the problem of task-oriented grasping for humanoid robots, emphasizing the need to align with human social norms and task-specific objectives. Existing methods, employ a variety of open-loop and closed-loop approaches but lack an end-to-end solution that can grasp several objects while taking into account the downstream task's constraints. Our proposed approach employs reinforcement learning to enhance task-oriented grasping, prioritizing the post-grasp intention of the agent. We extract human grasp preferences from the ContactPose dataset, and train a hand synergy model based on the Variational Autoencoder (VAE) to imitate the participant's grasping actions. Based on this data, we train an agent able to grasp multiple objects while taking into account distinct post-grasp intentions that are task-specific. By combining data-driven insights from human grasping behavior with learning by exploration provided by reinforcement learning, we can develop humanoid robots capable of context-aware manipulation actions, facilitating collaboration in human-centered environments.",
          "authors": [
            "Dimitrios Dimou",
            "José Santos-Victor",
            "Plinio Moreno"
          ],
          "published": "2026-02-24T13:51:09Z",
          "updated": "2026-02-24T13:51:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20915v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20871v1",
          "title": "GeCo-SRT: Geometry-aware Continual Adaptation for Robotic Cross-Task Sim-to-Real Transfer",
          "summary": "Bridging the sim-to-real gap is important for applying low-cost simulation data to real-world robotic systems. However, previous methods are severely limited by treating each transfer as an isolated endeavor, demanding repeated, costly tuning and wasting prior transfer experience.To move beyond isolated sim-to-real, we build a continual cross-task sim-to-real transfer paradigm centered on knowledge accumulation across iterative transfers, thereby enabling effective and efficient adaptation to novel tasks. Thus, we propose GeCo-SRT, a geometry-aware continual adaptation method. It utilizes domain-invariant and task-invariant knowledge from local geometric features as a transferable foundation to accelerate adaptation during subsequent sim-to-real transfers. This method starts with a geometry-aware mixture-of-experts module, which dynamically activates experts to specialize in distinct geometric knowledge to bridge observation sim-to-real gap. Further, the geometry-expert-guided prioritized experience replay module preferentially samples from underutilized experts, refreshing specialized knowledge to combat forgetting and maintain robust cross-task performance. Leveraging knowledge accumulated during iterative transfer, GeCo-SRT method not only achieves 52% average performance improvement over the baseline, but also demonstrates significant data efficiency for new task adaptation with only 1/6 data.We hope this work inspires approaches for efficient, low-cost cross-task sim-to-real transfer.",
          "authors": [
            "Wenbo Yu",
            "Wenke Xia",
            "Weitao Zhang",
            "Di Hu"
          ],
          "published": "2026-02-24T13:15:38Z",
          "updated": "2026-02-24T13:15:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20871v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20871v2",
          "title": "GeCo-SRT: Geometry-aware Continual Adaptation for Robotic Cross-Task Sim-to-Real Transfer",
          "summary": "Bridging the sim-to-real gap is important for applying low-cost simulation data to real-world robotic systems. However, previous methods are severely limited by treating each transfer as an isolated endeavor, demanding repeated, costly tuning and wasting prior transfer experience. To move beyond isolated sim-to-real, we build a continual cross-task sim-to-real transfer paradigm centered on knowledge accumulation across iterative transfers, thereby enabling effective and efficient adaptation to novel tasks. Thus, we propose GeCo-SRT, a geometry-aware continual adaptation method. It utilizes domain-invariant and task-invariant knowledge from local geometric features as a transferable foundation to accelerate adaptation during subsequent sim-to-real transfers. This method starts with a geometry-aware mixture-of-experts module, which dynamically activates experts to specialize in distinct geometric knowledge to bridge observation sim-to-real gap. Further, the geometry-expert-guided prioritized experience replay module preferentially samples from underutilized experts, refreshing specialized knowledge to combat forgetting and maintain robust cross-task performance. Leveraging knowledge accumulated during iterative transfer, GeCo-SRT method not only achieves 52% average performance improvement over the baseline, but also demonstrates significant data efficiency for new task adaptation with only 1/6 data. We hope this work inspires approaches for efficient, low-cost cross-task sim-to-real transfer.",
          "authors": [
            "Wenbo Yu",
            "Wenke Xia",
            "Weitao Zhang",
            "Di Hu"
          ],
          "published": "2026-02-24T13:15:38Z",
          "updated": "2026-02-25T14:34:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20871v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20850v1",
          "title": "KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion",
          "summary": "Legged robots face significant challenges in navigating complex environments, as they require precise real-time decisions for foothold selection and contact planning. While existing research has explored methods to select footholds based on terrain geometry or kinematics, a critical gap remains: few existing methods efficiently validate the existence of a non-collision swing trajectory. This paper addresses this gap by introducing KCFRC, a novel approach for efficient foothold reachability analysis. We first formally define the foothold reachability problem and establish a sufficient condition for foothold reachability. Based on this condition, we develop the KCFRC algorithm, which enables robots to validate foothold reachability in real time. Our experimental results demonstrate that KCFRC achieves remarkable time efficiency, completing foothold reachability checks for a single leg across 900 potential footholds in an average of 2 ms. Furthermore, we show that KCFRC can accelerate trajectory optimization and is particularly beneficial for contact planning in confined spaces, enhancing the adaptability and robustness of legged robots in challenging environments.",
          "authors": [
            "Lei Ye",
            "Haibo Gao",
            "Huaiguang Yang",
            "Peng Xu",
            "Haoyu Wang",
            "Tie Liu",
            "Junqi Shan",
            "Zongquan Deng",
            "Liang Ding"
          ],
          "published": "2026-02-24T12:46:34Z",
          "updated": "2026-02-24T12:46:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20850v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.22243v1",
          "title": "SODA-CitrON: Static Object Data Association by Clustering Multi-Modal Sensor Detections Online",
          "summary": "The online fusion and tracking of static objects from heterogeneous sensor detections is a fundamental problem in robotics, autonomous systems, and environmental mapping. Although classical data association approaches such as JPDA are well suited for dynamic targets, they are less effective for static objects observed intermittently and with heterogeneous uncertainties, where motion models provide minimal discriminative with respect to clutter. In this paper, we propose a novel method for static object data association by clustering multi-modal sensor detections online (SODA-CitrON), while simultaneously estimating positions and maintaining persistent tracks for an unknown number of objects. The proposed unsupervised machine learning approach operates in a fully online manner and handles temporally uncorrelated and multi-sensor measurements. Additionally, it has a worst-case loglinear complexity in the number of sensor detections while providing full output explainability. We evaluate the proposed approach in different Monte Carlo simulation scenarios and compare it against state-of-the-art methods, including Bayesian filtering, DBSTREAM clustering, and JPDA. The results demonstrate that SODA-CitrON consistently outperforms the compared methods in terms of F1 score, position RMSE, MOTP, and MOTA in the static object mapping scenarios studied.",
          "authors": [
            "Jan Nausner",
            "Kilian Wohlleben",
            "Michael Hubner"
          ],
          "published": "2026-02-24T12:38:45Z",
          "updated": "2026-02-24T12:38:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.22243v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20807v1",
          "title": "RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction",
          "summary": "Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io",
          "authors": [
            "Yangfan Zhao",
            "Hanwei Zhang",
            "Ke Huang",
            "Qiufeng Wang",
            "Zhenzhou Shao",
            "Dengyu Wu"
          ],
          "published": "2026-02-24T11:47:43Z",
          "updated": "2026-02-24T11:47:43Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20807v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20790v1",
          "title": "Real-time Motion Segmentation with Event-based Normal Flow",
          "summary": "Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.",
          "authors": [
            "Sheng Zhong",
            "Zhongyang Ren",
            "Xiya Zhu",
            "Dehao Yuan",
            "Cornelia Fermuller",
            "Yi Zhou"
          ],
          "published": "2026-02-24T11:29:07Z",
          "updated": "2026-02-24T11:29:07Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20790v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20768v1",
          "title": "Visual Cooperative Drone Tracking for Open-Path Gas Measurements",
          "summary": "Open-path Tunable Diode Laser Absorption Spectroscopy offers an effective method for measuring, mapping, and monitoring gas concentrations, such as leaking CO2 or methane. Compared to spatial sampling of gas distributions using in-situ sensors, open-path sensors in combination with gas tomography algorithms can cover large outdoor environments faster in a non-invasive way. However, the requirement of a dedicated reflection surface for the open-path laser makes automating the spatial sampling process challenging. This publication presents a robotic system for collecting open-path measurements, making use of a sensor mounted on a ground-based pan-tilt unit and a small drone carrying a reflector. By means of a zoom camera, the ground unit visually tracks red LED markers mounted on the drone and aligns the sensor's laser beam with the reflector. Incorporating GNSS position information provided by the drone's flight controller further improves the tracking approach. Outdoor experiments validated the system's performance, demonstrating successful autonomous tracking and valid CO2 measurements at distances up to 60 meters. Furthermore, the system successfully measured a CO2 plume without interference from the drone's propulsion system, demonstrating its superiority compared to flying in-situ sensors.",
          "authors": [
            "Marius Schaab",
            "Alisha Kiefer",
            "Thomas Wiedemann",
            "Patrick Hinsen",
            "Achim J. Lilienthal"
          ],
          "published": "2026-02-24T11:00:23Z",
          "updated": "2026-02-24T11:00:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20768v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21266v1",
          "title": "Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints",
          "summary": "Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models.",
          "authors": [
            "Mor Levenhar",
            "Itzik Klein"
          ],
          "published": "2026-02-24T09:47:31Z",
          "updated": "2026-02-24T09:47:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21266v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20715v1",
          "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation",
          "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential for generalist robotic policies; however, they struggle to generalize to long-horizon complex tasks in novel real-world domains due to distribution shifts and the scarcity of high-quality demonstrations. Although reinforcement learning (RL) offers a promising avenue for policy improvement, applying it to real-world VLA fine-tuning faces challenges regarding exploration efficiency, training stability, and sample cost. To address these issues, we propose IG-RFT, a novel Interaction-Guided Reinforced Fine-Tuning system designed for flow-based VLA models. Firstly, to facilitate effective policy optimization, we introduce Interaction-Guided Advantage Weighted Regression (IG-AWR), an RL algorithm that dynamically modulates exploration intensity based on the robot's interaction status. Furthermore, to address the limitations of sparse or task-specific rewards, we design a novel hybrid dense reward function that integrates the trajectory-level reward and the subtask-level reward. Finally, we construct a three-stage RL system comprising SFT, Offline RL, and Human-in-the-Loop RL for fine-tuning VLA models. Extensive real-world experiments on four challenging long-horizon tasks demonstrate that IG-RFT achieves an average success rate of 85.0%, significantly outperforming SFT (18.8%) and standard Offline RL baselines (40.0%). Ablation studies confirm the critical contributions of IG-AWR and hybrid reward shaping. In summary, our work establishes and validates a novel reinforced fine-tuning system for VLA models in real-world robotic manipulation.",
          "authors": [
            "Zhian Su",
            "Weijie Kong",
            "Haonan Dong",
            "Huixu Dong"
          ],
          "published": "2026-02-24T09:19:50Z",
          "updated": "2026-02-24T09:19:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20715v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20645v1",
          "title": "Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments",
          "summary": "The objective of this study is to enable fast and safe manipulation tasks in home environments. Specifically, we aim to develop a system that can recognize its surroundings and identify target objects while in motion, enabling it to plan and execute actions accordingly. We propose a periodic sampling-based whole-body trajectory planning method, called the \"Robot Local Planner (RLP).\" This method leverages unique features of home environments to enhance computational efficiency, motion optimality, and robustness against recognition and control errors, all while ensuring safety. The RLP minimizes computation time by planning with minimal waypoints and generating safe trajectories. Furthermore, overall motion optimality is improved by periodically executing trajectory planning to select more optimal motions. This approach incorporates inverse kinematics that are robust to base position errors, further enhancing robustness. Evaluation experiments demonstrated that the RLP outperformed existing methods in terms of motion planning time, motion duration, and robustness, confirming its effectiveness in home environments. Moreover, application experiments using a tidy-up task achieved high success rates and short operation times, thereby underscoring its practical feasibility.",
          "authors": [
            "Keisuke Takeshita",
            "Takahiro Yamazaki",
            "Tomohiro Ono",
            "Takashi Yamamoto"
          ],
          "published": "2026-02-24T07:46:31Z",
          "updated": "2026-02-24T07:46:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20645v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20627v1",
          "title": "Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection",
          "summary": "Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.",
          "authors": [
            "Zhaonian Kuang",
            "Rui Ding",
            "Meng Yang",
            "Xinhu Zheng",
            "Gang Hua"
          ],
          "published": "2026-02-24T07:22:58Z",
          "updated": "2026-02-24T07:22:58Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20627v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20596v1",
          "title": "Acoustic Feedback for Closed-Loop Force Control in Robotic Grinding",
          "summary": "Acoustic feedback is a critical indicator for assessing the contact condition between the tool and the workpiece when humans perform grinding tasks with rotary tools. In contrast, robotic grinding systems typically rely on force sensing, with acoustic information largely ignored. This reliance on force sensors is costly and difficult to adapt to different grinding tools, whereas audio sensors (microphones) are low-cost and can be mounted on any medium that conducts grinding sound. This paper introduces a low-cost Acoustic Feedback Robotic Grinding System (AFRG) that captures audio signals with a contact microphone, estimates grinding force from the audio in real time, and enables closed-loop force control of the grinding process. Compared with conventional force-sensing approaches, AFRG achieves a 4-fold improvement in consistency across different grinding disc conditions. AFRG relies solely on a low-cost microphone, which is approximately 200-fold cheaper than conventional force sensors, as the sensing modality, providing an easily deployable, cost-effective robotic grinding solution.",
          "authors": [
            "Zongyuan Zhang",
            "Christopher Lehnert",
            "Will N. Browne",
            "Jonathan M. Roberts"
          ],
          "published": "2026-02-24T06:35:43Z",
          "updated": "2026-02-24T06:35:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20596v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20584v1",
          "title": "Long-Term Multi-Session 3D Reconstruction Under Substantial Appearance Change",
          "summary": "Long-term environmental monitoring requires the ability to reconstruct and align 3D models across repeated site visits separated by months or years. However, existing Structure-from-Motion (SfM) pipelines implicitly assume near-simultaneous image capture and limited appearance change, and therefore fail when applied to long-term monitoring scenarios such as coral reef surveys, where substantial visual and structural change is common. In this paper, we show that the primary limitation of current approaches lies in their reliance on post-hoc alignment of independently reconstructed sessions, which is insufficient under large temporal appearance change. We address this limitation by enforcing cross-session correspondences directly within a joint SfM reconstruction. Our approach combines complementary handcrafted and learned visual features to robustly establish correspondences across large temporal gaps, enabling the reconstruction of a single coherent 3D model from imagery captured years apart, where standard independent and joint SfM pipelines break down. We evaluate our method on long-term coral reef datasets exhibiting significant real-world change, and demonstrate consistent joint reconstruction across sessions in cases where existing methods fail to produce coherent reconstructions. To ensure scalability to large datasets, we further restrict expensive learned feature matching to a small set of likely cross-session image pairs identified via visual place recognition, which reduces computational cost and improves alignment robustness.",
          "authors": [
            "Beverley Gorry",
            "Tobias Fischer",
            "Michael Milford",
            "Alejandro Fontan"
          ],
          "published": "2026-02-24T06:12:51Z",
          "updated": "2026-02-24T06:12:51Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20584v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20566v1",
          "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model",
          "summary": "Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.",
          "authors": [
            "Haosheng Li",
            "Weixin Mao",
            "Zihan Lan",
            "Hongwei Xiong",
            "Hongan Wang",
            "Chenyang Si",
            "Ziwei Liu",
            "Xiaoming Deng",
            "Hua Chen"
          ],
          "published": "2026-02-24T05:31:52Z",
          "updated": "2026-02-24T05:31:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20566v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20512v1",
          "title": "Conflict-Based Search for Multi-Agent Path Finding with Elevators",
          "summary": "This paper investigates a problem called Multi-Agent Path Finding with Elevators (MAPF-E), which seeks conflict-free paths for multiple agents from their start to goal locations that may locate on different floors, and the agents can use elevators to travel between floors. The existence of elevators complicates the interaction among the agents and introduces new challenges to the planning. On the one hand, elevators can cause many conflicts among the agents due to its relatively long traversal time across floors, especially when many agents need to reach a different floor. On the other hand, the planner has to reason in a larger state space including the states of the elevators, besides the locations of the agents.",
          "authors": [
            "Haitong He",
            "Xuemian Wu",
            "Shizhe Zhao",
            "Zhongqiang Ren"
          ],
          "published": "2026-02-24T03:29:27Z",
          "updated": "2026-02-24T03:29:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20512v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20500v1",
          "title": "Strategy-Supervised Autonomous Laparoscopic Camera Control via Event-Driven Graph Mining",
          "summary": "Autonomous laparoscopic camera control must maintain a stable and safe surgical view under rapid tool-tissue interactions while remaining interpretable to surgeons. We present a strategy-grounded framework that couples high-level vision-language inference with low-level closed-loop control. Offline, raw surgical videos are parsed into camera-relevant temporal events (e.g., interaction, working-distance deviation, and view-quality degradation) and structured as attributed event graphs. Mining these graphs yields a compact set of reusable camera-handling strategy primitives, which provide structured supervision for learning. Online, a fine-tuned Vision-Language Model (VLM) processes the live laparoscopic view to predict the dominant strategy and discrete image-based motion commands, executed by an IBVS-RCM controller under strict safety constraints; optional speech input enables intuitive human-in-the-loop conditioning. On a surgeon-annotated dataset, event parsing achieves reliable temporal localization (F1-score 0.86), and the mined strategies show strong semantic alignment with expert interpretation (cluster purity 0.81). Extensive ex vivo experiments on silicone phantoms and porcine tissues demonstrate that the proposed system outperforms junior surgeons in standardized camera-handling evaluations, reducing field-of-view centering error by 35.26% and image shaking by 62.33%, while preserving smooth motion and stable working-distance regulation.",
          "authors": [
            "Keyu Zhou",
            "Peisen Xu",
            "Yahao Wu",
            "Jiming Chen",
            "Gaofeng Li",
            "Shunlei Li"
          ],
          "published": "2026-02-24T02:56:39Z",
          "updated": "2026-02-24T02:56:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20500v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20466v1",
          "title": "Grasp to Act: Dexterous Grasping for Tool Use in Dynamic Settings",
          "summary": "Achieving robust grasping with dexterous hands remains challenging, especially when manipulation involves dynamic forces such as impacts, torques, and continuous resistance--situations common in real-world tool use. Existing methods largely optimize grasps for static geometric stability and often fail once external forces arise during manipulation. We present Grasp-to-Act, a hybrid system that combines physics-based grasp optimization with reinforcement-learning-based grasp adaptation to maintain stable grasps throughout functional manipulation tasks. Our method synthesizes robust grasp configurations informed by human demonstrations and employs an adaptive controller that residually issues joint corrections to prevent in-hand slip while tracking the object trajectory. Grasp-to-Act enables robust zero-shot sim-to-real transfer across five dynamic tool-use tasks--hammering, sawing, cutting, stirring, and scooping--consistently outperforming baselines. Across simulation and real-world hardware trials with a 16-DoF dexterous hand, our method reduces translational and rotational in-hand slip and achieves the highest task completion rates, demonstrating stable functional grasps under dynamic, contact-rich conditions.",
          "authors": [
            "Harsh Gupta",
            "Mohammad Amin Mirzaee",
            "Wenzhen Yuan"
          ],
          "published": "2026-02-24T01:53:39Z",
          "updated": "2026-02-24T01:53:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20466v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20375v1",
          "title": "Generalizing from References using a Multi-Task Reference and Goal-Driven RL Framework",
          "summary": "Learning agile humanoid behaviors from human motion offers a powerful route to natural, coordinated control, but existing approaches face a persistent trade-off: reference-tracking policies are often brittle outside the demonstration dataset, while purely task-driven Reinforcement Learning (RL) can achieve adaptability at the cost of motion quality. We introduce a unified multi-task RL framework that bridges this gap by treating reference motion as a prior for behavioral shaping rather than a deployment-time constraint. A single goal-conditioned policy is trained jointly on two tasks that share the same observation and action spaces, but differ in their initialization schemes, command spaces, and reward structures: (i) a reference-guided imitation task in which reference trajectories define dense imitation rewards but are not provided as policy inputs, and (ii) a goal-conditioned generalization task in which goals are sampled independently of any reference and where rewards reflect only task success. By co-optimizing these objectives within a shared formulation, the policy acquires structured, human-like motor skills from dense reference supervision while learning to adapt these skills to novel goals and initial conditions. This is achieved without adversarial objectives, explicit trajectory tracking, phase variables, or reference-dependent inference. We evaluate the method on a challenging box-based parkour playground that demands diverse athletic behaviors (e.g., jumping and climbing), and show that the learned controller transfers beyond the reference distribution while preserving motion naturalness. Finally, we demonstrate long-horizon behavior generation by composing multiple learned skills, illustrating the flexibility of the learned polices in complex scenarios.",
          "authors": [
            "Jiashun Wang",
            "M. Eva Mungai",
            "He Li",
            "Jean Pierre Sleiman",
            "Jessica Hodgins",
            "Farbod Farshidian"
          ],
          "published": "2026-02-23T21:25:06Z",
          "updated": "2026-02-23T21:25:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20375v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20362v1",
          "title": "Energy-Based Injury Protection Database: Including Shearing Contact Thresholds for Hand and Finger Using Porcine Surrogates",
          "summary": "While robotics research continues to propose strategies for collision avoidance in human-robot interaction, the reality of constrained environments and future humanoid systems makes contact inevitable. To mitigate injury risks, energy-constraining control approaches are commonly used, often relying on safety thresholds derived from blunt impact data in EN ISO 10218-2:2025. However, this dataset does not extend to edged or pointed collisions. Without scalable, clinically grounded datasets covering diverse contact scenarios, safety validation remains limited. Previous studies have laid the groundwork by assessing surrogate-based velocity and mass limits across various geometries, focusing on perpendicular impacts. This study expands those datasets by including shearing contact scenarios in unconstrained collisions, revealing that collision angle significantly affects injury outcomes. Notably, unconstrained shearing contacts result in fewer injuries than perpendicular ones. By reevaluating all prior porcine surrogate data, we establish energy thresholds across geometries and contact types, forming the first energy-based Injury Protection Database. This enables the development of meaningful energy-limiting controllers that ensure safety across a wide range of realistic collision events.",
          "authors": [
            "Robin Jeanne Kirschner",
            "Anna Huber",
            "Carina M. Micheler",
            "Dirk Müller",
            "Nader Rajaei",
            "Rainer Burgkart",
            "Sami Haddadin"
          ],
          "published": "2026-02-23T21:07:48Z",
          "updated": "2026-02-23T21:07:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20362v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.21259v1",
          "title": "Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles",
          "summary": "Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles.",
          "authors": [
            "Ricardo B. Grando",
            "Victor A. Kich",
            "Alisson H. Kolling",
            "Junior C. D. Jesus",
            "Rodrigo S. Guerra",
            "Paulo L. J. Drews-Jr"
          ],
          "published": "2026-02-23T20:56:21Z",
          "updated": "2026-02-23T20:56:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.21259v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20342v1",
          "title": "Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques",
          "summary": "In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.",
          "authors": [
            "Christos Maikos",
            "Georgios Angelidis",
            "Georgios Th. Papadopoulos"
          ],
          "published": "2026-02-23T20:40:26Z",
          "updated": "2026-02-23T20:40:26Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20342v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20334v1",
          "title": "UAMTERS: Uncertainty-Aware Mutation Analysis for DL-enabled Robotic Software",
          "summary": "Self-adaptive robots adjust their behaviors in response to unpredictable environmental changes. These robots often incorporate deep learning (DL) components into their software to support functionality such as perception, decision-making, and control, enhancing autonomy and self-adaptability. However, the inherent uncertainty of DL-enabled software makes it challenging to ensure its dependability in dynamic environments. Consequently, test generation techniques have been developed to test robot software, and classical mutation analysis injects faults into the software to assess the test suite's effectiveness in detecting the resulting failures. However, there is a lack of mutation analysis techniques to assess the effectiveness under the uncertainty inherent to DL-enabled software. To this end, we propose UAMTERS, an uncertainty-aware mutation analysis framework that introduces uncertainty-aware mutation operators to explicitly inject stochastic uncertainty into DL-enabled robotic software, simulating uncertainty in its behavior. We further propose mutation score metrics to quantify a test suite's ability to detect failures under varying levels of uncertainty. We evaluate UAMTERS across three robotic case studies, demonstrating that UAMTERS more effectively distinguishes test suite quality and captures uncertainty-induced failures in DL-enabled software.",
          "authors": [
            "Chengjie Lu",
            "Jiahui Wu",
            "Shaukat Ali",
            "Malaika Din Hashmi",
            "Sebastian Mathias Thomle Mason",
            "Francois Picard",
            "Mikkel Labori Olsen",
            "Thomas Peyrucain"
          ],
          "published": "2026-02-23T20:31:07Z",
          "updated": "2026-02-23T20:31:07Z",
          "primary_category": "cs.SE",
          "categories": [
            "cs.SE",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20334v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20323v1",
          "title": "Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory",
          "summary": "Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.",
          "authors": [
            "Haoyang Li",
            "Yang You",
            "Hao Su",
            "Leonidas Guibas"
          ],
          "published": "2026-02-23T20:18:35Z",
          "updated": "2026-02-23T20:18:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20323v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20304v1",
          "title": "Smoothly Differentiable and Efficiently Vectorizable Contact Manifold Generation",
          "summary": "Simulating rigid-body dynamics with contact in a fast, massively vectorizable, and smoothly differentiable manner is highly desirable in robotics. An important bottleneck faced by existing differentiable simulation frameworks is contact manifold generation: representing the volume of intersection between two colliding geometries via a discrete set of properly distributed contact points. A major factor contributing to this bottleneck is that the related routines of commonly used robotics simulators were not designed with vectorization and differentiability as a primary concern, and thus rely on logic and control flow that hinder these goals. We instead propose a framework designed from the ground up with these goals in mind, by trying to strike a middle ground between: i) convex primitive based approaches used by common robotics simulators (efficient but not differentiable), and ii) mollified vertex-face and edge-edge unsigned distance-based approaches used by barrier methods (differentiable but inefficient). Concretely, we propose: i) a representative set of smooth analytical signed distance primitives to implement vertex-face collisions, and ii) a novel differentiable edge-edge collision routine that can provide signed distances and signed contact normals. The proposed framework is evaluated via a set of didactic experiments and benchmarked against the collision detection routine of the well-established Mujoco XLA framework, where we observe a significant speedup. Supplementary videos can be found at https://github.com/bekeronur/contax, where a reference implementation in JAX will also be made available at the conclusion of the review process.",
          "authors": [
            "Onur Beker",
            "Andreas René Geist",
            "Anselm Paulus",
            "Nico Gürtler",
            "Ji Shi",
            "Sylvain Calinon",
            "Georg Martius"
          ],
          "published": "2026-02-23T19:34:47Z",
          "updated": "2026-02-23T19:34:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20304v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20159v1",
          "title": "A Very Big Video Reasoning Suite",
          "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
          "authors": [
            "Maijunxian Wang",
            "Ruisi Wang",
            "Juyi Lin",
            "Ran Ji",
            "Thaddäus Wiedemer",
            "Qingying Gao",
            "Dezhi Luo",
            "Yaoyao Qian",
            "Lianyu Huang",
            "Zelong Hong",
            "Jiahui Ge",
            "Qianli Ma",
            "Hang He",
            "Yifan Zhou",
            "Lingzi Guo",
            "Lantao Mei",
            "Jiachen Li",
            "Hanwen Xing",
            "Tianqi Zhao",
            "Fengyuan Yu",
            "Weihang Xiao",
            "Yizheng Jiao",
            "Jianheng Hou",
            "Danyang Zhang",
            "Pengcheng Xu",
            "Boyang Zhong",
            "Zehong Zhao",
            "Gaoyun Fang",
            "John Kitaoka",
            "Yile Xu",
            "Hua Xu",
            "Kenton Blacutt",
            "Tin Nguyen",
            "Siyuan Song",
            "Haoran Sun",
            "Shaoyue Wen",
            "Linyang He",
            "Runming Wang",
            "Yanzhi Wang",
            "Mengyue Yang",
            "Ziqiao Ma",
            "Raphaël Millière",
            "Freda Shi",
            "Nuno Vasconcelos",
            "Daniel Khashabi",
            "Alan Yuille",
            "Yilun Du",
            "Ziming Liu",
            "Bo Li",
            "Dahua Lin",
            "Ziwei Liu",
            "Vikash Kumar",
            "Yijiang Li",
            "Lei Yang",
            "Zhongang Cai",
            "Hokin Deng"
          ],
          "published": "2026-02-23T18:59:41Z",
          "updated": "2026-02-23T18:59:41Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20159v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20159v2",
          "title": "A Very Big Video Reasoning Suite",
          "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
          "authors": [
            "Maijunxian Wang",
            "Ruisi Wang",
            "Juyi Lin",
            "Ran Ji",
            "Thaddäus Wiedemer",
            "Qingying Gao",
            "Dezhi Luo",
            "Yaoyao Qian",
            "Lianyu Huang",
            "Zelong Hong",
            "Jiahui Ge",
            "Qianli Ma",
            "Hang He",
            "Yifan Zhou",
            "Lingzi Guo",
            "Lantao Mei",
            "Jiachen Li",
            "Hanwen Xing",
            "Tianqi Zhao",
            "Fengyuan Yu",
            "Weihang Xiao",
            "Yizheng Jiao",
            "Jianheng Hou",
            "Danyang Zhang",
            "Pengcheng Xu",
            "Boyang Zhong",
            "Zehong Zhao",
            "Gaoyun Fang",
            "John Kitaoka",
            "Yile Xu",
            "Hua Xu",
            "Kenton Blacutt",
            "Tin Nguyen",
            "Siyuan Song",
            "Haoran Sun",
            "Shaoyue Wen",
            "Linyang He",
            "Runming Wang",
            "Yanzhi Wang",
            "Mengyue Yang",
            "Ziqiao Ma",
            "Raphaël Millière",
            "Freda Shi",
            "Nuno Vasconcelos",
            "Daniel Khashabi",
            "Alan Yuille",
            "Yilun Du",
            "Ziming Liu",
            "Bo Li",
            "Dahua Lin",
            "Ziwei Liu",
            "Vikash Kumar",
            "Yijiang Li",
            "Lei Yang",
            "Zhongang Cai",
            "Hokin Deng"
          ],
          "published": "2026-02-23T18:59:41Z",
          "updated": "2026-02-24T17:59:15Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20159v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20150v1",
          "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
          "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.",
          "authors": [
            "Wei-Cheng Huang",
            "Jiaheng Han",
            "Xiaohan Ye",
            "Zherong Pan",
            "Kris Hauser"
          ],
          "published": "2026-02-23T18:58:24Z",
          "updated": "2026-02-23T18:58:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20150v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20231v1",
          "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models",
          "summary": "Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.",
          "authors": [
            "Manish Kumar Govind",
            "Dominick Reilly",
            "Pu Wang",
            "Srijan Das"
          ],
          "published": "2026-02-23T18:41:41Z",
          "updated": "2026-02-23T18:41:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20231v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20119v1",
          "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
          "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
          "authors": [
            "Jiahui Fu",
            "Junyu Nan",
            "Lingfeng Sun",
            "Hongyu Li",
            "Jianing Qian",
            "Jennifer L. Barry",
            "Kris Kitani",
            "George Konidaris"
          ],
          "published": "2026-02-23T18:35:18Z",
          "updated": "2026-02-23T18:35:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20119v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20076v1",
          "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems",
          "summary": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods.",
          "authors": [
            "Wei Xiao",
            "Christos Cassandras",
            "Anni Li"
          ],
          "published": "2026-02-23T17:40:05Z",
          "updated": "2026-02-23T17:40:05Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20076v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20060v1",
          "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
          "summary": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.",
          "authors": [
            "Junli Wang",
            "Xueyi Liu",
            "Yinan Zheng",
            "Zebing Xing",
            "Pengfei Li",
            "Guang Li",
            "Kun Ma",
            "Guang Chen",
            "Hangjun Ye",
            "Zhongpu Xia",
            "Long Chen",
            "Qichao Zhang"
          ],
          "published": "2026-02-23T17:17:26Z",
          "updated": "2026-02-23T17:17:26Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20060v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20057v1",
          "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
          "summary": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.",
          "authors": [
            "Ge Yuan",
            "Qiyuan Qiao",
            "Jing Zhang",
            "Dong Xu"
          ],
          "published": "2026-02-23T17:12:25Z",
          "updated": "2026-02-23T17:12:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20057v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20055v1",
          "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
          "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
          "authors": [
            "Apoorva Vashisth",
            "Manav Kulshrestha",
            "Pranav Bakshi",
            "Damon Conover",
            "Guillaume Sartoretti",
            "Aniket Bera"
          ],
          "published": "2026-02-23T17:10:00Z",
          "updated": "2026-02-23T17:10:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20055v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20054v1",
          "title": "Hydrodynamic Performance Enhancement of Unmanned Underwater Gliders with Soft Robotic Morphing Wings for Agility Improvement",
          "summary": "This work assesses the hydrodynamic efficiency of Underwater Unmanned Vehicles (UUVs) equipped with soft morphing wings compared to conventional rigid wings. Unlike rigid wings, deformable counterparts can alter their aerodynamic properties on demand. Improvements in hydrodynamic efficiency extend a UUV's operational range and may determine mission feasibility. Structural and Computational Fluid Dynamics (CFD) simulations were conducted for both a soft morphing wing and a UUV incorporating it. The results show that a UUV employing soft wings achieves 9.75 percent higher overall efficiency than an equivalent vehicle with traditional rigid wings. These findings confirm the potential of soft robotics to enhance underwater vehicle performance, particularly in applications requiring pressure-agnostic operation.",
          "authors": [
            "A. Giordano",
            "G. De Meurichy",
            "V. Telazzi",
            "C. Mucignat",
            "I. Lunati",
            "D. A. L. M. Louchard",
            "M. Iovieno",
            "S. F. Armanini",
            "M. Kovac"
          ],
          "published": "2026-02-23T17:04:21Z",
          "updated": "2026-02-23T17:04:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20054v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20041v1",
          "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover",
          "summary": "Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) signals were recorded with a 16-channel OpenBCI cap and aligned with motor actions at Delta = 0 ms and future prediction horizons (Delta > 0 ms). After preprocessing, several deep learning models were benchmarked, including convolutional neural networks, recurrent neural networks, and Transformer architectures. ShallowConvNet achieved the highest performance for both action prediction and intent prediction. By combining real-world robotic control with multi-horizon EEG intention decoding, this study introduces a reproducible benchmark and reveals key design insights for predictive deep learning-based BCI systems.",
          "authors": [
            "Ghadah Alosaimi",
            "Maha Alsayyari",
            "Yixin Sun",
            "Stamos Katsigiannis",
            "Amir Atapour-Abarghouei",
            "Toby P. Breckon"
          ],
          "published": "2026-02-23T16:50:21Z",
          "updated": "2026-02-23T16:50:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20041v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20225v1",
          "title": "FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators",
          "summary": "This paper introduces Function-space Adaptive Constrained Trajectory Optimization (FACTO), a new trajectory optimization algorithm for both single- and multi-arm manipulators. Trajectory representations are parameterized as linear combinations of orthogonal basis functions, and optimization is performed directly in the coefficient space. The constrained problem formulation consists of both an objective functional and a finite-dimensional objective defined over truncated coefficients. To address nonlinearity, FACTO uses a Gauss-Newton approximation with exponential moving averaging, yielding a smoothed quadratic subproblem. Trajectory-wide constraints are addressed using coefficient-space mappings, and an adaptive constrained update using the Levenberg-Marquardt algorithm is performed in the null space of active constraints. Comparisons with optimization-based planners (CHOMP, TrajOpt, GPMP2) and sampling-based planners (RRT-Connect, RRT*, PRM) show the improved solution quality and feasibility, especially in constrained single- and multi-arm scenarios. The experimental evaluation of FACTO on Franka robots verifies the feasibility of deployment.",
          "authors": [
            "Yichang Feng",
            "Xiao Liang",
            "Minghui Zheng"
          ],
          "published": "2026-02-23T16:38:13Z",
          "updated": "2026-02-23T16:38:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20225v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19983v1",
          "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
          "summary": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.",
          "authors": [
            "Zachary Ravichadran",
            "David Snyder",
            "Alexander Robey",
            "Hamed Hassani",
            "Vijay Kumar",
            "George J. Pappas"
          ],
          "published": "2026-02-23T15:51:23Z",
          "updated": "2026-02-23T15:51:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19983v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19983v2",
          "title": "Contextual Safety Reasoning and Grounding for Open-World Robots",
          "summary": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.",
          "authors": [
            "Zachary Ravichandran",
            "David Snyder",
            "Alexander Robey",
            "Hamed Hassani",
            "Vijay Kumar",
            "George J. Pappas"
          ],
          "published": "2026-02-23T15:51:23Z",
          "updated": "2026-02-24T20:23:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19983v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19943v1",
          "title": "Scaling Law of Neural Koopman Operators",
          "summary": "Data-driven neural Koopman operator theory has emerged as a powerful tool for linearizing and controlling nonlinear robotic systems. However, the performance of these data-driven models fundamentally depends on the trade-off between sample size and model dimensions, a relationship for which the scaling laws have remained unclear. This paper establishes a rigorous framework to address this challenge by deriving and empirically validating scaling laws that connect sample size, latent space dimension, and downstream control quality. We derive a theoretical upper bound on the Koopman approximation error, explicitly decomposing it into sampling error and projection error. We show that these terms decay at specific rates relative to dataset size and latent dimension, providing a rigorous basis for the scaling law. Based on the theoretical results, we introduce two lightweight regularizers for the neural Koopman operator: a covariance loss to help stabilize the learned latent features and an inverse control loss to ensure the model aligns with physical actuation. The results from systematic experiments across six robotic environments confirm that model fitting error follows the derived scaling laws, and the regularizers improve dynamic model fitting fidelity, with enhanced closed-loop control performance. Together, our results provide a simple recipe for allocating effort between data collection and model capacity when learning Koopman dynamics for control.",
          "authors": [
            "Abulikemu Abuduweili",
            "Yuyang Pang",
            "Feihan Li",
            "Changliu Liu"
          ],
          "published": "2026-02-23T15:13:43Z",
          "updated": "2026-02-23T15:13:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19943v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19917v1",
          "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning",
          "summary": "Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \\TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.",
          "authors": [
            "Thanh Nguyen",
            "Tung Luu",
            "Tri Ton",
            "Sungwoong Kim",
            "Chang D. Yoo"
          ],
          "published": "2026-02-23T14:57:52Z",
          "updated": "2026-02-23T14:57:52Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19917v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19898v1",
          "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform",
          "summary": "In disaster response and situation assessment, robots have great potential in reducing the risks to the safety and health of first responders. As the situations encountered and the required capabilities of the robots deployed in such missions differ wildly and are often not known in advance, heterogeneous fleets of robots are needed to cover a wide range of mission requirements. While UAVs can quickly survey the mission environment, their ability to carry heavy payloads such as sensors and manipulators is limited. UGVs can carry required payloads to assess and manipulate the mission environment, but need to be able to deal with difficult and unstructured terrain such as rubble and stairs. The ability of tracked platforms with articulated arms (flippers) to reconfigure their geometry makes them particularly effective for navigating challenging terrain. In this paper, we present Athena, an open-hardware rescue ground robot research platform with four individually reconfigurable flippers and a reliable low-cost remote emergency stop (E-Stop) solution. A novel mounting solution using an industrial PU belt and tooth inserts allows the replacement and testing of different track profiles. The manipulator with a maximum reach of 1.54m can be used to operate doors, valves, and other objects of interest. Full CAD & PCB files, as well as all low-level software, are released as open-source contributions.",
          "authors": [
            "Stefan Fabian",
            "Aljoscha Schmidt",
            "Jonas Süß",
            "Dishant",
            "Aum Oza",
            "Oskar von Stryk"
          ],
          "published": "2026-02-23T14:38:23Z",
          "updated": "2026-02-23T14:38:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19898v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19862v1",
          "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems",
          "summary": "In-Motion physical coupling of multiple mobile ground robots has the potential to enable new applications like in-motion transfer that improves efficiency in handling and transferring goods, which tackles current challenges in logistics. A key challenge lies in achieving reliable autonomous in-motion physical coupling of two mobile ground robots starting at any initial position. Existing approaches neglect the modeling of the docking interface and the strategy for approaching it, resulting in uncontrolled collisions that make in-motion physical coupling either impossible or inefficient. To address this challenge, we propose a central mpc approach that explicitly models the dynamics and states of two omnidirectional wheeled robots, incorporates constraints related to their docking interface, and implements an approaching strategy for rendezvous and docking. This novel approach enables omnidirectional wheeled robots with a docking interface to physically couple in motion regardless of their initial position. In addition, it makes in-motion transfer possible, which is 19.75% more time- and 21.04% energy-efficient compared to a non-coupling approach in a logistic scenario.",
          "authors": [
            "Lars Fischer",
            "Daniel Flögel",
            "Sören Hohmann"
          ],
          "published": "2026-02-23T14:01:37Z",
          "updated": "2026-02-23T14:01:37Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19862v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19850v1",
          "title": "TactiVerse: Generalizing Multi-Point Tactile Sensing in Soft Robotics Using Single-Point Data",
          "summary": "Real-time prediction of deformation in highly compliant soft materials remains a significant challenge in soft robotics. While vision-based soft tactile sensors can track internal marker displacements, learning-based models for 3D contact estimation heavily depend on their training datasets, inherently limiting their ability to generalize to complex scenarios such as multi-point sensing. To address this limitation, we introduce TactiVerse, a U-Net-based framework that formulates contact geometry estimation as a spatial heatmap prediction task. Even when trained exclusively on a limited dataset of single-point indentations, our architecture achieves highly accurate single-point sensing, yielding a superior mean absolute error of 0.0589 mm compared to the 0.0612 mm of a conventional regression-based CNN baseline. Furthermore, we demonstrate that augmenting the training dataset with multi-point contact data substantially enhances the sensor's multi-point sensing capabilities, significantly improving the overall mean MAE for two-point discrimination from 1.214 mm to 0.383 mm. By successfully extrapolating complex contact geometries from fundamental interactions, this methodology unlocks advanced multi-point and large-area shape sensing. Ultimately, it significantly streamlines the development of marker-based soft sensors, offering a highly scalable solution for real-world tactile perception.",
          "authors": [
            "Junhui Lee",
            "Hyosung Kim",
            "Saekwang Nam"
          ],
          "published": "2026-02-23T13:53:14Z",
          "updated": "2026-02-23T13:53:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19850v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19764v1",
          "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling",
          "summary": "Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions.",
          "authors": [
            "Yirui Sun",
            "Guangyu Zhuge",
            "Keliang Liu",
            "Jie Gu",
            "Zhihao xia",
            "Qionglin Ren",
            "Chunxu tian",
            "Zhongxue Ga"
          ],
          "published": "2026-02-23T12:12:51Z",
          "updated": "2026-02-23T12:12:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19764v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19710v1",
          "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
          "summary": "Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns. To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision. Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.",
          "authors": [
            "Haitao Lin",
            "Hanyang Yu",
            "Jingshun Huang",
            "He Zhang",
            "Yonggen Ling",
            "Ping Tan",
            "Xiangyang Xue",
            "Yanwei Fu"
          ],
          "published": "2026-02-23T11:00:08Z",
          "updated": "2026-02-23T11:00:08Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19710v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19699v1",
          "title": "CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization",
          "summary": "Trajectory Optimization (TO) and Reinforcement Learning (RL) offer complementary strengths for solving optimal control problems. TO efficiently computes locally optimal solutions but can struggle with non-convexity, while RL is more robust to non-convexity at the cost of significantly higher computational demands. CACTO (Continuous Actor-Critic with Trajectory Optimization) was introduced to combine these advantages by learning a warm-start policy that guides the TO solver towards low-cost trajectories. However, scalability remains a key limitation, as increasing system complexity significantly raises the computational cost of TO. This work introduces CACTO-BIC to address these challenges. CACTO-BIC improves data efficiency by biasing initial-state sampling leveraging a property of the value function associated with locally optimal policies; moreover, it reduces computation time by exploiting GPU acceleration. Empirical evaluations show improved sample efficiency and faster computation compared to CACTO. Comparisons with PPO demonstrate that our approach can achieve similar solutions in less time. Finally, experiments on the AlienGO quadruped robot demonstrate that CACTO-BIC can scale to high-dimensional systems and is suitable for real-time applications.",
          "authors": [
            "Elisa Alboni",
            "Pietro Noah Crestaz",
            "Elias Fontanari",
            "Andrea Del Prete"
          ],
          "published": "2026-02-23T10:45:36Z",
          "updated": "2026-02-23T10:45:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19699v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19697v1",
          "title": "BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU",
          "summary": "Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however, they depend on heuristic weighting and fail to transparently convey uncertainty in a systematic way. Recent neural implicit methods, on the other hand, get very high fidelity but usually need a lot of GPU power for optimization and aren't very easy to understand for making decisions later on. This work presents BayesFusion-SDF, a CPU-centric probabilistic signed distance fusion framework that conceptualizes geometry as a sparse Gaussian random field with a defined posterior distribution over voxel distances. First, a rough TSDF reconstruction is used to create an adaptive narrow-band domain. Then, depth observations are combined using a heteroscedastic Bayesian formulation that is solved using sparse linear algebra and preconditioned conjugate gradients. Randomized diagonal estimators are a quick way to get an idea of posterior uncertainty. This makes it possible to extract surfaces and plan the next best view while taking into account uncertainty. Tests on a controlled ablation scene and a CO3D object sequence show that the new method is more accurate geometrically than TSDF baselines and gives useful estimates of uncertainty for active sensing. The proposed formulation provides a clear and easy-to-use alternative to GPU-heavy neural reconstruction methods while still being able to be understood in a probabilistic way and acting in a predictable way. GitHub: https://mazumdarsoumya.github.io/BayesFusionSDF",
          "authors": [
            "Soumya Mazumdar",
            "Vineet Kumar Rakesh",
            "Tapas Samanta"
          ],
          "published": "2026-02-23T10:44:15Z",
          "updated": "2026-02-23T10:44:15Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.GR",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19697v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20220v1",
          "title": "What Matters for Simulation to Online Reinforcement Learning on Real Robots",
          "summary": "We investigate what specific design choices enable successful online reinforcement learning (RL) on physical robots. Across 100 real-world training runs on three distinct robotic platforms, we systematically ablate algorithmic, systems, and experimental decisions that are typically left implicit in prior work. We find that some widely used defaults can be harmful, while a set of robust, readily adopted design choices within standard RL practice yield stable learning across tasks and hardware. These results provide the first large-sample empirical study of such design choices, enabling practitioners to deploy online RL with lower engineering effort.",
          "authors": [
            "Yarden As",
            "Dhruva Tirumala",
            "René Zurbrügg",
            "Chenhao Li",
            "Stelian Coros",
            "Andreas Krause",
            "Markus Wulfmeier"
          ],
          "published": "2026-02-23T10:34:15Z",
          "updated": "2026-02-23T10:34:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20220v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19653v1",
          "title": "Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array",
          "summary": "Distributed Manipulator Systems, composed of arrays of robotic actuators necessitate dense actuator arrays to effectively manipulate small objects. This paper presents a system composed of modular 3-DoF robotic tiles interconnected by a compliant surface layer, forming a continuous, controllable manipulation surface. The compliant layer permits increased actuator spacing without compromising object manipulation capabilities, significantly reducing actuator density while maintaining robust control, even for smaller objects. We characterize the coupled workspace of the array and develop a manipulation strategy capable of translating objects to arbitrary positions within an N X N array. The approach is validated experimentally using a minimal 2 X 2 prototype, demonstrating the successful manipulation of objects with varied shapes and sizes.",
          "authors": [
            "Bailey Dacre",
            "Rodrigo Moreno",
            "Jørn Lambertsen",
            "Kasper Stoy",
            "Andrés Faíña"
          ],
          "published": "2026-02-23T09:54:32Z",
          "updated": "2026-02-23T09:54:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19653v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19651v1",
          "title": "Denoising Particle Filters: Learning State Estimation with Single-Step Objectives",
          "summary": "Learning-based methods commonly treat state estimation in robotics as a sequence modeling problem. While this paradigm can be effective at maximizing end-to-end performance, models are often difficult to interpret and expensive to train, since training requires unrolling sequences of predictions in time. As an alternative to end-to-end trained state estimation, we propose a novel particle filtering algorithm in which models are trained from individual state transitions, fully exploiting the Markov property in robotic systems. In this framework, measurement models are learned implicitly by minimizing a denoising score matching objective. At inference, the learned denoiser is used alongside a (learned) dynamics model to approximately solve the Bayesian filtering equation at each time step, effectively guiding predicted states toward the data manifold informed by measurements. We evaluate the proposed method on challenging robotic state estimation tasks in simulation, demonstrating competitive performance compared to tuned end-to-end trained baselines. Importantly, our method offers the desirable composability of classical filtering algorithms, allowing prior information and external sensor models to be incorporated without retraining.",
          "authors": [
            "Lennart Röstel",
            "Berthold Bäuml"
          ],
          "published": "2026-02-23T09:53:23Z",
          "updated": "2026-02-23T09:53:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19651v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20219v1",
          "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction",
          "summary": "Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, speech processing, and fuzzy logic to enable precise and adaptive control of a Dobot Magician robotic arm. The proposed system integrates Florence-2 for object detection, Llama 3.1 for natural language understanding, and Whisper for speech recognition, providing users with a seamless and intuitive interface for object manipulation through spoken commands. By jointly addressing scene perception and action planning, the approach enhances the reliability of command interpretation and execution. Experimental evaluations conducted on consumer-grade hardware demonstrate a command execution accuracy of 75\\%, highlighting both the robustness and adaptability of the system. Beyond its current performance, the proposed architecture serves as a flexible and extensible foundation for future HRI research, offering a practical pathway toward more sophisticated and natural human-robot collaboration through tightly coupled speech and vision-language processing.",
          "authors": [
            "Guanting Shen",
            "Zi Tian"
          ],
          "published": "2026-02-23T09:05:15Z",
          "updated": "2026-02-23T09:05:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20219v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20216v1",
          "title": "Sample-Efficient Learning with Online Expert Correction for Autonomous Catheter Steering in Endovascular Bifurcation Navigation",
          "summary": "Robot-assisted endovascular intervention offers a safe and effective solution for remote catheter manipulation, reducing radiation exposure while enabling precise navigation. Reinforcement learning (RL) has recently emerged as a promising approach for autonomous catheter steering; however, conventional methods suffer from sparse reward design and reliance on static vascular models, limiting their sample efficiency and generalization to intraoperative variations. To overcome these challenges, this paper introduces a sample-efficient RL framework with online expert correction for autonomous catheter steering in endovascular bifurcation navigation. The proposed framework integrates three key components: (1) A segmentation-based pose estimation module for accurate real-time state feedback, (2) A fuzzy controller for bifurcation-aware orientation adjustment, and (3) A structured reward generator incorporating expert priors to guide policy learning. By leveraging online expert correction, the framework reduces exploration inefficiency and enhances policy robustness in complex vascular structures. Experimental validation on a robotic platform using a transparent vascular phantom demonstrates that the proposed approach achieves convergence in 123 training episodes -- a 25.9% reduction compared to the baseline Soft Actor-Critic (SAC) algorithm -- while reducing average positional error to 83.8% of the baseline. These results indicate that combining sample-efficient RL with online expert correction enables reliable and accurate catheter steering, particularly in anatomically challenging bifurcation scenarios critical for endovascular navigation.",
          "authors": [
            "Hao Wang",
            "Tianliang Yao",
            "Bo Lu",
            "Zhiqiang Pei",
            "Liu Dong",
            "Lei Ma",
            "Peng Qi"
          ],
          "published": "2026-02-23T07:58:08Z",
          "updated": "2026-02-23T07:58:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20216v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19577v1",
          "title": "Chasing Ghosts: A Simulation-to-Real Olfactory Navigation Stack with Optional Vision Augmentation",
          "summary": "Autonomous odor source localization remains a challenging problem for aerial robots due to turbulent airflow, sparse and delayed sensory signals, and strict payload and compute constraints. While prior unmanned aerial vehicle (UAV)-based olfaction systems have demonstrated gas distribution mapping or reactive plume tracing, they rely on predefined coverage patterns, external infrastructure, or extensive sensing and coordination. In this work, we present a complete, open-source UAV system for online odor source localization using a minimal sensor suite. The system integrates custom olfaction hardware, onboard sensing, and a learning-based navigation policy trained in simulation and deployed on a real quadrotor. Through our minimal framework, the UAV is able to navigate directly toward an odor source without constructing an explicit gas distribution map or relying on external positioning systems. Vision is incorporated as an optional complementary modality to accelerate navigation under certain conditions. We validate the proposed system through real-world flight experiments in a large indoor environment using an ethanol source, demonstrating consistent source-finding behavior under realistic airflow conditions. The primary contribution of this work is a reproducible system and methodological framework for UAV-based olfactory navigation and source finding under minimal sensing assumptions. We elaborate on our hardware design and open source our UAV firmware, simulation code, olfaction-vision dataset, and circuit board to the community. Code, data, and designs will be made available at https://github.com/KordelFranceTech/ChasingGhosts.",
          "authors": [
            "Kordel K. France",
            "Ovidiu Daescu",
            "Latifur Khan",
            "Rohith Peddi"
          ],
          "published": "2026-02-23T07:51:27Z",
          "updated": "2026-02-23T07:51:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19577v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20215v1",
          "title": "Vision-Based Reasoning with Topology-Encoded Graphs for Anatomical Path Disambiguation in Robot-Assisted Endovascular Navigation",
          "summary": "Robotic-assisted percutaneous coronary intervention (PCI) is constrained by the inherent limitations of 2D Digital Subtraction Angiography (DSA). Unlike physicians, who can directly manipulate guidewires and integrate tactile feedback with their prior anatomical knowledge, teleoperated robotic systems must rely solely on 2D projections. This mode of operation, simultaneously lacking spatial context and tactile sensation, may give rise to projection-induced ambiguities at vascular bifurcations. To address this challenge, we propose a two-stage framework (SCAR-UNet-GAT) for real-time robotic path planning. In the first stage, SCAR-UNet, a spatial-coordinate-attention-regularized U-Net, is employed for accurate coronary vessel segmentation. The integration of multi-level attention mechanisms enhances the delineation of thin, tortuous vessels and improves robustness against imaging noise. From the resulting binary masks, vessel centerlines and bifurcation points are extracted, and geometric descriptors (e.g., branch diameter, intersection angles) are fused with local DSA patches to construct node features. In the second stage, a Graph Attention Network (GAT) reasons over the vessel graph to identify anatomically consistent and clinically feasible trajectories, effectively distinguishing true bifurcations from projection-induced false crossings. On a clinical DSA dataset, SCAR-UNet achieved a Dice coefficient of 93.1%. For path disambiguation, the proposed GAT-based method attained a success rate of 95.0% and a target-arrival success rate of 90.0%, substantially outperforming conventional shortest-path planning (60.0% and 55.0%) and heuristic-based planning (75.0% and 70.0%). Validation on a robotic platform further confirmed the practical feasibility and robustness of the proposed framework.",
          "authors": [
            "Jiyuan Zhao",
            "Zhengyu Shi",
            "Wentong Tian",
            "Tianliang Yao",
            "Dong Liu",
            "Tao Liu",
            "Yizhe Wu",
            "Peng Qi"
          ],
          "published": "2026-02-23T07:39:55Z",
          "updated": "2026-02-23T07:39:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20215v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19538v1",
          "title": "Cost-Aware Diffusion Active Search",
          "summary": "Active search for recovering objects of interest through online, adaptive decision making with autonomous agents requires trading off exploration of unknown environments with exploitation of prior observations in the search space. Prior work has proposed information gain and Thompson sampling based myopic, greedy approaches for agents to actively decide query or search locations when the number of targets is unknown. Decision making algorithms in such partially observable environments have also shown that agents capable of lookahead over a finite horizon outperform myopic policies for active search. Unfortunately, lookahead algorithms typically rely on building a computationally expensive search tree that is simulated and updated based on the agent's observations and a model of the environment dynamics. Instead, in this work, we leverage the sequence modeling abilities of diffusion models to sample lookahead action sequences that balance the exploration-exploitation trade-off for active search without building an exhaustive search tree. We identify the optimism bias in prior diffusion based reinforcement learning approaches when applied to the active search setting and propose mitigating solutions for efficient cost-aware decision making with both single and multi-agent teams. Our proposed algorithm outperforms standard baselines in offline reinforcement learning in terms of full recovery rate and is computationally more efficient than tree search in cost-aware active decision making.",
          "authors": [
            "Arundhati Banerjee",
            "Jeff Schneider"
          ],
          "published": "2026-02-23T06:11:51Z",
          "updated": "2026-02-23T06:11:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19538v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19534v1",
          "title": "Large Language Model-Assisted UAV Operations and Communications: A Multifaceted Survey and Tutorial",
          "summary": "Uncrewed Aerial Vehicles (UAVs) are widely deployed across diverse applications due to their mobility and agility. Recent advances in Large Language Models (LLMs) offer a transformative opportunity to enhance UAV intelligence beyond conventional optimization-based and learning-based approaches. By integrating LLMs into UAV systems, advanced environmental understanding, swarm coordination, mobility optimization, and high-level task reasoning can be achieved, thereby allowing more adaptive and context-aware aerial operations. This survey systematically explores the intersection of LLMs and UAV technologies and proposes a unified framework that consolidates existing architectures, methodologies, and applications for UAVs. We first present a structured taxonomy of LLM adaptation techniques for UAVs, including pretraining, fine-tuning, Retrieval-Augmented Generation (RAG), and prompt engineering, along with key reasoning capabilities such as Chain-of-Thought (CoT) and In-Context Learning (ICL). We then examine LLM-assisted UAV communications and operations, covering navigation, mission planning, swarm control, safety, autonomy, and network management. After that, the survey further discusses Multimodal LLMs (MLLMs) for human-swarm interaction, perception-driven navigation, and collaborative control. Finally, we address ethical considerations, including bias, transparency, accountability, and Human-in-the-Loop (HITL) strategies, and outline future research directions. Overall, this work positions LLM-assisted UAVs as a foundation for intelligent and adaptive aerial systems.",
          "authors": [
            "Yousef Emami",
            "Hao Zhou",
            "Radha Reddy",
            "Atefeh Hajijamali Arani",
            "Biliang Wang",
            "Kai Li",
            "Luis Almeida",
            "Zhu Han"
          ],
          "published": "2026-02-23T05:56:43Z",
          "updated": "2026-02-23T05:56:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19534v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19532v1",
          "title": "Bellman Value Decomposition for Task Logic in Safe Optimal Control",
          "summary": "Real-world tasks involve nuanced combinations of goal and safety specifications. In high dimensions, the challenge is exacerbated: formal automata become cumbersome, and the combination of sparse rewards tends to require laborious tuning. In this work, we consider the innate structure of the Bellman Value as a means to naturally organize the problem for improved automatic performance. Namely, we prove the Bellman Value for a complex task defined in temporal logic can be decomposed into a graph of Bellman Values, connected by a set of well-known Bellman equations (BEs): the Reach-Avoid BE, the Avoid BE, and a novel type, the Reach-Avoid-Loop BE. To solve the Value and optimal policy, we propose VDPPO, which embeds the decomposed Value graph into a two-layer neural net, bootstrapping the implicit dependencies. We conduct a variety of simulated and hardware experiments to test our method on complex, high-dimensional tasks involving heterogeneous teams and nonlinear dynamics. Ultimately, we find this approach greatly improves performance over existing baselines, balancing safety and liveness automatically.",
          "authors": [
            "William Sharpless",
            "Oswin So",
            "Dylan Hirsch",
            "Sylvia Herbert",
            "Chuchu Fan"
          ],
          "published": "2026-02-23T05:48:58Z",
          "updated": "2026-02-23T05:48:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19532v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19518v1",
          "title": "Anticipate, Adapt, Act: A Hybrid Framework for Task Planning",
          "summary": "Anticipating and adapting to failures is a key capability robots need to collaborate effectively with humans in complex domains. This continues to be a challenge despite the impressive performance of state of the art AI planning systems and Large Language Models (LLMs) because of the uncertainty associated with the tasks and their outcomes. Toward addressing this challenge, we present a hybrid framework that integrates the generic prediction capabilities of an LLM with the probabilistic sequential decision-making capability of Relational Dynamic Influence Diagram Language. For any given task, the robot reasons about the task and the capabilities of the human attempting to complete it; predicts potential failures due to lack of ability (in the human) or lack of relevant domain objects; and executes actions to prevent such failures or recover from them. Experimental evaluation in the VirtualHome 3D simulation environment demonstrates substantial improvement in performance compared with state of the art baselines.",
          "authors": [
            "Nabanita Dash",
            "Ayush Kaura",
            "Shivam Singh",
            "Ramandeep Singh",
            "Snehasis Banerjee",
            "Mohan Sridharan",
            "K. Madhava Krishna"
          ],
          "published": "2026-02-23T05:18:11Z",
          "updated": "2026-02-23T05:18:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19518v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19491v1",
          "title": "Botson: An Accessible and Low-Cost Platform for Social Robotics Research",
          "summary": "Trust remains a critical barrier to the effective integration of Artificial Intelligence (AI) into human-centric domains. Disembodied agents, such as voice assistants, often fail to establish trust due to their inability to convey non-verbal social cues. This paper introduces the architecture of Botson: an anthropomorphic social robot powered by a large language model (LLM). Botson was created as a low-cost and accessible platform for social robotics research.",
          "authors": [
            "Samuel Bellaire",
            "Abdalmalek Abu-raddaha",
            "Natalie Kim",
            "Nathan Morhan",
            "William Elliott",
            "Samir Rawashdeh"
          ],
          "published": "2026-02-23T04:21:05Z",
          "updated": "2026-02-23T04:21:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19491v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19422v1",
          "title": "Positioning Modular Co-Design in Future HRI Design Research",
          "summary": "Design-oriented HRI is increasingly interested in robots as long-term companions, yet many designs still assume a fixed form and a stable set of functions. We present an ongoing design research program that treats modularity as a designerly medium - a way to make long-term human-robot relationships discussable and material through co-design. Across a series of lifespan-oriented co-design activities, participants repeatedly reconfigured the same robot for different life stages, using modular parts to express changing needs, values, and roles. From these outcomes, we articulate PAS (Personalization-Adaptability-Sustainability) as a human-centered lens on how people enact modularity in practice: configuring for self-expression, adapting across transitions, and sustaining robots through repair, reuse, and continuity. We then sketch next steps toward a fabrication-aware, community-extensible modular platform and propose evaluation criteria for designerly HRI work that prioritize expressive adequacy, lifespan plausibility, repairability-in-use, and responsible stewardship - not only usability or performance.",
          "authors": [
            "Lingyun Chen",
            "Qing Xiao",
            "Zitao Zhang",
            "Eli Blevis",
            "Selma Šabanović"
          ],
          "published": "2026-02-23T01:29:39Z",
          "updated": "2026-02-23T01:29:39Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19422v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19400v1",
          "title": "Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration",
          "summary": "We present a coverage framework that integrates Hilbert space-filling priors into decentralized multi-robot learning and execution. We augment DQN and PPO with Hilbert-based spatial indices to structure exploration and reduce redundancy in sparse-reward environments, and we evaluate scalability in multi-robot grid coverage. We further describe a waypoint interface that converts Hilbert orderings into curvature-bounded, time-parameterized SE(2) trajectories (planar (x, y, θ)), enabling onboard feasibility on resource-constrained robots. Experiments show improvements in coverage efficiency, redundancy, and convergence speed over DQN/PPO baselines. In addition, we validate the approach on a Boston Dynamics Spot legged robot, executing the generated trajectories in indoor environments and observing reliable coverage with low redundancy. These results indicate that geometric priors improve autonomy and scalability for swarm and legged robotics.",
          "authors": [
            "Tamil Selvan Gurunathan",
            "Aryya Gangopadhyay"
          ],
          "published": "2026-02-23T00:19:19Z",
          "updated": "2026-02-23T00:19:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19400v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19372v1",
          "title": "Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization",
          "summary": "Solving complex, long-horizon robotic manipulation tasks requires a deep understanding of physical interactions, reasoning about their long-term consequences, and precise high-level planning. Vision-Language Models (VLMs) offer a general perceive-reason-act framework for this goal. However, previous approaches using reflective planning to guide VLMs in correcting actions encounter significant limitations. These methods rely on inefficient and often inaccurate implicit learning of state-values from noisy foresight predictions, evaluate only a single greedy future, and suffer from substantial inference latency. To address these limitations, we propose a novel test-time computation framework that decouples state evaluation from action generation. This provides a more direct and fine-grained supervisory signal for robust decision-making. Our method explicitly models the advantage of an action plan, quantified by its reduction in distance to the goal, and uses a scalable critic to estimate. To address the stochastic nature of single-trajectory evaluation, we employ beam search to explore multiple future paths and aggregate them during decoding to model their expected long-term returns, leading to more robust action generation. Additionally, we introduce a lightweight, confidence-based trigger that allows for early exit when direct predictions are reliable, invoking reflection only when necessary. Extensive experiments on diverse, unseen multi-stage robotic manipulation tasks demonstrate a 24.6% improvement in success rate over state-of-the-art baselines, while significantly reducing inference time by 56.5%.",
          "authors": [
            "Yanting Yang",
            "Shenyuan Gao",
            "Qingwen Bu",
            "Li Chen",
            "Dimitris N. Metaxas"
          ],
          "published": "2026-02-22T22:53:16Z",
          "updated": "2026-02-22T22:53:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19372v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19366v1",
          "title": "Self-Configurable Mesh-Networks for Scalable Distributed Submodular Bandit Optimization",
          "summary": "We study how to scale distributed bandit submodular coordination under realistic communication constraints in bandwidth, data rate, and connectivity. We are motivated by multi-agent tasks of active situational awareness in unknown, partially-observable, and resource-limited environments, where the agents must coordinate through agent-to-agent communication. Our approach enables scalability by (i) limiting information relays to only one-hop communication and (ii) keeping inter-agent messages small, having each agent transmit only its own action information. Despite these information-access restrictions, our approach enables near-optimal action coordination by optimizing the agents' communication neighborhoods over time, through distributed online bandit optimization, subject to the agents' bandwidth constraints. Particularly, our approach enjoys an anytime suboptimality bound that is also strictly positive for arbitrary network topologies, even disconnected. To prove the bound, we define the Value of Coordination (VoC), an information-theoretic metric that quantifies for each agent the benefit of information access to its neighbors. We validate in simulations the scalability and near-optimality of our approach: it is observed to converge faster, outperform benchmarks for bandit submodular coordination, and can even outperform benchmarks that are privileged with a priori knowledge of the environment.",
          "authors": [
            "Zirui Xu",
            "Vasileios Tzoumas"
          ],
          "published": "2026-02-22T22:36:37Z",
          "updated": "2026-02-22T22:36:37Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.MA",
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19366v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19359v1",
          "title": "Vid2Sid: Videos Can Help Close the Sim2Real Gap",
          "summary": "Calibrating a robot simulator's physics parameters (friction, damping, material stiffness) to match real hardware is often done by hand or with black-box optimizers that reduce error but cannot explain which physical discrepancies drive the error. When sensing is limited to external cameras, the problem is further compounded by perception noise and the absence of direct force or state measurements. We present Vid2Sid, a video-driven system identification pipeline that couples foundation-model perception with a VLM-in-the-loop optimizer that analyzes paired sim-real videos, diagnoses concrete mismatches, and proposes physics parameter updates with natural language rationales. We evaluate our approach on a tendon-actuated finger (rigid-body dynamics in MuJoCo) and a deformable continuum tentacle (soft-body dynamics in PyElastica). On sim2real holdout controls unseen during training, Vid2Sid achieves the best average rank across all settings, matching or exceeding black-box optimizers while uniquely providing interpretable reasoning at each iteration. Sim2sim validation confirms that Vid2Sid recovers ground-truth parameters most accurately (mean relative error under 13\\% vs. 28--98\\%), and ablation analysis reveals three calibration regimes. VLM-guided optimization excels when perception is clean and the simulator is expressive, while model-class limitations bound performance in more challenging settings.",
          "authors": [
            "Kevin Qiu",
            "Yu Zhang",
            "Marek Cygan",
            "Josie Hughes"
          ],
          "published": "2026-02-22T22:08:16Z",
          "updated": "2026-02-22T22:08:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19359v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19346v1",
          "title": "Design and Control of Modular Magnetic Millirobots for Multimodal Locomotion and Shape Reconfiguration",
          "summary": "Modular small-scale robots offer the potential for on-demand assembly and disassembly, enabling task-specific adaptation in dynamic and constrained environments. However, existing modular magnetic platforms often depend on workspace collisions for reconfiguration, employ bulky three-dimensional electromagnetic systems, and lack robust single-module control, which limits their applicability in biomedical settings. In this work, we present a modular magnetic millirobotic platform comprising three cube-shaped modules with embedded permanent magnets, each designed for a distinct functional role: a free module that supports self-assembly and reconfiguration, a fixed module that enables flip-and-walk locomotion, and a gripper module for cargo manipulation. Locomotion and reconfiguration are actuated by programmable combinations of time-varying two-dimensional uniform and gradient magnetic field inputs. Experiments demonstrate closed-loop navigation using real-time vision feedback and A* path planning, establishing robust single-module control capabilities. Beyond locomotion, the system achieves self-assembly, multimodal transformations, and disassembly at low field strengths. Chain-to-gripper transformations succeeded in 90% of trials, while chain-to-square transformations were less consistent, underscoring the role of module geometry in reconfiguration reliability. These results establish a versatile modular robotic platform capable of multimodal behavior and robust control, suggesting a promising pathway toward scalable and adaptive task execution in confined environments.",
          "authors": [
            "Erik Garcia Oyono",
            "Jialin Lin",
            "Dandan Zhang"
          ],
          "published": "2026-02-22T21:25:05Z",
          "updated": "2026-02-22T21:25:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19346v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19315v1",
          "title": "Online Navigation Planning for Long-term Autonomous Operation of Underwater Gliders",
          "summary": "Underwater glider robots have become an indispensable tool for ocean sampling. Although stakeholders are calling for tools to manage increasingly large fleets of gliders, successful autonomous long-term deployments have thus far been scarce, which hints at a lack of suitable methodologies and systems. In this work, we formulate glider navigation planning as a stochastic shortest-path Markov Decision Process and propose a sample-based online planner based on Monte Carlo Tree Search. Samples are generated by a physics-informed simulator that captures uncertain execution of controls and ocean current forecasts while remaining computationally tractable. The simulator parameters are fitted using historical glider data. We integrate these methods into an autonomous command-and-control system for Slocum gliders that enables closed-loop replanning at each surfacing. The resulting system was validated in two field deployments in the North Sea totalling approximately 3 months and 1000 km of autonomous operation. Results demonstrate improved efficiency compared to straight-to-goal navigation and show the practicality of sample-based planning for long-term marine autonomy.",
          "authors": [
            "Victor-Alexandru Darvariu",
            "Charlotte Z. Reed",
            "Jan Stratmann",
            "Bruno Lacerda",
            "Benjamin Allsup",
            "Stephen Woodward",
            "Elizabeth Siddle",
            "Trishna Saeharaseelan",
            "Owain Jones",
            "Dan Jones",
            "Tobias Ferreira",
            "Chloe Baker",
            "Kevin Chaplin",
            "James Kirk",
            "Ashley Morris",
            "Ryan Patmore",
            "Jeff Polton",
            "Charlotte Williams",
            "Alexandra Kokkinaki",
            "Alvaro Lorenzo Lopez",
            "Justin J. H. Buck",
            "Nick Hawes"
          ],
          "published": "2026-02-22T19:34:10Z",
          "updated": "2026-02-22T19:34:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19315v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19313v1",
          "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
          "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.",
          "authors": [
            "Shirui Chen",
            "Cole Harrison",
            "Ying-Chun Lee",
            "Angela Jin Yang",
            "Zhongzheng Ren",
            "Lillian J. Ratliff",
            "Jiafei Duan",
            "Dieter Fox",
            "Ranjay Krishna"
          ],
          "published": "2026-02-22T19:25:48Z",
          "updated": "2026-02-22T19:25:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19313v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19308v1",
          "title": "WildOS: Open-Vocabulary Object Search in the Wild",
          "summary": "Autonomous navigation in complex, unstructured outdoor environments requires robots to operate over long ranges without prior maps and limited depth sensing. In such settings, relying solely on geometric frontiers for exploration is often insufficient. In such settings, the ability to reason semantically about where to go and what is safe to traverse is crucial for robust, efficient exploration. This work presents WildOS, a unified system for long-range, open-vocabulary object search that combines safe geometric exploration with semantic visual reasoning. WildOS builds a sparse navigation graph to maintain spatial memory, while utilizing a foundation-model-based vision module, ExploRFM, to score frontier nodes of the graph. ExploRFM simultaneously predicts traversability, visual frontiers, and object similarity in image space, enabling real-time, onboard semantic navigation tasks. The resulting vision-scored graph enables the robot to explore semantically meaningful directions while ensuring geometric safety. Furthermore, we introduce a particle-filter-based method for coarse localization of the open-vocabulary target query, that estimates candidate goal positions beyond the robot's immediate depth horizon, enabling effective planning toward distant goals. Extensive closed-loop field experiments across diverse off-road and urban terrains demonstrate that WildOS enables robust navigation, significantly outperforming purely geometric and purely vision-based baselines in both efficiency and autonomy. Our results highlight the potential of vision foundation models to drive open-world robotic behaviors that are both semantically informed and geometrically grounded. Project Page: https://leggedrobotics.github.io/wildos/",
          "authors": [
            "Hardik Shah",
            "Erica Tevere",
            "Deegan Atha",
            "Marcel Kaufmann",
            "Shehryar Khattak",
            "Manthan Patel",
            "Marco Hutter",
            "Jonas Frey",
            "Patrick Spieler"
          ],
          "published": "2026-02-22T19:14:00Z",
          "updated": "2026-02-22T19:14:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19308v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19304v1",
          "title": "Safe and Interpretable Multimodal Path Planning for Multi-Agent Cooperation",
          "summary": "Successful cooperation among decentralized agents requires each agent to quickly adapt its plan to the behavior of other agents. In scenarios where agents cannot confidently predict one another's intentions and plans, language communication can be crucial for ensuring safety. In this work, we focus on path-level cooperation in which agents must adapt their paths to one another in order to avoid collisions or perform physical collaboration such as joint carrying. In particular, we propose a safe and interpretable multimodal path planning method, CaPE (Code as Path Editor), which generates and updates path plans for an agent based on the environment and language communication from other agents. CaPE leverages a vision-language model (VLM) to synthesize a path editing program verified by a model-based planner, grounding communication to path plan updates in a safe and interpretable way. We evaluate our approach in diverse simulated and real-world scenarios, including multi-robot and human-robot cooperation in autonomous driving, household, and joint carrying tasks. Experimental results demonstrate that CaPE can be integrated into different robotic systems as a plug-and-play module, greatly enhancing a robot's ability to align its plan to language communication from other robots or humans. We also show that the combination of the VLM-based path editing program synthesis and model-based planning safety enables robots to achieve open-ended cooperation while maintaining safety and interpretability.",
          "authors": [
            "Haojun Shi",
            "Suyu Ye",
            "Katherine M. Guerrerio",
            "Jianzhi Shen",
            "Yifan Yin",
            "Daniel Khashabi",
            "Chien-Ming Huang",
            "Tianmin Shu"
          ],
          "published": "2026-02-22T18:57:07Z",
          "updated": "2026-02-22T18:57:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19304v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19273v1",
          "title": "3D Shape Control of Extensible Multi-Section Soft Continuum Robots via Visual Servoing",
          "summary": "In this paper, we propose a novel vision-based control algorithm for regulating the whole body shape of extensible multisection soft continuum manipulators. Contrary to existing vision-based control algorithms in the literature that regulate the robot's end effector pose, our proposed control algorithm regulates the robot's whole body configuration, enabling us to leverage its kinematic redundancy. Additionally, our model-based 2.5D shape visual servoing provides globally stable asymptotic convergence in the robot's 3D workspace compared to the closest works in the literature that report local minima. Unlike existing visual servoing algorithms in the literature, our approach does not require information from proprioceptive sensors, making it suitable for continuum manipulators without such capabilities. Instead, robot state is estimated from images acquired by an external camera that observes the robot's whole body shape and is also utilized to close the shape control loop. Traditionally, visual servoing schemes require an image of the robot at its reference pose to generate the reference features. In this work, we utilize an inverse kinematics solver to generate reference features for the desired robot configuration and do not require images of the robot at the reference. Experiments are performed on a multisection continuum manipulator demonstrating the controller's capability to regulate the robot's whole body shape while precisely positioning the robot's end effector. Results validate our controller's ability to regulate the shape of continuum robots while demonstrating a smooth transient response and a steady-state error within 1 mm. Proof-of-concept object manipulation experiments including stacking, pouring, and pulling tasks are performed to demonstrate our controller's applicability.",
          "authors": [
            "Abhinav Gandhi",
            "Shou-Shan Chiang",
            "Cagdas D. Onal",
            "Berk Calli"
          ],
          "published": "2026-02-22T17:06:16Z",
          "updated": "2026-02-22T17:06:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19273v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19260v1",
          "title": "The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption",
          "summary": "Vision-Language-Action (VLA) models have recently been proposed as a pathway toward generalist robotic policies capable of interpreting natural language and visual inputs to generate manipulation actions. However, their effectiveness and efficiency on structured, long-horizon manipulation tasks remain unclear. In this work, we present a head-to-head empirical comparison between a fine-tuned open-weight VLA model π0 and a neuro-symbolic architecture that combines PDDL-based symbolic planning with learned low-level control. We evaluate both approaches on structured variants of the Towers of Hanoi manipulation task in simulation while measuring both task performance and energy consumption during training and execution. On the 3-block task, the neuro-symbolic model achieves 95% success compared to 34% for the best-performing VLA. The neuro-symbolic model also generalizes to an unseen 4-block variant (78% success), whereas both VLAs fail to complete the task. During training, VLA fine-tuning consumes nearly two orders of magnitude more energy than the neuro-symbolic approach. These results highlight important trade-offs between end-to-end foundation-model approaches and structured reasoning architectures for long-horizon robotic manipulation, emphasizing the role of explicit symbolic structure in improving reliability, data efficiency, and energy efficiency. Code and models are available at https://price-is-not-right.github.io",
          "authors": [
            "Timothy Duggan",
            "Pierrick Lorang",
            "Hong Lu",
            "Matthias Scheutz"
          ],
          "published": "2026-02-22T16:22:06Z",
          "updated": "2026-02-22T16:22:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19260v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20200v1",
          "title": "Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation",
          "summary": "Hierarchical Vision-Language-Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision-Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce OptimusVLA, a dual-memory VLA framework with Global Prior Memory (GPM) and Local Consistency Memory (LCM). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over pi_0 by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing pi_0 by 42.9% and 52.4%, respectively, while delivering 2.9x inference speedup.",
          "authors": [
            "Zaijing Li",
            "Bing Hu",
            "Rui Shao",
            "Gongwei Chen",
            "Dongmei Jiang",
            "Pengwei Xie",
            "Jianye Hao",
            "Liqiang Nie"
          ],
          "published": "2026-02-22T15:39:34Z",
          "updated": "2026-02-22T15:39:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20200v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19193v1",
          "title": "Visual Prompt Guided Unified Pushing Policy",
          "summary": "As one of the simplest non-prehensile manipulation skills, pushing has been widely studied as an effective means to rearrange objects. Existing approaches, however, typically rely on multi-step push plans composed of pre-defined pushing primitives with limited application scopes, which restrict their efficiency and versatility across different scenarios. In this work, we propose a unified pushing policy that incorporates a lightweight prompting mechanism into a flow matching policy to guide the generation of reactive, multimodal pushing actions. The visual prompt can be specified by a high-level planner, enabling the reuse of the pushing policy across a wide range of planning problems. Experimental results demonstrate that the proposed unified pushing policy not only outperforms existing baselines but also effectively serves as a low-level primitive within a VLM-guided planning framework to solve table-cleaning tasks efficiently.",
          "authors": [
            "Hieu Bui",
            "Ziyan Gao",
            "Yuya Hosoda",
            "Joo-Ho Lee"
          ],
          "published": "2026-02-22T13:48:38Z",
          "updated": "2026-02-22T13:48:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19193v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19184v1",
          "title": "Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation",
          "summary": "Learning from Demonstration (LfD) offers a promising paradigm for robot skill acquisition. Recent approaches attempt to extract manipulation commands directly from video demonstrations, yet face two critical challenges: (1) general video captioning models prioritize global scene features over task-relevant objects, producing descriptions unsuitable for precise robotic execution, and (2) end-to-end architectures coupling visual understanding with policy learning require extensive paired datasets and struggle to generalize across objects and scenarios. To address these limitations, we propose a novel ``Human-to-Robot'' imitation learning pipeline that enables robots to acquire manipulation skills directly from unstructured video demonstrations, inspired by the human ability to learn by watching and imitating. Our key innovation is a modular framework that decouples the learning process into two distinct stages: (1) Video Understanding, which combines Temporal Shift Modules (TSM) with Vision-Language Models (VLMs) to extract actions and identify interacted objects, and (2) Robot Imitation, which employs TD3-based deep reinforcement learning to execute the demonstrated manipulations. We validated our approach in PyBullet simulation environments with a UR5e manipulator and in a real-world experiment with a UF850 manipulator across four fundamental actions: reach, pick, move, and put. For video understanding, our method achieves 89.97% action classification accuracy and BLEU-4 scores of 0.351 on standard objects and 0.265 on novel objects, representing improvements of 76.4% and 128.4% over the best baseline, respectively. For robot manipulation, our framework achieves an average success rate of 87.5% across all actions, with 100% success on reaching tasks and up to 90% on complex pick-and-place operations. The project website is available at https://thanhnguyencanh.github.io/LfD4hri.",
          "authors": [
            "Thanh Nguyen Canh",
            "Thanh-Tuan Tran",
            "Haolan Zhang",
            "Ziyan Gao",
            "Nak Young Chong",
            "Xiem HoangVan"
          ],
          "published": "2026-02-22T13:26:27Z",
          "updated": "2026-02-22T13:26:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19184v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19179v1",
          "title": "Distributional Stability of Tangent-Linearized Gaussian Inference on Smooth Manifolds",
          "summary": "Gaussian inference on smooth manifolds is central to robotics, but exact marginalization and conditioning are generally non-Gaussian and geometry-dependent. We study tangent-linearized Gaussian inference and derive explicit non-asymptotic $W_2$ stability bounds for projection marginalization and surface-measure conditioning. The bounds separate local second-order geometric distortion from nonlocal tail leakage and, for Gaussian inputs, yield closed-form diagnostics from $(μ,Σ)$ and curvature/reach surrogates. Circle and planar-pushing experiments validate the predicted calibration transition near $\\sqrt{\\|Σ\\|_{\\mathrm{op}}}/R\\approx 1/6$ and indicate that normal-direction uncertainty is the dominant failure mode when locality breaks. These diagnostics provide practical triggers for switching from single-chart linearization to multi-chart or sample-based manifold inference.",
          "authors": [
            "Junghoon Seo",
            "Hakjin Lee",
            "Jaehoon Sim"
          ],
          "published": "2026-02-22T13:18:45Z",
          "updated": "2026-02-22T13:18:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19179v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19173v1",
          "title": "Distributed and Consistent Multi-Robot Visual-Inertial-Ranging Odometry on Lie Groups",
          "summary": "Reliable localization is a fundamental requirement for multi-robot systems operating in GPS-denied environments. Visual-inertial odometry (VIO) provides lightweight and accurate motion estimation but suffers from cumulative drift in the absence of global references. Ultra-wideband (UWB) ranging offers complementary global observations, yet most existing UWB-aided VIO methods are designed for single-robot scenarios and rely on pre-calibrated anchors, which limits their robustness in practice. This paper proposes a distributed collaborative visual-inertial-ranging odometry (DC-VIRO) framework that tightly fuses VIO and UWB measurements across multiple robots. Anchor positions are explicitly included in the system state to address calibration uncertainty, while shared anchor observations are exploited through inter-robot communication to provide additional geometric constraints. By leveraging a right-invariant error formulation on Lie groups, the proposed approach preserves the observability properties of standard VIO, ensuring estimator consistency. Simulation results with multiple robots demonstrate that DC-VIRO significantly improves localization accuracy and robustness, while simultaneously enabling anchor self-calibration in distributed settings.",
          "authors": [
            "Ziwei Kang",
            "Yizhi Zhou"
          ],
          "published": "2026-02-22T13:07:50Z",
          "updated": "2026-02-22T13:07:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19173v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19108v1",
          "title": "Understanding Fire Through Thermal Radiation Fields for Mobile Robots",
          "summary": "Safely moving through environments affected by fire is a critical capability for autonomous mobile robots deployed in disaster response. In this work, we present a novel approach for mobile robots to understand fire through building real-time thermal radiation fields. We register depth and thermal images to obtain a 3D point cloud annotated with temperature values. From these data, we identify fires and use the Stefan-Boltzmann law to approximate the thermal radiation in empty spaces. This enables the construction of a continuous thermal radiation field over the environment. We show that this representation can be used for robot navigation, where we embed thermal constraints into the cost map to compute collision-free and thermally safe paths. We validate our approach on a Boston Dynamics Spot robot in controlled experimental settings. Our experiments demonstrate the robot's ability to avoid hazardous regions while still reaching navigation goals. Our approach paves the way toward mobile robots that can be autonomously deployed in fire-affected environments, with potential applications in search-and-rescue, firefighting, and hazardous material response.",
          "authors": [
            "Anton R. Wagner",
            "Madhan Balaji Rao",
            "Xuesu Xiao",
            "Sören Pirk"
          ],
          "published": "2026-02-22T09:44:12Z",
          "updated": "2026-02-22T09:44:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19108v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19107v1",
          "title": "A User-driven Design Framework for Robotaxi",
          "summary": "Robotaxis are emerging as a promising form of urban mobility, yet research has largely emphasized technical driving performance while leaving open how passengers experience and evaluate rides without a human driver. To address the limitations of prior work that often relies on simulated or hypothetical settings, we investigate real-world robotaxi use through 18 semi-structured interviews and autoethnographic ride experiences. We found that users were drawn to robotaxis by low cost, social recommendation, and curiosity. They valued a distinctive set of benefits, such as an increased sense of agency, and consistent driving behavioral consistency and standardized ride experiences. However, they encountered persistent challenges around limited flexibility, insufficient transparency, management difficulty, robustness concerns in edge cases, and emergency handling concerns. Robotaxi experiences were shaped by privacy, safety, ethics, and trust. Users were often privacy-indifferent yet sensitive to opaque access and leakage risks; safety perceptions were polarized; and ethical considerations surfaced round issues such as accountability, feedback responsibility and absence of human-like social norms. Based on these findings, we propose a user-driven design framework spanning the end-to-end journey, such as pre-ride configuration (hailing), context-aware pickup facilitation (pick-up) in-ride explainability (traveling), and accountable post-ride feedback (drop-off) to guide robotaxi interaction and service design.",
          "authors": [
            "Yue Deng",
            "Changyang He"
          ],
          "published": "2026-02-22T09:33:18Z",
          "updated": "2026-02-22T09:33:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19107v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19077v1",
          "title": "Design, Locomotion, and Control of Amphibious Robots: Recent Advances",
          "summary": "Amphibious robots, operating seamlessly across land and water, are advancing applications in conservation, disaster response, and defense. Their performance depends on locomotion mechanisms, actuation technologies, and sensor-control integration. This review highlights recent progress in these areas, examining movement strategies, material-based actuators, and control systems for autonomy and adaptability. Challenges and opportunities are outlined to guide future research toward more efficient, resilient, and multifunctional amphibious robots.",
          "authors": [
            "Yi Jin",
            "Chang Liu",
            "Roger D. Quinn",
            "Robert J. Wood",
            "C. Chase Cao"
          ],
          "published": "2026-02-22T07:18:02Z",
          "updated": "2026-02-22T07:18:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "physics.app-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19077v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19062v1",
          "title": "Path planning for unmanned surface vehicle based on predictive artificial potential field. International Journal of Advanced Robotic Systems",
          "summary": "Path planning for high-speed unmanned surface vehicles requires more complex solutions to reduce sailing time and save energy. This article proposes a new predictive artificial potential field that incorporates time information and predictive potential to plan smoother paths. It explores the principles of the artificial potential field, considering vehicle dynamics and local minimum reachability. The study first analyzes the most advanced traditional artificial potential field and its drawbacks in global and local path planning. It then introduces three modifications to the predictive artificial potential field-angle limit, velocity adjustment, and predictive potential to enhance the feasibility and flatness of the generated path. A comparison between the traditional and predictive artificial potential fields demonstrates that the latter successfully restricts the maximum turning angle, shortens sailing time, and intelligently avoids obstacles. Simulation results further verify that the predictive artificial potential field addresses the concave local minimum problem and improves reachability in special scenarios, ultimately generating a more efficient path that reduces sailing time and conserves energy for unmanned surface vehicles.",
          "authors": [
            "Jia Song",
            "Ce Hao",
            "Jiangcheng Su"
          ],
          "published": "2026-02-22T06:30:34Z",
          "updated": "2026-02-22T06:30:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19062v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19053v1",
          "title": "TeFlow: Enabling Multi-frame Supervision for Self-Supervised Feed-forward Scene Flow Estimation",
          "summary": "Self-supervised feed-forward methods for scene flow estimation offer real-time efficiency, but their supervision from two-frame point correspondences is unreliable and often breaks down under occlusions. Multi-frame supervision has the potential to provide more stable guidance by incorporating motion cues from past frames, yet naive extensions of two-frame objectives are ineffective because point correspondences vary abruptly across frames, producing inconsistent signals. In the paper, we present TeFlow, enabling multi-frame supervision for feed-forward models by mining temporally consistent supervision. TeFlow introduces a temporal ensembling strategy that forms reliable supervisory signals by aggregating the most temporally consistent motion cues from a candidate pool built across multiple frames. Extensive evaluations demonstrate that TeFlow establishes a new state-of-the-art for self-supervised feed-forward methods, achieving performance gains of up to 33\\% on the challenging Argoverse 2 and nuScenes datasets. Our method performs on par with leading optimization-based methods, yet speeds up 150 times. The code is open-sourced at https://github.com/KTH-RPL/OpenSceneFlow along with trained model weights.",
          "authors": [
            "Qingwen Zhang",
            "Chenhan Jiang",
            "Xiaomeng Zhu",
            "Yunqi Miao",
            "Yushan Zhang",
            "Olov Andersson",
            "Patric Jensfelt"
          ],
          "published": "2026-02-22T05:50:16Z",
          "updated": "2026-02-22T05:50:16Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19053v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.19038v1",
          "title": "A Checklist for Deploying Robots in Public: Articulating Tacit Knowledge in the HRI Community",
          "summary": "Many of the challenges encountered in in-the-wild public deployments of robots remain undocumented despite sharing many common pitfalls. This creates a high barrier of entry and results in repetition of avoidable mistakes. To articulate the tacit knowledge in the HRI community, this paper presents a guideline in the form of a checklist to support researchers in preparing for robot deployments in public. Drawing on their own experience with public robot deployments, the research team collected essential topics to consider in public HRI research. These topics are represented as modular flip cards in a hierarchical table, structured into deployment phases and important domains. We interviewed six interdisciplinary researchers with expertise in public HRI and show how including community input refines the checklist. We further show the checklist in action in context of real public studies. Finally, we contribute the checklist as an open-source, customizable community resource that both collects joint expertise for continual evolution and is usable as a list, set of cards, and an interactive web tool.",
          "authors": [
            "Claire Liang",
            "Franziska Babel",
            "Hannah Pelikan",
            "Sydney Thompson",
            "Xiang Zhi Tan"
          ],
          "published": "2026-02-22T04:26:57Z",
          "updated": "2026-02-22T04:26:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.19038v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18991v1",
          "title": "FruitTouch: A Perceptive Gripper for Gentle and Scalable Fruit Harvesting",
          "summary": "The automation of fruit harvesting has gained increasing significance in response to rising labor shortages. A sensorized gripper is a key component of this process, which must be compact enough for confined spaces, able to stably grasp diverse fruits, and provide reliable feedback on fruit conditions for efficient harvesting. To address this need, we propose FruitTouch, a compact gripper that integrates high-resolution, vision-based tactile sensing through an optimized optical design. This configuration accommodates a wide range of fruit sizes while maintaining low cost and mechanical simplicity. Tactile images captured by an embedded camera provide rich information for real-time force estimation, slip detection, and softness prediction. We validate the gripper in real-world fruit harvesting experiments, demonstrating robust grasp stability and effective damage prevention.",
          "authors": [
            "Ruohan Zhang",
            "Mohammad Amin Mirzaee",
            "Wenzhen Yuan"
          ],
          "published": "2026-02-22T00:38:33Z",
          "updated": "2026-02-22T00:38:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18991v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18976v1",
          "title": "Bumper Drone: Elastic Morphology Design for Aerial Physical Interaction",
          "summary": "Aerial robots are evolving from avoiding obstacles to exploiting the environmental contact interactions for navigation, exploration and manipulation. A key challenge in such aerial physical interactions lies in handling uncertain contact forces on unknown targets, which typically demand accurate sensing and active control. We present a drone platform with elastic horns that enables touch-and-go manoeuvres - a self-regulated, consecutive bumping motion that allows the drone to maintain proximity to a wall without relying on active obstacle avoidance. It leverages environmental interaction as a form of embodied control, where low-level stabilisation and near-obstacle navigation emerge from the passive dynamic responses of the drone-obstacle system that resembles a mass-spring-damper system. Experiments show that the elastic horn can absorb impact energy while maintaining vehicle stability, reducing pitch oscillations by 38% compared to the rigid horn configuration. The lower horn arrangement was found to reduce pitch oscillations by approximately 54%. In addition to intermittent contact, the platform equipped with elastic horns also demonstrates stable, sustained contact with static objects, relying on a standard attitude PID controller.",
          "authors": [
            "Pongporn Supa",
            "Alex Dunnett",
            "Feng Xiao",
            "Rui Wu",
            "Mirko Kovac",
            "Basaran Bahadir Kocer"
          ],
          "published": "2026-02-21T22:44:40Z",
          "updated": "2026-02-21T22:44:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18976v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18967v1",
          "title": "TactEx: An Explainable Multimodal Robotic Interaction Framework for Human-Like Touch and Hardness Estimation",
          "summary": "Accurate perception of object hardness is essential for safe and dexterous contact-rich robotic manipulation. Here, we present TactEx, an explainable multimodal robotic interaction framework that unifies vision, touch, and language for human-like hardness estimation and interactive guidance. We evaluate TactEx on fruit-ripeness assessment, a representative task that requires both tactile sensing and contextual understanding. The system fuses GelSight-Mini tactile streams with RGB observations and language prompts. A ResNet50+LSTM model estimates hardness from sequential tactile data, while a cross-modal alignment module combines visual cues with guidance from a large language model (LLM). This explainable multimodal interface allows users to distinguish ripeness levels with statistically significant class separation (p < 0.01 for all fruit pairs). For touch placement, we compare YOLO with Grounded-SAM (GSAM) and find GSAM to be more robust for fine-grained segmentation and contact-site selection. A lightweight LLM parses user instructions and produces grounded natural-language explanations linked to the tactile outputs. In end-to-end evaluations, TactEx attains 90% task success on simple user queries and generalises to novel tasks without large-scale tuning. These results highlight the promise of combining pretrained visual and tactile models with language grounding to advance explainable, human-like touch perception and decision-making in robotics.",
          "authors": [
            "Felix Verstraete",
            "Lan Wei",
            "Wen Fan",
            "Dandan Zhang"
          ],
          "published": "2026-02-21T22:18:17Z",
          "updated": "2026-02-21T22:18:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18967v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18951v1",
          "title": "Temporal-Logic-Aware Frontier-Based Exploration",
          "summary": "This paper addresses the problem of temporal logic motion planning for an autonomous robot operating in an unknown environment. The objective is to enable the robot to satisfy a syntactically co-safe Linear Temporal Logic (scLTL) specification when the exact locations of the desired labels are not known a priori. We introduce a new type of automaton state, referred to as commit states. These states capture intermediate task progress resulting from actions whose consequences are irreversible. In other words, certain future paths to satisfaction become not feasible after taking those actions that lead to the commit states. By leveraging commit states, we propose a sound and complete frontier-based exploration algorithm that strategically guides the robot to make progress toward the task while preserving all possible ways of satisfying it. The efficacy of the proposed method is validated through simulations.",
          "authors": [
            "Azizollah Taheri",
            "Derya Aksaray"
          ],
          "published": "2026-02-21T20:08:46Z",
          "updated": "2026-02-21T20:08:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18951v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18872v1",
          "title": "Equivalence and Divergence of Bayesian Log-Odds and Dempster's Combination Rule for 2D Occupancy Grids",
          "summary": "We introduce a pignistic-transform-based methodology for fair comparison of Bayesian log-odds and Dempster's combination rule in occupancy grid mapping, matching per-observation decision probabilities to isolate the fusion rule from sensor parameterization. Under BetP matching across simulation, two real lidar datasets, and downstream path planning, Bayesian fusion is consistently favored (15/15 directional consistency, p = 3.1e-5) with small absolute differences (0.001-0.022). Under normalized plausibility matching, the direction reverses, confirming the result is matching-criterion-specific. The methodology is reusable for any future Bayesian/belief function comparison.",
          "authors": [
            "Tatiana Berlenko",
            "Kirill Krinkin"
          ],
          "published": "2026-02-21T15:39:22Z",
          "updated": "2026-02-21T15:39:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18872v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18862v1",
          "title": "Gait Asymmetry from Unilateral Weakness and Improvement With Ankle Assistance: a Reinforcement Learning based Simulation Study",
          "summary": "Unilateral muscle weakness often leads to asymmetric gait, disrupting interlimb coordination and stance timing. This study presents a reinforcement learning (RL) based musculoskeletal simulation framework to (1) quantify how progressive unilateral muscle weakness affects gait symmetry and (2) evaluate whether ankle exoskeleton assistance can improve gait symmetry under impaired conditions. The overarching goal is to establish a simulation- and learning-based workflow that supports early controller development prior to patient experiments. Asymmetric gait was induced by reducing right-leg muscle strength to 75%, 50%, and 25% of baseline. Gait asymmetry was quantified using toe-off timing, peak contact forces, and joint-level symmetry metrics. Increasing weakness produced progressively larger temporal and kinematic asymmetry, most pronounced at the ankle. Ankle range of motion symmetry degraded from near-symmetric behavior at 100% strength (symmetry index, SI = +6.4%; correlation r=0.974) to severe asymmetry at 25% strength (SI = -47.1%, r=0.889), accompanied by a load shift toward the unimpaired limb. At 50% strength, ankle exoskeleton assistance improved kinematic symmetry relative to the unassisted impaired condition, reducing the magnitude of ankle SI from 25.8% to 18.5% and increasing ankle correlation from r=0.948 to 0.966, although peak loading remained biased toward the unimpaired side. Overall, this framework supports controlled evaluation of impairment severity and assistive strategies, and provides a basis for future validation in human experiments.",
          "authors": [
            "Yifei Yuan",
            "Ghaith Androwis",
            "Xianlian Zhou"
          ],
          "published": "2026-02-21T15:04:37Z",
          "updated": "2026-02-21T15:04:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18862v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18850v1",
          "title": "When the Inference Meets the Explicitness or Why Multimodality Can Make Us Forget About the Perfect Predictor",
          "summary": "Although in the literature it is common to find predictors and inference systems that try to predict human intentions, the uncertainty of these models due to the randomness of human behavior has led some authors to start advocating the use of communication systems that explicitly elicit human intention. In this work, it is analyzed the use of four different communication systems with a human-robot collaborative object transportation task as experimental testbed: two intention predictors (one based on force prediction and another with an enhanced velocity prediction algorithm) and two explicit communication methods (a button interface and a voice-command recognition system). These systems were integrated into IVO, a custom mobile social robot equipped with force sensor to detect the force exchange between both agents and LiDAR to detect the environment. The collaborative task required transporting an object over a 5-7 meter distance with obstacles in the middle, demanding rapid decisions and precise physical coordination. 75 volunteers perform a total of 255 executions divided into three groups, testing inference systems in the first round, communication systems in the second, and the combined strategies in the third. The results show that, 1) once sufficient performance is achieved, the human no longer notices and positively assesses technical improvements; 2) the human prefers systems that are more natural to them even though they have higher failure rates; and 3) the preferred option is the right combination of both systems.",
          "authors": [
            "J. E. Domínguez-Vidal",
            "Alberto Sanfeliu"
          ],
          "published": "2026-02-21T14:27:10Z",
          "updated": "2026-02-21T14:27:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18850v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18835v1",
          "title": "GRAB: A Systematic Real-World Grasping Benchmark for Robotic Food Waste Sorting",
          "summary": "Food waste management is critical for sustainability, yet inorganic contaminants hinder recycling potential. Robotic automation presents a compelling approach to this challenge by accelerating the sorting process through automated contaminant removal. Still, the diverse and unpredictable nature of contaminants creates major challenges for robotic grasping. Benchmarking frameworks are critical for evaluating challenges from various perspectives. However, existing protocols rely on limited simulation datasets, prioritise simple metrics such as success rate, and overlook key object and environment-related pre-grasp conditions. This paper introduces GRAB, a comprehensive Grasping Real-World Article Benchmarking framework that addresses this gap by integrating diverse deformable objects, advanced grasp-pose-estimation vision, and, importantly, pre-grasp conditions, establishing a set of critical graspability metrics. It systematically compares industrial grasping modalities through an in-depth experimental evaluation involving 1,750 food contaminant grasp attempts across four high-fidelity scenes. This large-scale evaluation provides an extensive assessment of grasp performance for food waste sorting, offering a level of depth that has rarely been explored in previous studies. The results reveal distinct gripper strengths and limitations, with object quality emerging as the dominant performance factor in cluttered environments, while vision quality and clutter levels play moderate roles. These findings highlight essential design considerations and reinforce the necessity of developing multimodal gripper technologies capable of robust cross-category performance for effective robotic food waste sorting.",
          "authors": [
            "Moniesha Thilakarathna",
            "Xing Wang",
            "Min Wang",
            "David Hinwood",
            "Shuangzhe Liu",
            "Damith Herath"
          ],
          "published": "2026-02-21T13:42:04Z",
          "updated": "2026-02-21T13:42:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18835v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18814v1",
          "title": "RotorSuite: A MATLAB/Simulink Toolbox for Tilt Multi-Rotor UAV Modeling",
          "summary": "In recent years, aerial platforms have evolved from passive flying sensors into versatile, contact-aware robotic systems, leading to rapid advances in platform design. Standard coplanar and collinear quadrotors have been complemented by modern tilted and tilting multi-rotor platforms with enhanced maneuverability. To properly analyze, control, and validate the performance of these emerging platforms, an accurate modeling step is required; however, this can be time-consuming, user-dependent and error-prone. To address this issue, we propose a MATLAB/Simulink toolbox for modeling and simulating the dynamics of a broad class of multi-rotor platforms through both an analytical and physics-based approaches. The toolbox, named RotorSuite, is provided with comprehensive documentation and example use cases, representing a valuable tool for didactic, research, and industrial development purposes.",
          "authors": [
            "Nicola Cigarini",
            "Giulia Michieletto",
            "Angelo Cenedese"
          ],
          "published": "2026-02-21T12:19:03Z",
          "updated": "2026-02-21T12:19:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18814v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18813v1",
          "title": "Habilis-$β$: A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model",
          "summary": "We introduce Habilis-$β$, a fast-motion and long-lasting on-device vision-language-action (VLA) model designed for real-world deployment. Current VLA evaluation remains largely confined to single-trial success rates under curated resets, which fails to capture the fast-motion and long-lasting capabilities essential for practical operation. To address this, we introduce the Productivity-Reliability Plane (PRP), which evaluates performance through Tasks per Hour (TPH) and Mean Time Between Intervention (MTBI) under a continuous-run protocol that demands both high-speed execution and sustained robustness. Habilis-$β$ achieves high performance by integrating language-free pre-training on large-scale play data for robust interaction priors with post-training on cyclic task demonstrations that capture state drift across consecutive task iterations. The system further employs ESPADA for phase-adaptive motion shaping to accelerate free-space transit, utilizes rectified-flow distillation to enable high-frequency control on edge devices, and incorporates classifier-free guidance (CFG) as a deployment-time knob to dynamically balance instruction adherence and learned interaction priors. In 1-hour continuous-run evaluations, Habilis-$β$ achieves strong performance under the PRP metrics, compared to $π_{0.5}$ in both simulation and real-world environments. In simulation, Habilis-$β$ achieves 572.6 TPH and 39.2 s MTBI (vs. 120.5 TPH and 30.5 s for $π_{0.5}$), while in a real-world humanoid logistics workflow it achieves 124 TPH and 137.4 s MTBI (vs. 19 TPH and 46.1 s for $π_{0.5}$). Finally, Habilis-$β$ achieves the highest reported performance on the standard RoboTwin 2.0 leaderboard across representative tasks, validating its effectiveness in complex manipulation scenarios.",
          "authors": [
            "Tommoro Robotics",
            ":",
            "Jesoon Kang",
            "Taegeon Park",
            "Jisu An",
            "Soo Min Kimm",
            "Jaejoon Kim",
            "Jinu Pahk",
            "Byungju Kim",
            "Junseok Lee",
            "Namheon Baek",
            "Sungwan Ha",
            "Hojun Baek",
            "Eduardo Ayerve Cruz",
            "Wontae Kim",
            "Junghyeon Choi",
            "Yousuk Lee",
            "Joonmo Han",
            "Sunghyun Cho",
            "Sunghyun Kwon",
            "Soyoung Lee",
            "Jun Ki Lee",
            "Seung-Joon Yi",
            "Byoung-Tak Zhang",
            "Theo Taeyeong Kim"
          ],
          "published": "2026-02-21T12:15:49Z",
          "updated": "2026-02-21T12:15:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18813v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18803v1",
          "title": "Learning to Localize Reference Trajectories in Image-Space for Visual Navigation",
          "summary": "We present LoTIS, a model for visual navigation that provides robot-agnostic image-space guidance by localizing a reference RGB trajectory in the robot's current view, without requiring camera calibration, poses, or robot-specific training. Instead of predicting actions tied to specific robots, we predict the image-space coordinates of the reference trajectory as they would appear in the robot's current view. This creates robot-agnostic visual guidance that easily integrates with local planning. Consequently, our model's predictions provide guidance zero-shot across diverse embodiments. By decoupling perception from action and learning to localize trajectory points rather than imitate behavioral priors, we enable a cross-trajectory training strategy for robustness to viewpoint and camera changes. We outperform state-of-the-art methods by 20-50 percentage points in success rate on conventional forward navigation, achieving 94-98% success rate across diverse sim and real environments. Furthermore, we achieve over 5x improvements on challenging tasks where baselines fail, such as backward traversal. The system is straightforward to use: we show how even a video from a phone camera directly enables different robots to navigate to any point on the trajectory. Videos, demo, and code are available at https://finnbusch.com/lotis.",
          "authors": [
            "Finn Lukas Busch",
            "Matti Vahs",
            "Quantao Yang",
            "Jesús Gerardo Ortega Peimbert",
            "Yixi Cai",
            "Jana Tumova",
            "Olov Andersson"
          ],
          "published": "2026-02-21T11:33:00Z",
          "updated": "2026-02-21T11:33:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18803v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18742v1",
          "title": "RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning",
          "summary": "Synthetic data generated by video generative models has shown promise for robot learning as a scalable pipeline, but it often suffers from inconsistent action quality due to imperfectly generated videos. Recently, vision-language models (VLMs) have been leveraged to validate video quality, but they have limitations in distinguishing physically accurate videos and, even then, cannot directly evaluate the generated actions themselves. To tackle this issue, we introduce RoboCurate, a novel synthetic robot data generation framework that evaluates and filters the quality of annotated actions by comparing them with simulation replay. Specifically, RoboCurate replays the predicted actions in a simulator and assesses action quality by measuring the consistency of motion between the simulator rollout and the generated video. In addition, we unlock observation diversity beyond the available dataset via image-to-image editing and apply action-preserving video-to-video transfer to further augment appearance. We observe RoboCurate's generated data yield substantial relative improvements in success rates compared to using real data only, achieving +70.1% on GR-1 Tabletop (300 demos), +16.1% on DexMimicGen in the pre-training setup, and +179.9% in the challenging real-world ALLEX humanoid dexterous manipulation setting.",
          "authors": [
            "Seungku Kim",
            "Suhyeok Jang",
            "Byungjun Yoon",
            "Dongyoung Kim",
            "John Won",
            "Jinwoo Shin"
          ],
          "published": "2026-02-21T07:33:24Z",
          "updated": "2026-02-21T07:33:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18742v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18735v1",
          "title": "LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency",
          "summary": "This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First, \\ourname{} harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at \\href{https://github.com/DavidYan2001/LaS-Comp}{LaS-Comp}.",
          "authors": [
            "Weilong Yan",
            "Haipeng Li",
            "Hao Xu",
            "Nianjin Ye",
            "Yihao Ai",
            "Shuaicheng Liu",
            "Jingyu Hu"
          ],
          "published": "2026-02-21T06:55:28Z",
          "updated": "2026-02-21T06:55:28Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18735v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18716v1",
          "title": "Temporal Action Representation Learning for Tactical Resource Control and Subsequent Maneuver Generation",
          "summary": "Autonomous robotic systems should reason about resource control and its impact on subsequent maneuvers, especially when operating with limited energy budgets or restricted sensing. Learning-based control is effective in handling complex dynamics and represents the problem as a hybrid action space unifying discrete resource usage and continuous maneuvers. However, prior works on hybrid action space have not sufficiently captured the causal dependencies between resource usage and maneuvers. They have also overlooked the multi-modal nature of tactical decisions, both of which are critical in fast-evolving scenarios. In this paper, we propose TART, a Temporal Action Representation learning framework for Tactical resource control and subsequent maneuver generation. TART leverages contrastive learning based on a mutual information objective, designed to capture inherent temporal dependencies in resource-maneuver interactions. These learned representations are quantized into discrete codebook entries that condition the policy, capturing recurring tactical patterns and enabling multi-modal and temporally coherent behaviors. We evaluate TART in two domains where resource deployment is critical: (i) a maze navigation task where a limited budget of discrete actions provides enhanced mobility, and (ii) a high-fidelity air combat simulator in which an F-16 agent operates weapons and defensive systems in coordination with flight maneuvers. Across both domains, TART consistently outperforms hybrid-action baselines, demonstrating its effectiveness in leveraging limited resources and producing context-aware subsequent maneuvers.",
          "authors": [
            "Hoseong Jung",
            "Sungil Son",
            "Daesol Cho",
            "Jonghae Park",
            "Changhyun Choi",
            "H. Jin Kim"
          ],
          "published": "2026-02-21T04:31:55Z",
          "updated": "2026-02-21T04:31:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18716v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18709v1",
          "title": "IRIS-SLAM: Unified Geo-Instance Representations for Robust Semantic Localization and Mapping",
          "summary": "Geometry foundation models have significantly advanced dense geometric SLAM, yet existing systems often lack deep semantic understanding and robust loop closure capabilities. Meanwhile, contemporary semantic mapping approaches are frequently hindered by decoupled architectures and fragile data association. We propose IRIS-SLAM, a novel RGB semantic SLAM system that leverages unified geometric-instance representations derived from an instance-extended foundation model. By extending a geometry foundation model to concurrently predict dense geometry and cross-view consistent instance embeddings, we enable a semantic-synergized association mechanism and instance-guided loop closure detection. Our approach effectively utilizes viewpoint-agnostic semantic anchors to bridge the gap between geometric reconstruction and open-vocabulary mapping. Experimental results demonstrate that IRIS-SLAM significantly outperforms state-of-the-art methods, particularly in map consistency and wide-baseline loop closure reliability.",
          "authors": [
            "Tingyang Xiao",
            "Liu Liu",
            "Wei Feng",
            "Zhengyu Zou",
            "Xiaolin Zhou",
            "Wei Sui",
            "Hao Li",
            "Dingwen Zhang",
            "Zhizhong Su"
          ],
          "published": "2026-02-21T03:57:01Z",
          "updated": "2026-02-21T03:57:01Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18709v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18707v1",
          "title": "CLASH: Collision Learning via Augmented Sim-to-real Hybridization to Bridge the Reality Gap",
          "summary": "The sim-to-real gap, particularly in the inaccurate modeling of contact-rich dynamics like collisions, remains a primary obstacle to deploying robot policies trained in simulation. Conventional physics engines often trade accuracy for computational speed, leading to discrepancies that prevent direct policy transfer. To address this, we introduce Collision Learning via Augmented Sim-to-real Hybridization (CLASH), a data-efficient framework that creates a high-fidelity hybrid simulator by learning a surrogate collision model from a minimal set of real-world data. In CLASH, a base model is first distilled from an imperfect simulator (MuJoCo) to capture general physical priors; this model is then fine-tuned with a remarkably small number of real-world interactions (as few as 10 samples) to correct for the simulator's inherent inaccuracies. The resulting hybrid simulator not only achieves higher predictive accuracy but also reduces collision computation time by nearly 50\\%. We demonstrate that policies obtained with our hybrid simulator transfer more robustly to the real world, doubling the success rate in sequential pushing tasks with reinforecement learning and significantly increase the task performance with model-based control.",
          "authors": [
            "Haotian He",
            "Ning Guo",
            "Siqi Shi",
            "Qipeng Liu",
            "Wenzhao Lian"
          ],
          "published": "2026-02-21T03:39:33Z",
          "updated": "2026-02-21T03:39:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18707v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18688v1",
          "title": "Scout-Rover cooperation: online terrain strength mapping and traversal risk estimation for planetary-analog explorations",
          "summary": "Robot-aided exploration of planetary surfaces is essential for understanding geologic processes, yet many scientifically valuable regions, such as Martian dunes and lunar craters, remain hazardous due to loose, deformable regolith. We present a scout-rover cooperation framework that expands safe access to such terrain using a hybrid team of legged and wheeled robots. In our approach, a high-mobility legged robot serves as a mobile scout, using proprioceptive leg-terrain interactions to estimate regolith strength during locomotion and construct spatially resolved terrain maps. These maps are integrated with rover locomotion models to estimate traversal risk and inform path planning. We validate the framework through analogue missions at the NASA Ames Lunar Simulant Testbed and the White Sands Dune Field. Experiments demonstrate (1) online terrain strength mapping from legged locomotion and (2) rover-specific traversal-risk estimation enabling safe navigation to scientific targets. Results show that scout-generated terrain maps reliably capture spatial variability and predict mobility failure modes, allowing risk-aware path planning that avoids hazardous regions. By combining embodied terrain sensing with heterogeneous rover cooperation, this framework enhances operational robustness and expands the reachable science workspace in deformable planetary environments.",
          "authors": [
            "Shipeng Liu",
            "J. Diego Caporale",
            "Yifeng Zhang",
            "Xingjue Liao",
            "William Hoganson",
            "Wilson Hu",
            "Shivangi Misra",
            "Neha Peddinti",
            "Rachel Holladay",
            "Ethan Fulcher",
            "Akshay Ram Panyam",
            "Andrik Puentes",
            "Jordan M. Bretzfelder",
            "Michael Zanetti",
            "Uland Wong",
            "Daniel E. Koditschek",
            "Mark Yim",
            "Douglas Jerolmack",
            "Cynthia Sung",
            "Feifei Qian"
          ],
          "published": "2026-02-21T01:41:40Z",
          "updated": "2026-02-21T01:41:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18688v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18688v2",
          "title": "Scout-Rover cooperation: online terrain strength mapping and traversal risk estimation for planetary-analog explorations",
          "summary": "Robot-aided exploration of planetary surfaces is essential for understanding geologic processes, yet many scientifically valuable regions, such as Martian dunes and lunar craters, remain hazardous due to loose, deformable regolith. We present a scout-rover cooperation framework that expands safe access to such terrain using a hybrid team of legged and wheeled robots. In our approach, a high-mobility legged robot serves as a mobile scout, using proprioceptive leg-terrain interactions to estimate regolith strength during locomotion and construct spatially resolved terrain maps. These maps are integrated with rover locomotion models to estimate traversal risk and inform path planning. We validate the framework through analogue missions at the NASA Ames Lunar Simulant Testbed and the White Sands Dune Field. Experiments demonstrate (1) online terrain strength mapping from legged locomotion and (2) rover-specific traversal-risk estimation enabling safe navigation to scientific targets. Results show that scout-generated terrain maps reliably capture spatial variability and predict mobility failure modes, allowing risk-aware path planning that avoids hazardous regions. By combining embodied terrain sensing with heterogeneous rover cooperation, this framework enhances operational robustness and expands the reachable science workspace in deformable planetary environments.",
          "authors": [
            "Shipeng Liu",
            "J. Diego Caporale",
            "Yifeng Zhang",
            "Xingjue Liao",
            "William Hoganson",
            "Wilson Hu",
            "Shivangi Misra",
            "Neha Peddinti",
            "Rachel Holladay",
            "Ethan Fulcher",
            "Akshay Ram Panyam",
            "Andrik Puentes",
            "Jordan M. Bretzfelder",
            "Michael Zanetti",
            "Uland Wong",
            "Daniel E. Koditschek",
            "Mark Yim",
            "Douglas Jerolmack",
            "Cynthia Sung",
            "Feifei Qian"
          ],
          "published": "2026-02-21T01:41:40Z",
          "updated": "2026-02-24T06:52:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18688v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18684v1",
          "title": "Systematic Analysis of Coupling Effects on Closed-Loop and Open-Loop Performance in Aerial Continuum Manipulators",
          "summary": "This paper investigates two distinct approaches to the dynamic modeling of aerial continuum manipulators (ACMs): the decoupled and the coupled formulations. Both open-loop and closed-loop behaviors of a representative ACM are analyzed. The primary objective is to determine the conditions under which the decoupled model attains accuracy comparable to the coupled model while offering reduced computational cost under identical numerical conditions. The system dynamics are first derived using the Euler--Lagrange method under the piecewise constant curvature (PCC) assumption, with explicit treatment of the near-zero curvature singularity. A decoupled model is then obtained by neglecting the coupling terms in the ACM dynamics, enabling systematic evaluation of open-loop responses under diverse actuation profiles and external wrenches. To extend the analysis to closed-loop performance, a novel dynamics-based proportional-derivative sliding mode image-based visual servoing (DPD-SM-IBVS) controller is developed for regulating image feature errors in the presence of a moving target. The controller is implemented with both coupled and decoupled models, allowing a direct comparison of their effectiveness. The open-loop simulations reveal pronounced discrepancies between the two modeling approaches, particularly under varying torque inputs and continuum arm parameters. Conversely, the closed-loop experiments demonstrate that the decoupled model achieves tracking accuracy on par with the coupled model (within subpixel error) while incurring lower computational cost.",
          "authors": [
            "Niloufar Amiri",
            "Shayan Sepahvand",
            "Iraj Mantegh",
            "Farrokh Janabi-Sharifi"
          ],
          "published": "2026-02-21T01:18:34Z",
          "updated": "2026-02-21T01:18:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18684v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18663v1",
          "title": "Toward AI Autonomous Navigation for Mechanical Thrombectomy using Hierarchical Modular Multi-agent Reinforcement Learning (HM-MARL)",
          "summary": "Mechanical thrombectomy (MT) is typically the optimal treatment for acute ischemic stroke involving large vessel occlusions, but access is limited due to geographic and logistical barriers. Reinforcement learning (RL) shows promise in autonomous endovascular navigation, but generalization across 'long' navigation tasks remains challenging. We propose a Hierarchical Modular Multi-Agent Reinforcement Learning (HM-MARL) framework for autonomous two-device navigation in vitro, enabling efficient and generalizable navigation. HM-MARL was developed to autonomously navigate a guide catheter and guidewire from the femoral artery to the internal carotid artery (ICA). A modular multi-agent approach was used to decompose the complex navigation task into specialized subtasks, each trained using Soft Actor-Critic RL. The framework was validated in both in silico and in vitro testbeds to assess generalization and real-world feasibility. In silico, a single-vasculature model achieved 92-100% success rates on individual anatomies, while a multi-vasculature model achieved 56-80% across multiple patient anatomies. In vitro, both HM-MARL models successfully navigated 100% of trials from the femoral artery to the right common carotid artery and 80% to the right ICA but failed on the left-side vessel superhuman challenge due to the anatomy and catheter type used in navigation. This study presents the first demonstration of in vitro autonomous navigation in MT vasculature. While HM-MARL enables generalization across anatomies, the simulation-to-real transition introduces challenges. Future work will refine RL strategies using world models and validate performance on unseen in vitro data, advancing autonomous MT towards clinical translation.",
          "authors": [
            "Harry Robertshaw",
            "Nikola Fischer",
            "Lennart Karstensen",
            "Benjamin Jackson",
            "Xingyu Chen",
            "S. M. Hadi Sadati",
            "Christos Bergeles",
            "Alejandro Granados",
            "Thomas C Booth"
          ],
          "published": "2026-02-20T23:50:35Z",
          "updated": "2026-02-20T23:50:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18663v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18661v1",
          "title": "Robotic Fruits with Tunable Stiffness and Sensing: Towards a Methodology for Developing Realistic Physical Twins of Fruits",
          "summary": "The global agri-food sector faces increasing challenges from labour shortages, high consumer demand, and supply-chain disruptions, resulting in substantial losses of unharvested produce. Robotic harvesting has emerged as a promising alternative; however, evaluating and training soft grippers for delicate fruits remains difficult due to the highly variable mechanical properties of natural produce. This makes it difficult to establish reliable benchmarks or data-driven control strategies. Existing testing practices rely on large quantities of real fruit to capture this variability, leading to inefficiency, higher costs, and waste. The methodology presented in this work aims to address these limitations by developing tunable soft physical twins that emulate the stiffness characteristics of real fruits at different ripeness levels. A fiber-reinforced pneumatic physical twin of a kiwi fruit was designed and fabricated to replicate the stiffness at different ripeness levels. Experimental results show that the stiffness of the physical twin can be tuned accurately over multiple trials (97.35 - 99.43% accuracy). Gripping tasks with a commercial robotic gripper showed that sensor feedback from the physical twin can reflect the applied gripping forces. Finally, a stress test was performed over 50 cycles showed reliable maintenance of desired stiffness (0.56 - 1.10% error). This work shows promise that robotic physical twins could adjust their stiffness to resemble that of real fruits. This can provide a sustainable, controllable platform for benchmarking and training robotic grippers.",
          "authors": [
            "Saitarun Nadipineni",
            "Keshav Pandiyan",
            "Kaspar Althoefer",
            "Shinichi Hirai",
            "Thilina Dulantha Lalitharatne"
          ],
          "published": "2026-02-20T23:40:16Z",
          "updated": "2026-02-20T23:40:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18661v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18655v1",
          "title": "Infinite-Dimensional Closed-Loop Inverse Kinematics for Soft Robots via Neural Operators",
          "summary": "While kinematic inversion is a purely geometric problem for fully actuated rigid robots, it becomes extremely challenging for underactuated soft robots with infinitely many degrees of freedom. Closed-loop inverse kinematics (CLIK) schemes address this by introducing end-to-end mappings from actuation to task space for the controller to operate on, but typically assume finite dimensions of the underlying virtual configuration space. In this work, we extend CLIK to the infinite-dimensional domain to reason about the entire soft robot shape while solving tasks. We do this by composing an actuation-to-shape map with a shape-to-task map, deriving the differential end-to-end kinematics via an infinite-dimensional chain rule, and thereby obtaining a Jacobian-based CLIK algorithm. Since the actuation-to-shape mapping is rarely available in closed form, we propose to learn it from simulation data using neural operator networks, which are differentiable. We first present an analytical study on a constant-curvature segment, and then apply the neural version of the algorithm to a three-fiber soft robotic arm whose underlying model relies on morphoelasticity and active filament theory. This opens new possibilities for differentiable control of soft robots by exploiting full-body shape information in a continuous, infinite-dimensional framework.",
          "authors": [
            "Carina Veil",
            "Moritz Flaschel",
            "Ellen Kuhl",
            "Cosimo Della Santina"
          ],
          "published": "2026-02-20T23:11:56Z",
          "updated": "2026-02-20T23:11:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18655v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18638v1",
          "title": "Soft Surfaced Vision-Based Tactile Sensing for Bipedal Robot Applications",
          "summary": "Legged locomotion benefits from embodied sensing, where perception emerges from the physical interaction between body and environment. We present a soft-surfaced, vision-based tactile foot sensor that endows a bipedal robot with a skin-like deformable layer that captures contact deformations optically, turning foot-ground interactions into rich haptic signals. From a contact image stream, our method estimates contact pose (position and orientation), visualizes shear, computes center of pressure (CoP), classifies terrain, and detects geometric features of the contact patch. We validate these capabilities on a tilting platform and in visually obscured conditions, showing that foot-borne tactile feedback improves balance control and terrain awareness beyond proprioception alone. These findings suggest that integrating tactile perception into legged robot feet improves stability, adaptability, and environmental awareness, offering a promising direction toward more compliant and intelligent locomotion systems. For the supplementary video, please visit: https://youtu.be/ceJiy9q_2Aw",
          "authors": [
            "Jaeeun Kim",
            "Junhee Lim",
            "Yu She"
          ],
          "published": "2026-02-20T22:16:49Z",
          "updated": "2026-02-20T22:16:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18638v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18638v2",
          "title": "Soft Surfaced Vision-Based Tactile Sensing for Bipedal Robot Applications",
          "summary": "Legged locomotion benefits from embodied sensing, where perception emerges from the physical interaction between body and environment. We present a soft-surfaced, vision-based tactile foot sensor that endows a bipedal robot with a skin-like deformable layer that captures contact deformations optically, turning foot-ground interactions into rich haptic signals. From a contact image stream, our method estimates contact pose (position and orientation), visualizes shear, computes center of pressure (CoP), classifies terrain, and detects geometric features of the contact patch. We validate these capabilities on a tilting platform and in visually obscured conditions, showing that foot-borne tactile feedback improves balance control and terrain awareness beyond proprioception alone. These findings suggest that integrating tactile perception into legged robot feet improves stability, adaptability, and environmental awareness, offering a promising direction toward more compliant and intelligent locomotion systems. For the supplementary video, please visit: https://youtu.be/ceJiy9q_2Aw",
          "authors": [
            "Jaeeun Kim",
            "Junhee Lim",
            "Yu She"
          ],
          "published": "2026-02-20T22:16:49Z",
          "updated": "2026-02-24T14:04:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18638v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18622v1",
          "title": "FORMICA: Decision-Focused Learning for Communication-Free Multi-Robot Task Allocation",
          "summary": "Most multi-robot task allocation methods rely on communication to resolve conflicts and reach consistent assignments. In environments with limited bandwidth, degraded infrastructure, or adversarial interference, existing approaches degrade sharply. We introduce a learning-based framework that achieves high-quality task allocation without any robot-to-robot communication. The key idea is that robots coordinate implicitly by predicting teammates' bids: if each robot can anticipate competition for a task, it can adjust its choices accordingly. Our method predicts bid distributions to correct systematic errors in analytical mean-field approximations. While analytical predictions assume idealized conditions (uniform distributions, known bid functions), our learned approach adapts to task clustering and spatial heterogeneity. Inspired by Smart Predict-then-Optimize (SPO), we train predictors end-to-end to minimize Task Allocation Regret rather than prediction error. To scale to large swarms, we develop a mean-field approximation where each robot predicts the distribution of competing bids rather than individual bids, reducing complexity from $O(NT)$ to $O(T)$. We call our approach FORMICA: Field-Oriented Regret-Minimizing Implicit Coordination Algorithm. Experiments show FORMICA substantially outperforms a natural analytical baseline. In scenarios with 16 robots and 64 tasks, our approach improves system reward by 17% and approaches the optimal MILP solution. When deployed on larger scenarios (256 robots, 4096 tasks), the same model improves performance by 7%, demonstrating strong generalization. Training requires only 21 seconds on a laptop, enabling rapid adaptation to new environments.",
          "authors": [
            "Antonio Lopez",
            "Jack Muirhead",
            "Carlo Pinciroli"
          ],
          "published": "2026-02-20T21:26:46Z",
          "updated": "2026-02-20T21:26:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18622v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18606v1",
          "title": "OVerSeeC: Open-Vocabulary Costmap Generation from Satellite Images and Natural Language",
          "summary": "Aerial imagery provides essential global context for autonomous navigation, enabling route planning at scales inaccessible to onboard sensing. We address the problem of generating global costmaps for long-range planning directly from satellite imagery when entities and mission-specific traversal rules are expressed in natural language at test time. This setting is challenging since mission requirements vary, terrain entities may be unknown at deployment, and user prompts often encode compositional traversal logic. Existing approaches relying on fixed ontologies and static cost mappings cannot accommodate such flexibility. While foundation models excel at language interpretation and open-vocabulary perception, no single model can simultaneously parse nuanced mission directives, locate arbitrary entities in large-scale imagery, and synthesize them into an executable cost function for planners. We therefore propose OVerSeeC, a zero-shot modular framework that decomposes the problem into Interpret-Locate-Synthesize: (i) an LLM extracts entities and ranked preferences, (ii) an open-vocabulary segmentation pipeline identifies these entities from high-resolution imagery, and (iii) the LLM uses the user's natural language preferences and masks to synthesize executable costmap code. Empirically, OVerSeeC handles novel entities, respects ranked and compositional preferences, and produces routes consistent with human-drawn trajectories across diverse regions, demonstrating robustness to distribution shifts. This shows that modular composition of foundation models enables open-vocabulary, preference-aligned costmap generation for scalable, mission-adaptive global planning.",
          "authors": [
            "Rwik Rana",
            "Jesse Quattrociocchi",
            "Dongmyeong Lee",
            "Christian Ellis",
            "Amanda Adkins",
            "Adam Uccello",
            "Garrett Warnell",
            "Joydeep Biswas"
          ],
          "published": "2026-02-20T20:49:07Z",
          "updated": "2026-02-20T20:49:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18606v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18603v1",
          "title": "Enhancing Goal Inference via Correction Timing",
          "summary": "Corrections offer a natural modality for people to provide feedback to a robot, by (i) intervening in the robot's behavior when they believe the robot is failing (or will fail) the task objectives and (ii) modifying the robot's behavior to successfully fulfill the task. Each correction offers information on what the robot should and should not do, where the corrected behavior is more aligned with task objectives than the original behavior. Most prior work on learning from corrections involves interpreting a correction as a new demonstration (consisting of the modified robot behavior), or a preference (for the modified trajectory compared to the robot's original behavior). However, this overlooks one essential element of the correction feedback, which is the human's decision to intervene in the robot's behavior in the first place. This decision can be influenced by multiple factors including the robot's task progress, alignment with human expectations, dynamics, motion legibility, and optimality. In this work, we investigate whether the timing of this decision can offer a useful signal for inferring these task-relevant influences. In particular, we investigate three potential applications for this learning signal: (1) identifying features of a robot's motion that may prompt people to correct it, (2) quickly inferring the final goal of a human's correction based on the timing and initial direction of their correction motion, and (3) learning more precise constraints for task objectives. Our results indicate that correction timing results in improved learning for the first two of these applications. Overall, our work provides new insights on the value of correction timing as a signal for robot learning.",
          "authors": [
            "Anjiabei Wang",
            "Shuangge Wang",
            "Tesca Fitzgerald"
          ],
          "published": "2026-02-20T20:38:47Z",
          "updated": "2026-02-20T20:38:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18603v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18569v1",
          "title": "Design and Biomechanical Evaluation of a Lightweight Low-Complexity Soft Bilateral Ankle Exoskeleton",
          "summary": "Many people could benefit from exoskeleton assistance during gait, for either medical or nonmedical purposes. But exoskeletons bring added mass and structure, which in turn require compensating for. In this work, we present a lightweight, low-complexity, soft bilateral ankle exoskeleton for plantarflexion assistance, with a shoe attachment design that can be mounted on top of any pair of shoes. Experimental tests show no significant difference in lower limb kinematics and kinetics when wearing the exoskeleton in zero-torque mode relative to not wearing an exoskeleton, showing that our device does not obstruct healthy gait, and proving it as a compliant and comfortable device, promising to provide effective assistance. Hence, a control system was developed, and additional tests are underway.",
          "authors": [
            "Josée Mallah",
            "Zakii Javed",
            "Zafer Azak",
            "Thomas Stone",
            "Luigi G. Occhipinti"
          ],
          "published": "2026-02-20T19:16:54Z",
          "updated": "2026-02-20T19:16:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18569v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18424v1",
          "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation",
          "summary": "Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav",
          "authors": [
            "Xia Su",
            "Ruiqi Chen",
            "Benlin Liu",
            "Jingwei Ma",
            "Zonglin Di",
            "Ranjay Krishna",
            "Jon Froehlich"
          ],
          "published": "2026-02-20T18:46:27Z",
          "updated": "2026-02-20T18:46:27Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18424v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18421v1",
          "title": "Snapping Actuators with Asymmetric and Sequenced Motion",
          "summary": "Snapping instabilities in soft structures offer a powerful pathway to achieve rapid and energy-efficient actuation. In this study, an eccentric dome-shaped snapping actuator is developed to generate controllable asymmetric motion through geometry-induced instability. Finite element simulations and experiments reveal consistent asymmetric deformation and the corresponding pressure characteristics. By coupling four snapping actuators in a pneumatic network, a compact quadrupedal robot achieves coordinated wavelike locomotion using only a single pressure input. The robot exhibits frequency-dependent performance with a maximum speed of 72.78~mm/s at 7.5~Hz. These findings demonstrate the potential of asymmetric snapping mechanisms for physically controlled actuation and lay the groundwork for fully untethered and efficient soft robotic systems.",
          "authors": [
            "Xin Li",
            "Ye Jin",
            "Mohsen Jafarpour",
            "Hugo de Souza Oliveira",
            "Edoardo Milana"
          ],
          "published": "2026-02-20T18:45:17Z",
          "updated": "2026-02-20T18:45:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cond-mat.soft"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18421v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18397v1",
          "title": "How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf",
          "summary": "Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.",
          "authors": [
            "Wenqi Jiang",
            "Jason Clemons",
            "Karu Sankaralingam",
            "Christos Kozyrakis"
          ],
          "published": "2026-02-20T18:02:28Z",
          "updated": "2026-02-20T18:02:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18397v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18386v1",
          "title": "Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO",
          "summary": "Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.",
          "authors": [
            "Mohamed Elgouhary",
            "Amr S. El-Wakeel"
          ],
          "published": "2026-02-20T17:48:21Z",
          "updated": "2026-02-20T17:48:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18386v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18379v1",
          "title": "Ori-Sense: origami capacitive sensing for soft robotic applications",
          "summary": "This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.",
          "authors": [
            "Hugo de Souza Oliveira",
            "Xin Li",
            "Mohsen Jafarpour",
            "Edoardo Milana"
          ],
          "published": "2026-02-20T17:39:16Z",
          "updated": "2026-02-20T17:39:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cond-mat.soft"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18379v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18374v1",
          "title": "Zero-shot Interactive Perception",
          "summary": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.",
          "authors": [
            "Venkatesh Sripada",
            "Frank Guerin",
            "Amir Ghalamzan"
          ],
          "published": "2026-02-20T17:30:25Z",
          "updated": "2026-02-20T17:30:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18374v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18344v1",
          "title": "Downwash-aware Configuration Optimization for Modular Aerial Systems",
          "summary": "This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.",
          "authors": [
            "Mengguang Li",
            "Heinz Koeppl"
          ],
          "published": "2026-02-20T16:52:52Z",
          "updated": "2026-02-20T16:52:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18344v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18330v1",
          "title": "Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams",
          "summary": "Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.",
          "authors": [
            "Mohsen Jafarpour",
            "Ayberk Yüksek",
            "Shahab Eshghi",
            "Stanislav Gorb",
            "Edoardo Milana"
          ],
          "published": "2026-02-20T16:35:51Z",
          "updated": "2026-02-20T16:35:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18330v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18314v1",
          "title": "Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting",
          "summary": "Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.",
          "authors": [
            "Tianyi Song",
            "Danail Stoyanov",
            "Evangelos Mazomenos",
            "Francisco Vasconcelos"
          ],
          "published": "2026-02-20T16:14:21Z",
          "updated": "2026-02-20T16:14:21Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.GR",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18314v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18312v1",
          "title": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty",
          "summary": "Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.",
          "authors": [
            "Zhaoming Xie",
            "Kevin Karol",
            "Jessica Hodgins"
          ],
          "published": "2026-02-20T16:11:19Z",
          "updated": "2026-02-20T16:11:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.GR"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18312v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18260v1",
          "title": "Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments",
          "summary": "This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.",
          "authors": [
            "Magnus Norén",
            "Marios-Nektarios Stamatopoulos",
            "Avijit Banerjee",
            "George Nikolakopoulos"
          ],
          "published": "2026-02-20T14:44:16Z",
          "updated": "2026-02-20T14:44:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18260v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18258v1",
          "title": "RoEL: Robust Event-based 3D Line Reconstruction",
          "summary": "Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/",
          "authors": [
            "Gwangtak Bae",
            "Jaeho Shin",
            "Seunggu Kang",
            "Junho Kim",
            "Ayoung Kim",
            "Young Min Kim"
          ],
          "published": "2026-02-20T14:43:46Z",
          "updated": "2026-02-20T14:43:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18258v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18224v1",
          "title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation",
          "summary": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA",
          "authors": [
            "Yuankai Luo",
            "Woping Chen",
            "Tong Liang",
            "Baiqiao Wang",
            "Zhenguo Li"
          ],
          "published": "2026-02-20T14:04:27Z",
          "updated": "2026-02-20T14:04:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18224v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18212v1",
          "title": "Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator",
          "summary": "Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation. User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.",
          "authors": [
            "Rui Chen",
            "Domenico Chiaradia",
            "Daniele Leonardis",
            "Antonio Frisoli"
          ],
          "published": "2026-02-20T13:51:05Z",
          "updated": "2026-02-20T13:51:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18212v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18174v1",
          "title": "Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark",
          "summary": "Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.",
          "authors": [
            "Hyoseok Ju",
            "Bokeon Suh",
            "Giseop Kim"
          ],
          "published": "2026-02-20T12:23:46Z",
          "updated": "2026-02-20T12:23:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18174v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18164v1",
          "title": "GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation",
          "summary": "Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \\boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.",
          "authors": [
            "Jonas Frey",
            "Turcan Tuna",
            "Frank Fu",
            "Katharine Patterson",
            "Tianao Xu",
            "Maurice Fallon",
            "Cesar Cadena",
            "Marco Hutter"
          ],
          "published": "2026-02-20T11:57:52Z",
          "updated": "2026-02-20T11:57:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18164v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18097v1",
          "title": "Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning",
          "summary": "In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.",
          "authors": [
            "Aarati Andrea Noronha",
            "Jean Oh"
          ],
          "published": "2026-02-20T09:38:38Z",
          "updated": "2026-02-20T09:38:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18097v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18532v1",
          "title": "VLANeXt: Recipes for Building Strong VLA Models",
          "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.",
          "authors": [
            "Xiao-Ming Wu",
            "Bin Fan",
            "Kang Liao",
            "Jian-Jian Jiang",
            "Runze Yang",
            "Yihang Luo",
            "Zhonghua Wu",
            "Wei-Shi Zheng",
            "Chen Change Loy"
          ],
          "published": "2026-02-20T09:26:17Z",
          "updated": "2026-02-20T09:26:17Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18532v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18079v1",
          "title": "Dynamic Deception: When Pedestrians Team Up to Fool Autonomous Cars",
          "summary": "Many adversarial attacks on autonomous-driving perception models fail to cause system-level failures once deployed in a full driving stack. The main reason for such ineffectiveness is that once deployed in a system (e.g., within a simulator), attacks tend to be spatially or temporally short-lived, due to the vehicle's dynamics, hence rarely influencing the vehicle behaviour. In this paper, we address both limitations by introducing a system-level attack in which multiple dynamic elements (e.g., two pedestrians) carry adversarial patches (e.g., on cloths) and jointly amplify their effect through coordination and motion. We evaluate our attacks in the CARLA simulator using a state-of-the-art autonomous driving agent. At the system level, single-pedestrian attacks fail in all runs (out of 10), while dynamic collusion by two pedestrians induces full vehicle stops in up to 50\\% of runs, with static collusion yielding no successful attack at all. These results show that system-level failures arise only when adversarial signals persist over time and are amplified through coordinated actors, exposing a gap between model-level robustness and end-to-end safety.",
          "authors": [
            "Masoud Jamshidiyan Tehrani",
            "Marco Gabriel",
            "Jinhan Kim",
            "Paolo Tonella"
          ],
          "published": "2026-02-20T09:09:24Z",
          "updated": "2026-02-20T09:09:24Z",
          "primary_category": "cs.CR",
          "categories": [
            "cs.CR",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18079v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18071v1",
          "title": "EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots",
          "summary": "Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.",
          "authors": [
            "Boyuan An",
            "Zhexiong Wang",
            "Yipeng Wang",
            "Jiaqi Li",
            "Sihang Li",
            "Jing Zhang",
            "Chen Feng"
          ],
          "published": "2026-02-20T08:54:20Z",
          "updated": "2026-02-20T08:54:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18071v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18025v1",
          "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets",
          "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.",
          "authors": [
            "Haruki Abe",
            "Takayuki Osa",
            "Yusuke Mukuta",
            "Tatsuya Harada"
          ],
          "published": "2026-02-20T06:39:17Z",
          "updated": "2026-02-20T06:39:17Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18025v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18020v1",
          "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models",
          "summary": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.",
          "authors": [
            "Jiabing Yang",
            "Yixiang Chen",
            "Yuan Xu",
            "Peiyan Li",
            "Xiangnan Wu",
            "Zichen Wen",
            "Bowen Fang",
            "Tao Yu",
            "Zhengbo Zhang",
            "Yingda Li",
            "Kai Wang",
            "Jing Liu",
            "Nianfeng Liu",
            "Yan Huang",
            "Liang Wang"
          ],
          "published": "2026-02-20T06:22:21Z",
          "updated": "2026-02-20T06:22:21Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18020v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.18014v1",
          "title": "Quasi-Periodic Gaussian Process Predictive Iterative Learning Control",
          "summary": "Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\\mathcal{O}(p^3)$ instead of $\\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.",
          "authors": [
            "Unnati Nigam",
            "Radhendushka Srivastava",
            "Faezeh Marzbanrad",
            "Michael Burke"
          ],
          "published": "2026-02-20T06:10:10Z",
          "updated": "2026-02-20T06:10:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY",
            "stat.ML"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.18014v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17997v1",
          "title": "Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly",
          "summary": "Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.",
          "authors": [
            "Zehao Jin",
            "Yaoye Zhu",
            "Chen Zhang",
            "Yanan Sui"
          ],
          "published": "2026-02-20T05:09:28Z",
          "updated": "2026-02-20T05:09:28Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17997v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17926v1",
          "title": "Homotopic information gain for sparse active target tracking",
          "summary": "The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.",
          "authors": [
            "Jennifer Wakulicz",
            "Ki Myung Brian Lee",
            "Teresa Vidal-Calleja",
            "Robert Fitch"
          ],
          "published": "2026-02-20T01:23:58Z",
          "updated": "2026-02-20T01:23:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17926v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17921v1",
          "title": "Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation",
          "summary": "Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.",
          "authors": [
            "Kei Ikemura",
            "Yifei Dong",
            "Florian T. Pokorny"
          ],
          "published": "2026-02-20T00:33:20Z",
          "updated": "2026-02-20T00:33:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17921v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17908v1",
          "title": "WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection",
          "summary": "Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.",
          "authors": [
            "Mingzhang Zhu",
            "Alvin Zhu",
            "Jose Victor S. H. Ramos",
            "Beom Jun Kim",
            "Yike Shi",
            "Yufeng Wu",
            "Ruochen Hou",
            "Quanyou Wang",
            "Eric Song",
            "Tony Fan",
            "Yuchen Cui",
            "Dennis W. Hong"
          ],
          "published": "2026-02-20T00:12:45Z",
          "updated": "2026-02-20T00:12:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17908v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17832v1",
          "title": "MePoly: Max Entropy Polynomial Policy Optimization",
          "summary": "Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.",
          "authors": [
            "Hang Liu",
            "Sangli Teng",
            "Maani Ghaffari"
          ],
          "published": "2026-02-19T20:52:41Z",
          "updated": "2026-02-19T20:52:41Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17832v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17822v1",
          "title": "Evolution of Safety Requirements in Industrial Robotics: Comparative Analysis of ISO 10218-1/2 (2011 vs. 2025) and Integration of ISO/TS 15066",
          "summary": "Industrial robotics has established itself as an integral component of large-scale manufacturing enterprises. Simultaneously, collaborative robotics is gaining prominence, introducing novel paradigms of human-machine interaction. These advancements have necessitated a comprehensive revision of safety standards, specifically incorporating requirements for cybersecurity and protection against unauthorized access in networked robotic systems. This article presents a comparative analysis of the ISO 10218:2011 and ISO 10218:2025 standards, examining the evolution of their structure, terminology, technical requirements, and annexes. The analysis reveals significant expansions in functional safety and cybersecurity, the introduction of new classifications for robots and collaborative applications, and the normative integration of the technical specification ISO/TS 15066. Consequently, the new edition synthesizes mechanical, functional, and digital safety requirements, establishing a comprehensive framework for the design and operation of modern robotic systems.",
          "authors": [
            "Daniel Hartmann",
            "Kristýna Hamříková",
            "Aleš Vysocký",
            "Vendula Laciok",
            "Aleš Bernatík"
          ],
          "published": "2026-02-19T20:40:41Z",
          "updated": "2026-02-19T20:40:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17822v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17818v1",
          "title": "Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array",
          "summary": "Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.",
          "authors": [
            "Zachary Turcotte",
            "François Grondin"
          ],
          "published": "2026-02-19T20:35:43Z",
          "updated": "2026-02-19T20:35:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.SD"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17818v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17794v1",
          "title": "Reinforcement-Learning-Based Assistance Reduces Squat Effort with a Modular Hip--Knee Exoskeleton",
          "summary": "Squatting is one of the most demanding lower-limb movements, requiring substantial muscular effort and coordination. Reducing the physical demands of this task through intelligent and personalized assistance has significant implications, particularly in industries involving repetitive low-level assembly activities. In this study, we evaluated the effectiveness of a neural network controller for a modular Hip-Knee exoskeleton designed to assist squatting tasks. The neural network controller was trained via reinforcement learning (RL) in a physics-based, human-exoskeleton interaction simulation environment. The controller generated real-time hip and knee assistance torques based on recent joint-angle and velocity histories. Five healthy adults performed three-minute metronome-guided squats under three conditions: (1) no exoskeleton (No-Exo), (2) exoskeleton with Zero-Torque, and (3) exoskeleton with active assistance (Assistance). Physiological effort was assessed using indirect calorimetry and heart rate monitoring, alongside concurrent kinematic data collection. Results show that the RL-based controller adapts to individuals by producing torque profiles tailored to each subject's kinematics and timing. Compared with the Zero-Torque and No-Exo condition, active assistance reduced the net metabolic rate by approximately 10%, with minor reductions observed in heart rate. However, assisted trials also exhibited reduced squat depth, reflected by smaller hip and knee flexion. These preliminary findings suggest that the proposed controller can effectively lower physiological effort during repetitive squatting, motivating further improvements in both hardware design and control strategies.",
          "authors": [
            "Neethan Ratnakumar",
            "Mariya Huzaifa Tohfafarosh",
            "Saanya Jauhri",
            "Xianlian Zhou"
          ],
          "published": "2026-02-19T19:51:38Z",
          "updated": "2026-02-19T19:51:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17794v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17659v1",
          "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
          "summary": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.",
          "authors": [
            "Yu Fang",
            "Yuchun Feng",
            "Dong Jing",
            "Jiaqi Liu",
            "Yue Yang",
            "Zhenyu Wei",
            "Daniel Szafir",
            "Mingyu Ding"
          ],
          "published": "2026-02-19T18:59:20Z",
          "updated": "2026-02-19T18:59:20Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17659v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17601v1",
          "title": "Graph Neural Model Predictive Control for High-Dimensional Systems",
          "summary": "The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.",
          "authors": [
            "Patrick Benito Eberhard",
            "Luis Pabon",
            "Daniele Gammelli",
            "Hugo Buurmeijer",
            "Amon Lahr",
            "Mark Leone",
            "Andrea Carron",
            "Marco Pavone"
          ],
          "published": "2026-02-19T18:26:42Z",
          "updated": "2026-02-19T18:26:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17601v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17586v1",
          "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space",
          "summary": "Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.",
          "authors": [
            "Antonio Guillen-Perez"
          ],
          "published": "2026-02-19T18:10:16Z",
          "updated": "2026-02-19T18:10:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17586v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17574v1",
          "title": "Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes",
          "summary": "Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.",
          "authors": [
            "Joshua A. Robbins",
            "Andrew F. Thompson",
            "Jonah J. Glunt",
            "Herschel C. Pangborn"
          ],
          "published": "2026-02-19T17:32:37Z",
          "updated": "2026-02-19T17:32:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17574v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17573v1",
          "title": "FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations",
          "summary": "The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.",
          "authors": [
            "Konstantinos Foteinos",
            "Georgios Angelidis",
            "Aggelos Psiris",
            "Vasileios Argyriou",
            "Panagiotis Sarigiannidis",
            "Georgios Th. Papadopoulos"
          ],
          "published": "2026-02-19T17:31:08Z",
          "updated": "2026-02-19T17:31:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17573v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17537v1",
          "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control",
          "summary": "Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.",
          "authors": [
            "Qilong Cheng",
            "Matthew Mackay",
            "Ali Bereyhi"
          ],
          "published": "2026-02-19T16:50:31Z",
          "updated": "2026-02-19T16:50:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17537v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17515v1",
          "title": "RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments",
          "summary": "Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.",
          "authors": [
            "Ziyi Zong",
            "Xin Dong",
            "Jinwu Xiang",
            "Daochun Li",
            "Zhan Tu"
          ],
          "published": "2026-02-19T16:26:43Z",
          "updated": "2026-02-19T16:26:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17515v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17512v1",
          "title": "Dodging the Moose: Experimental Insights in Real-Life Automated Collision Avoidance",
          "summary": "The sudden appearance of a static obstacle on the road, i.e. the moose test, is a well-known emergency scenario in collision avoidance for automated driving. Model Predictive Control (MPC) has long been employed for planning and control of automated vehicles in the state of the art. However, real-time implementation of automated collision avoidance in emergency scenarios such as the moose test remains unaddressed due to the high computational demand of MPC for evasive action in such hazardous scenarios. This paper offers new insights into real-time collision avoidance via the experimental imple- mentation of MPC for motion planning after a sudden and unexpected appearance of a static obstacle. As the state-of-the-art nonlinear MPC shows limited capability to provide an acceptable solution in real-time, we propose a human-like feed-forward planner to assist when the MPC optimization problem is either infeasible or unable to find a suitable solution due to the poor quality of its initial guess. We introduce the concept of maximum steering maneuver to design the feed-forward planner and mimic a human-like reaction after detecting the static obstacle on the road. Real-life experiments are conducted across various speeds and level of emergency using FPEV2-Kanon electric vehicle. Moreover, we demonstrate the effectiveness of our planning strategy via comparison with the state-of- the-art MPC motion planner.",
          "authors": [
            "Leila Gharavi",
            "Simone Baldi",
            "Yuki Hosomi",
            "Tona Sato",
            "Bart De Schutter",
            "Binh-Minh Nguyen",
            "Hiroshi Fujimoto"
          ],
          "published": "2026-02-19T16:24:40Z",
          "updated": "2026-02-19T16:24:40Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17512v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17502v1",
          "title": "Proximal powered knee placement: a case study",
          "summary": "Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.",
          "authors": [
            "Kyle R. Embry",
            "Lorenzo Vianello",
            "Jim Lipsey",
            "Frank Ursetta",
            "Michael Stephens",
            "Zhi Wang",
            "Ann M. Simon",
            "Andrea J. Ikeda",
            "Suzanne B. Finucane",
            "Shawana Anarwala",
            "Levi J. Hargrove"
          ],
          "published": "2026-02-19T16:16:20Z",
          "updated": "2026-02-19T16:16:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17502v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17474v1",
          "title": "Optically Sensorized Electro-Ribbon Actuator (OS-ERA)",
          "summary": "Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.",
          "authors": [
            "Carolina Gay",
            "Petr Trunin",
            "Diana Cafiso",
            "Yuejun Xu",
            "Majid Taghavi",
            "Lucia Beccai"
          ],
          "published": "2026-02-19T15:38:22Z",
          "updated": "2026-02-19T15:38:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17474v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17472v1",
          "title": "A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras",
          "summary": "Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment. To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously. Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.",
          "authors": [
            "Mohamed Sabry",
            "Joseba Gorospe",
            "Cristina Olaverri-Monreal"
          ],
          "published": "2026-02-19T15:37:18Z",
          "updated": "2026-02-19T15:37:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17472v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17421v1",
          "title": "3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing",
          "summary": "Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots",
          "authors": [
            "Diana Cafiso",
            "Petr Trunin",
            "Carolina Gay",
            "Lucia Beccai"
          ],
          "published": "2026-02-19T14:54:00Z",
          "updated": "2026-02-19T14:54:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17421v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17415v1",
          "title": "Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace",
          "summary": "We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.",
          "authors": [
            "Yi Zhang",
            "Omar Faris",
            "Chapa Sirithunge",
            "Kai-Fung Chu",
            "Fumiya Iida",
            "Fulvio Forni"
          ],
          "published": "2026-02-19T14:45:10Z",
          "updated": "2026-02-19T14:45:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17415v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17407v1",
          "title": "Bluetooth Phased-array Aided Inertial Navigation Using Factor Graphs: Experimental Verification",
          "summary": "Phased-array Bluetooth systems have emerged as a low-cost alternative for performing aided inertial navigation in GNSS-denied use cases such as warehouse logistics, drone landings, and autonomous docking. Basing a navigation system off of commercial-off-the-shelf components may reduce the barrier of entry for phased-array radio navigation systems, albeit at the cost of significantly noisier measurements and relatively short feasible range. In this paper, we compare robust estimation strategies for a factor graph optimisation-based estimator using experimental data collected from multirotor drone flight. We evaluate performance in loss-of-GNSS scenarios when aided by Bluetooth angular measurements, as well as range or barometric pressure.",
          "authors": [
            "Glen Hjelmerud Mørkbak Sørensen",
            "Torleiv H. Bryne",
            "Kristoffer Gryte",
            "Tor Arne Johansen"
          ],
          "published": "2026-02-19T14:34:04Z",
          "updated": "2026-02-19T14:34:04Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17407v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17393v1",
          "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots",
          "summary": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git",
          "authors": [
            "Minxing Sun",
            "Yao Mao"
          ],
          "published": "2026-02-19T14:18:26Z",
          "updated": "2026-02-19T14:18:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SP"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17393v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17393v2",
          "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots",
          "summary": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git",
          "authors": [
            "Minxing Sun",
            "Yao Mao"
          ],
          "published": "2026-02-19T14:18:26Z",
          "updated": "2026-02-20T10:01:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SP"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17393v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17745v1",
          "title": "Driving-Over Detection in the Railway Environment",
          "summary": "To enable fully automated driving of trains, numerous new technological components must be introduced into the railway system. Tasks that are nowadays carried out by the operating stuff, need to be taken over by automatic systems. Therefore, equipment for automatic train operation and observing the environment is needed. Here, an important task is the detection of collisions, including both (1) collisions with the front of the train as well as (2) collisions with the wheel, corresponding to an driving-over event. Technologies for detecting the driving-over events are barely investigated nowadays. Therefore, detailed driving-over experiments were performed to gather knowledge for fully automated rail operations, using a variety of objects made from steel, wood, stone and bones. Based on the captured test data, three methods were developed to detect driving-over events automatically. The first method is based on convolutional neural networks and the other two methods are classical threshold-based approaches. The neural network based approach provides an mean accuracy of 99.6% while the classical approaches show 85% and 88.6%, respectively.",
          "authors": [
            "Tobias Herrmann",
            "Nikolay Chenkov",
            "Florian Stark",
            "Matthias Härter",
            "Martin Köppel"
          ],
          "published": "2026-02-19T12:58:31Z",
          "updated": "2026-02-19T12:58:31Z",
          "primary_category": "eess.SP",
          "categories": [
            "eess.SP",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17745v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17259v1",
          "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment",
          "summary": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.",
          "authors": [
            "Han Zhao",
            "Jingbo Wang",
            "Wenxuan Song",
            "Shuai Chen",
            "Yang Liu",
            "Yan Wang",
            "Haoang Li",
            "Donglin Wang"
          ],
          "published": "2026-02-19T11:00:46Z",
          "updated": "2026-02-19T11:00:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17259v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17226v1",
          "title": "Multi-session Localization and Mapping Exploiting Topological Information",
          "summary": "Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.",
          "authors": [
            "Lorenzo Montano-Olivan",
            "Julio A. Placed",
            "Luis Montano",
            "Maria T. Lazaro"
          ],
          "published": "2026-02-19T10:17:46Z",
          "updated": "2026-02-19T10:17:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17226v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17199v1",
          "title": "Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place",
          "summary": "This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.",
          "authors": [
            "Antonio Rapuano",
            "Yaolei Shen",
            "Federico Califano",
            "Chiara Gabellieri",
            "Antonio Franchi"
          ],
          "published": "2026-02-19T09:38:32Z",
          "updated": "2026-02-19T09:38:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17199v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17182v1",
          "title": "NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting",
          "summary": "Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.",
          "authors": [
            "Jiwei Shan",
            "Zeyu Cai",
            "Yirui Li",
            "Yongbo Chen",
            "Lijun Han",
            "Yun-hui Liu",
            "Hesheng Wang",
            "Shing Shin Cheng"
          ],
          "published": "2026-02-19T09:03:47Z",
          "updated": "2026-02-19T09:03:47Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17182v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17166v1",
          "title": "Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft",
          "summary": "We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.",
          "authors": [
            "Antonio Franchi",
            "Chiara Gabellieri"
          ],
          "published": "2026-02-19T08:26:17Z",
          "updated": "2026-02-19T08:26:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17166v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17128v1",
          "title": "Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy",
          "summary": "Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.",
          "authors": [
            "Huishi Huang",
            "Jack Klusmann",
            "Haozhe Wang",
            "Shuchen Ji",
            "Fengkang Ying",
            "Yiyuan Zhang",
            "John Nassour",
            "Gordon Cheng",
            "Daniela Rus",
            "Jun Liu",
            "Marcelo H Ang",
            "Cecilia Laschi"
          ],
          "published": "2026-02-19T06:56:47Z",
          "updated": "2026-02-19T06:56:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17128v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17124v1",
          "title": "3D Scene Rendering with Multimodal Gaussian Splatting",
          "summary": "3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.",
          "authors": [
            "Chi-Shiang Gau",
            "Konstantinos D. Polyzos",
            "Athanasios Bacharis",
            "Saketh Madhuvarasu",
            "Tara Javidi"
          ],
          "published": "2026-02-19T06:49:53Z",
          "updated": "2026-02-19T06:49:53Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17124v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17110v1",
          "title": "Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching",
          "summary": "A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.",
          "authors": [
            "Tanisha Parulekar",
            "Ge Shi",
            "Josh Pinskier",
            "David Howard",
            "Jen Jen Chung"
          ],
          "published": "2026-02-19T06:12:29Z",
          "updated": "2026-02-19T06:12:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17110v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17101v1",
          "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success",
          "summary": "3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.",
          "authors": [
            "Varun Burde",
            "Pavel Burget",
            "Torsten Sattler"
          ],
          "published": "2026-02-19T05:55:01Z",
          "updated": "2026-02-19T05:55:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17101v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17049v1",
          "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
          "summary": "Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.",
          "authors": [
            "Seoyoung Lee",
            "Seobin Yoon",
            "Seongbeen Lee",
            "Yoojung Chun",
            "Dayoung Park",
            "Doyeon Kim",
            "Joo Yong Sim"
          ],
          "published": "2026-02-19T03:42:15Z",
          "updated": "2026-02-19T03:42:15Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17049v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17030v1",
          "title": "Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings",
          "summary": "As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.",
          "authors": [
            "Eric Chen",
            "Patricia Alves-Oliveira"
          ],
          "published": "2026-02-19T02:51:30Z",
          "updated": "2026-02-19T02:51:30Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17030v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16975v1",
          "title": "\"It's like a pet...but my pet doesn't collect data about me\": Multi-person Households' Privacy Design Preferences for Household Robots",
          "summary": "Household robots boasting mobility, more sophisticated sensors, and powerful processing models have become increasingly prevalent in the commercial market. However, these features may expose users to unwanted privacy risks, including unsolicited data collection and unauthorized data sharing. While security and privacy researchers thus far have explored people's privacy concerns around household robots, literature investigating people's preferred privacy designs and mitigation strategies is still limited. Additionally, the existing literature has not yet accounted for multi-user perspectives on privacy design and household robots. We aimed to fill this gap by conducting in-person participatory design sessions with 15 households to explore how they would design a privacy-aware household robot based on their concerns and expectations. We found that participants did not trust that robots, or their respective manufacturers, would respect the data privacy of household members or operate in a multi-user ecosystem without jeopardizing users' personal data. Based on these concerns, they generated designs that gave them authority over their data, contained accessible controls and notification systems, and could be customized and tailored to suit the needs and preferences of each user over time. We synthesize our findings into actionable design recommendations for robot manufacturers and developers.",
          "authors": [
            "Jennica Li",
            "Shirley Zhang",
            "Dakota Sullivan",
            "Bengisu Cagiltay",
            "Heather Kirkorian",
            "Bilge Mutlu",
            "Kassem Fawaz"
          ],
          "published": "2026-02-19T00:30:00Z",
          "updated": "2026-02-19T00:30:00Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16975v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17737v1",
          "title": "Nested Training for Mutual Adaptation in Human-AI Teaming",
          "summary": "Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.",
          "authors": [
            "Upasana Biswas",
            "Durgesh Kalwar",
            "Subbarao Kambhampati",
            "Sarath Sreedharan"
          ],
          "published": "2026-02-18T23:07:48Z",
          "updated": "2026-02-18T23:07:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17737v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16911v1",
          "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations",
          "summary": "Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.",
          "authors": [
            "Adrian Röfer",
            "Nick Heppert",
            "Abhinav Valada"
          ],
          "published": "2026-02-18T21:54:35Z",
          "updated": "2026-02-18T21:54:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16911v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16898v1",
          "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation",
          "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.",
          "authors": [
            "Iman Ahmadi",
            "Mehrshad Taji",
            "Arad Mahdinezhad Kashani",
            "AmirHossein Jadidi",
            "Saina Kashani",
            "Babak Khalaj"
          ],
          "published": "2026-02-18T21:28:56Z",
          "updated": "2026-02-18T21:28:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16898v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16898v2",
          "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
          "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.MALLVi present a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.",
          "authors": [
            "Iman Ahmadi",
            "Mehrshad Taji",
            "Arad Mahdinezhad Kashani",
            "AmirHossein Jadidi",
            "Saina Kashani",
            "Babak Khalaj"
          ],
          "published": "2026-02-18T21:28:56Z",
          "updated": "2026-02-20T10:41:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16898v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16898v3",
          "title": "MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation",
          "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings. MALLVI presents a Multi Agent Large Language and Vision framework that enables closed-loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVI generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step. Rather than using a single model, MALLVI coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning. Experiments in simulation and real-world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks. Code available at https://github.com/iman1234ahmadi/MALLVI .",
          "authors": [
            "Iman Ahmadi",
            "Mehrshad Taji",
            "Arad Mahdinezhad Kashani",
            "AmirHossein Jadidi",
            "Saina Kashani",
            "Babak Khalaj"
          ],
          "published": "2026-02-18T21:28:56Z",
          "updated": "2026-02-25T11:49:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16898v3",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.20180v1",
          "title": "Is Robot Labor Labor? Delivery Robots and the Politics of Work in Public Space",
          "summary": "As sidewalk delivery robots become increasingly integrated into urban life, this paper begins with a critical provocation: Is robot labor labor? More than a rhetorical question, this inquiry invites closer attention to the social and political arrangements that robot labor entails. Drawing on ethnographic fieldwork across two smart-city districts in Seoul, we examine how delivery robot labor is collectively sustained. While robotic actions are often framed as autonomous and efficient, we show that each successful delivery is in fact a distributed sociotechnical achievement--reliant on human labor, regulatory coordination, and social accommodations. We argue that delivery robots do not replace labor but reconfigure it--rendering some forms more visible (robotic performance) while obscuring others (human and institutional support). Unlike industrial robots, delivery robots operate in shared public space, engage everyday passersby, and are embedded in policy and progress narratives. In these spaces, we identify \"robot privilege\"--humans routinely yielding to robots--and distinct perceptions between casual observers (\"cute\") and everyday coexisters (\"admirable\"). We contribute a conceptual reframing of robot labor as a collective assemblage, empirical insights into South Korea's smart-city automation, and a call for HRI to engage more deeply with labor and spatial politics to better theorize public-facing robots.",
          "authors": [
            "EunJeong Cheon",
            "Do Yeon Shin"
          ],
          "published": "2026-02-18T20:57:58Z",
          "updated": "2026-02-18T20:57:58Z",
          "primary_category": "cs.CY",
          "categories": [
            "cs.CY",
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.20180v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16870v1",
          "title": "Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads",
          "summary": "The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.",
          "authors": [
            "Daniil Lisus",
            "Katya M. Papais",
            "Cedric Le Gentil",
            "Elliot Preston-Krebs",
            "Andrew Lambert",
            "Keith Y. K. Leung",
            "Timothy D. Barfoot"
          ],
          "published": "2026-02-18T20:53:59Z",
          "updated": "2026-02-18T20:53:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.DB"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16870v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16863v1",
          "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation",
          "summary": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.",
          "authors": [
            "Kushal Kedia",
            "Tyler Ga Wei Lum",
            "Jeannette Bohg",
            "C. Karen Liu"
          ],
          "published": "2026-02-18T20:42:39Z",
          "updated": "2026-02-18T20:42:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16863v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16863v2",
          "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation",
          "summary": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.",
          "authors": [
            "Kushal Kedia",
            "Tyler Ga Wei Lum",
            "Jeannette Bohg",
            "C. Karen Liu"
          ],
          "published": "2026-02-18T20:42:39Z",
          "updated": "2026-02-24T17:10:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16863v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16861v1",
          "title": "\"Hello, I'm Delivering. Let Me Pass By\": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets",
          "summary": "As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.",
          "authors": [
            "EunJeong Cheon",
            "Do Yeon Shin"
          ],
          "published": "2026-02-18T20:42:03Z",
          "updated": "2026-02-18T20:42:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16861v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16846v1",
          "title": "Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations",
          "summary": "Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.",
          "authors": [
            "Xili Yi",
            "Ying Xing",
            "Zachary Manchester",
            "Nima Fazeli"
          ],
          "published": "2026-02-18T20:18:40Z",
          "updated": "2026-02-18T20:18:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16846v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16825v1",
          "title": "RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness",
          "summary": "Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration. We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.",
          "authors": [
            "Ahmad Ahmad",
            "Shuo Liu",
            "Roberto Tron",
            "Calin Belta"
          ],
          "published": "2026-02-18T19:45:43Z",
          "updated": "2026-02-18T19:45:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16825v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16712v1",
          "title": "One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation",
          "summary": "Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.",
          "authors": [
            "Zhenyu Wei",
            "Yunchao Yao",
            "Mingyu Ding"
          ],
          "published": "2026-02-18T18:59:57Z",
          "updated": "2026-02-18T18:59:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16712v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16710v1",
          "title": "EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data",
          "summary": "Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.",
          "authors": [
            "Ruijie Zheng",
            "Dantong Niu",
            "Yuqi Xie",
            "Jing Wang",
            "Mengda Xu",
            "Yunfan Jiang",
            "Fernando Castañeda",
            "Fengyuan Hu",
            "You Liang Tan",
            "Letian Fu",
            "Trevor Darrell",
            "Furong Huang",
            "Yuke Zhu",
            "Danfei Xu",
            "Linxi Fan"
          ],
          "published": "2026-02-18T18:59:05Z",
          "updated": "2026-02-18T18:59:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16710v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16705v1",
          "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
          "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
          "authors": [
            "Runpei Dong",
            "Ziyan Li",
            "Xialin He",
            "Saurabh Gupta"
          ],
          "published": "2026-02-18T18:55:02Z",
          "updated": "2026-02-18T18:55:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16705v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16705v2",
          "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
          "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
          "authors": [
            "Runpei Dong",
            "Ziyan Li",
            "Xialin He",
            "Saurabh Gupta"
          ],
          "published": "2026-02-18T18:55:02Z",
          "updated": "2026-02-24T06:15:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16705v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16675v1",
          "title": "Learning to unfold cloth: Scaling up world models to deformable object manipulation",
          "summary": "Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.",
          "authors": [
            "Jack Rome",
            "Stephen James",
            "Subramanian Ramamoorthy"
          ],
          "published": "2026-02-18T18:14:41Z",
          "updated": "2026-02-18T18:14:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16675v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16641v1",
          "title": "Towards Autonomous Robotic Kidney Ultrasound: Spatial-Efficient Volumetric Imaging via Template Guided Optimal Pivoting",
          "summary": "Medical ultrasound (US) imaging is a frontline tool for the diagnosis of kidney diseases. However, traditional freehand imaging procedure suffers from inconsistent, operator-dependent outcomes, lack of 3D localization information, and risks of work-related musculoskeletal disorders. While robotic ultrasound (RUS) systems offer the potential for standardized, operator-independent 3D kidney data acquisition, the existing scanning methods lack the ability to determine the optimal imaging window for efficient imaging. As a result, the scan is often blindly performed with excessive probe footprint, which frequently leads to acoustic shadowing and incomplete organ coverage. Consequently, there is a critical need for a spatially efficient imaging technique that can maximize the kidney coverage through minimum probe footprint. Here, we propose an autonomous workflow to achieve efficient kidney imaging via template-guided optimal pivoting. The system first performs an explorative imaging to generate partial observations of the kidney. This data is then registered to a kidney template to estimate the organ pose. With the kidney localized, the robot executes a fixed-point pivoting sweep where the imaging plane is aligned with the kidney long axis to minimize the probe translation. The proposed method was validated in simulation and in-vivo. Simulation results indicate that a 60% exploration ratio provides optimal balance between kidney localization accuracy and scanning efficiency. In-vivo evaluation on two male subjects demonstrates a kidney localization accuracy up to 7.36 mm and 13.84 degrees. Moreover, the optimal pivoting approach shortened the probe footprint by around 75 mm when compared with the baselines. These results valid our approach of leveraging anatomical templates to align the probe optimally for volumetric sweep.",
          "authors": [
            "Xihan Ma",
            "Haichong Zhang"
          ],
          "published": "2026-02-18T17:31:11Z",
          "updated": "2026-02-18T17:31:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16641v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16764v1",
          "title": "Machine Learning Argument of Latitude Error Model for LEO Satellite Orbit and Covariance Correction",
          "summary": "Low Earth orbit (LEO) satellites are leveraged to support new position, navigation, and timing (PNT) service alternatives to GNSS. These alternatives require accurate propagation of satellite position and velocity with a realistic quantification of uncertainty. It is commonly assumed that the propagated uncertainty distribution is Gaussian; however, the validity of this assumption can be quickly compromised by the mismodeling of atmospheric drag. We develop a machine learning approach that corrects error growth in the argument of latitude for a diverse set of LEO satellites. The improved orbit propagation accuracy extends the applicability of the Gaussian assumption and modeling of the errors with a corrected mean and covariance. We compare the performance of a time-conditioned neural network and a Gaussian Process on datasets computed with an open source orbit propagator and publicly available Vector Covariance Message (VCM) ephemerides. The learned models predict the argument of latitude error as a Gaussian distribution given parameters from a single VCM epoch and reverse propagation errors. We show that this one-dimensional model captures the effect of mismodeled drag, which can be mapped to the Cartesian state space. The correction method only updates information along the dimensions of dominant error growth, while maintaining the physics-based propagation of VCM covariance in the remaining dimensions. We therefore extend the utility of VCM ephemerides to longer time horizons without modifying the functionality of the existing propagator.",
          "authors": [
            "Alex Moody",
            "Penina Axelrad",
            "Rebecca Russell"
          ],
          "published": "2026-02-18T17:23:14Z",
          "updated": "2026-02-18T17:23:14Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16764v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16598v2",
          "title": "Sensor Query Schedule and Sensor Noise Covariances for Accuracy-constrained Trajectory Estimation",
          "summary": "Trajectory estimation involves determining the trajectory of a mobile robot by combining prior knowledge about its dynamic model with noisy observations of its state obtained using sensors. The accuracy of such a procedure is dictated by the system model fidelity and the sensor parameters, such as the accuracy of the sensor (as represented by its noise covariance) and the rate at which it can generate observations, referred to as the sensor query schedule. Intuitively, high-rate measurements from accurate sensors lead to accurate trajectory estimation. However, cost and resource constraints limit the sensor accuracy and its measurement rate. Our work's novel contribution is the estimation of sensor schedules and sensor covariances necessary to achieve a specific estimation accuracy. Concretely, we focus on estimating: (i) the rate or schedule with which a sensor of known covariance must generate measurements to achieve specific estimation accuracy, and alternatively, (ii) the sensor covariance necessary to achieve specific estimation accuracy for a given sensor update rate. We formulate the problem of estimating these sensor parameters as semidefinite programs, which can be solved by off-the-shelf solvers. We validate our approach in simulation and real experiments by showing that the sensor schedules and the sensor covariances calculated using our proposed method achieve the desired trajectory estimation accuracy. Our method also identifies scenarios where certain estimation accuracy is unachievable with the given system and sensor characteristics.",
          "authors": [
            "Abhishek Goudar",
            "Angela P. Schoellig"
          ],
          "published": "2026-02-18T16:52:28Z",
          "updated": "2026-02-19T02:48:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16598v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16594v2",
          "title": "Decentralized and Fully Onboard: Range-Aided Cooperative Localization and Navigation on Micro Aerial Vehicles",
          "summary": "Controlling a team of robots in a coordinated manner is challenging because centralized approaches (where all computation is performed on a central machine) scale poorly, and globally referenced external localization systems may not always be available. In this work, we consider the problem of range-aided decentralized localization and formation control. In such a setting, each robot estimates its relative pose by combining data only from onboard odometry sensors and distance measurements to other robots in the team. Additionally, each robot calculates the control inputs necessary to collaboratively navigate an environment to accomplish a specific task, for example, moving in a desired formation while monitoring an area. We present a block coordinate descent approach to localization that does not require strict coordination between the robots. We present a novel formulation for formation control as inference on factor graphs that takes into account the state estimation uncertainty and can be solved efficiently. Our approach to range-aided localization and formation-based navigation is completely decentralized, does not require specialized trajectories to maintain formation, and achieves decimeter-level positioning and formation control accuracy. We demonstrate our approach through multiple real experiments involving formation flights in diverse indoor and outdoor environments.",
          "authors": [
            "Abhishek Goudar",
            "Angela P. Schoellig"
          ],
          "published": "2026-02-18T16:46:15Z",
          "updated": "2026-02-19T02:49:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16594v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16511v1",
          "title": "VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety",
          "summary": "Reliable fall recovery is critical for humanoids operating in cluttered environments. Unlike quadrupeds or wheeled robots, humanoids experience high-energy impacts, complex whole-body contact, and large viewpoint changes during a fall, making recovery essential for continued operation. Existing methods fragment fall safety into separate problems such as fall avoidance, impact mitigation, and stand-up recovery, or rely on end-to-end policies trained without vision through reinforcement learning or imitation learning, often on flat terrain. At a deeper level, fall safety is treated as monolithic data complexity, coupling pose, dynamics, and terrain and requiring exhaustive coverage, limiting scalability and generalization. We present a unified fall safety approach that spans all phases of fall recovery. It builds on two insights: 1) Natural human fall and recovery poses are highly constrained and transferable from flat to complex terrain through alignment, and 2) Fast whole-body reactions require integrated perceptual-motor representations. We train a privileged teacher using sparse human demonstrations on flat terrain and simulated complex terrains, and distill it into a deployable student that relies only on egocentric depth and proprioception. The student learns how to react by matching the teacher's goal-in-context latent representation, which combines the next target pose with the local terrain, rather than separately encoding what it must perceive and how it must act. Results in simulation and on a real Unitree G1 humanoid demonstrate robust, zero-shot fall safety across diverse non-flat environments without real-world fine-tuning. The project page is available at https://vigor2026.github.io/",
          "authors": [
            "Osher Azulay",
            "Zhengjie Xu",
            "Andrew Scheffer",
            "Stella X. Yu"
          ],
          "published": "2026-02-18T14:57:33Z",
          "updated": "2026-02-18T14:57:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16511v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16462v1",
          "title": "Reactive Motion Generation With Particle-Based Perception in Dynamic Environments",
          "summary": "Reactive motion generation in dynamic and unstructured scenarios is typically subject to essentially static perception and system dynamics. Reliably modeling dynamic obstacles and optimizing collision-free trajectories under perceptive and control uncertainty are challenging. This article focuses on revealing tight connection between reactive planning and dynamic mapping for manipulators from a model-based perspective. To enable efficient particle-based perception with expressively dynamic property, we present a tensorized particle weight update scheme that explicitly maintains obstacle velocities and covariance meanwhile. Building upon this dynamic representation, we propose an obstacle-aware MPPI-based planning formulation that jointly propagates robot-obstacle dynamics, allowing future system motion to be predicted and evaluated under uncertainty. The model predictive method is shown to significantly improve safety and reactivity with dynamic surroundings. By applying our complete framework in simulated and noisy real-world environments, we demonstrate that explicit modeling of robot-obstacle dynamics consistently enhances performance over state-of-the-art MPPI-based perception-planning baselines avoiding multiple static and dynamic obstacles.",
          "authors": [
            "Xiyuan Zhao",
            "Huijun Li",
            "Lifeng Zhu",
            "Zhikai Wei",
            "Xianyi Zhu",
            "Aiguo Song"
          ],
          "published": "2026-02-18T13:48:54Z",
          "updated": "2026-02-18T13:48:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16462v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16444v2",
          "title": "RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation",
          "summary": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.",
          "authors": [
            "Yixue Zhang",
            "Kun Wu",
            "Zhi Gao",
            "Zhen Zhao",
            "Pei Ren",
            "Zhiyuan Xu",
            "Fei Liao",
            "Xinhua Wang",
            "Shichao Fan",
            "Di Wu",
            "Qiuxuan Feng",
            "Meng Li",
            "Zhengping Che",
            "Chang Liu",
            "Jian Tang"
          ],
          "published": "2026-02-18T13:29:43Z",
          "updated": "2026-02-19T04:26:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16444v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16758v1",
          "title": "Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot",
          "summary": "This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.",
          "authors": [
            "Sina Akhbari",
            "Mehran Mahboubkhah"
          ],
          "published": "2026-02-18T11:57:52Z",
          "updated": "2026-02-18T11:57:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16758v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16371v1",
          "title": "Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped",
          "summary": "SLOT (Soft Legged Omnidirectional Tetrapod), a tendon-driven soft quadruped robot with 3D-printed TPU legs, is presented to study physics-informed modeling and control of compliant legged locomotion using only four actuators. Each leg is modeled as a deformable continuum using discrete Cosserat rod theory, enabling the capture of large bending deformations, distributed elasticity, tendon actuation, and ground contact interactions. A modular whole-body modeling framework is introduced, in which compliant leg dynamics are represented through physically consistent reaction forces applied to a rigid torso, providing a scalable interface between continuum soft limbs and rigid-body locomotion dynamics. This formulation allows efficient whole-body simulation and real-time control without sacrificing physical fidelity. The proposed model is embedded into a convex model predictive control framework that optimizes ground reaction forces over a 0.495 s prediction horizon and maps them to tendon actuation through a physics-informed force-angle relationship. The resulting controller achieves asymptotic stability under diverse perturbations. The framework is experimentally validated on a physical prototype during crawling and walking gaits, achieving high accuracy with less than 5 mm RMSE in center of mass trajectories. These results demonstrate a generalizable approach for integrating continuum soft legs into model-based locomotion control, advancing scalable and reusable modeling and control methods for soft quadruped robots.",
          "authors": [
            "Saumya Karan",
            "Neerav Maram",
            "Suraj Borate",
            "Madhu Vadali"
          ],
          "published": "2026-02-18T11:14:22Z",
          "updated": "2026-02-18T11:14:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16371v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16365v1",
          "title": "Markerless 6D Pose Estimation and Position-Based Visual Servoing for Endoscopic Continuum Manipulators",
          "summary": "Continuum manipulators in flexible endoscopic surgical systems offer high dexterity for minimally invasive procedures; however, accurate pose estimation and closed-loop control remain challenging due to hysteresis, compliance, and limited distal sensing. Vision-based approaches reduce hardware complexity but are often constrained by limited geometric observability and high computational overhead, restricting real-time closed-loop applicability. This paper presents a unified framework for markerless stereo 6D pose estimation and position-based visual servoing of continuum manipulators. A photo-realistic simulation pipeline enables large-scale automatic training with pixel-accurate annotations. A stereo-aware multi-feature fusion network jointly exploits segmentation masks, keypoints, heatmaps, and bounding boxes to enhance geometric observability. To enforce geometric consistency without iterative optimization, a feed-forward rendering-based refinement module predicts residual pose corrections in a single pass. A self-supervised sim-to-real adaptation strategy further improves real-world performance using unlabeled data. Extensive real-world validation achieves a mean translation error of 0.83 mm and a mean rotation error of 2.76° across 1,000 samples. Markerless closed-loop visual servoing driven by the estimated pose attains accurate trajectory tracking with a mean translation error of 2.07 mm and a mean rotation error of 7.41°, corresponding to 85% and 59% reductions compared to open-loop control, together with high repeatability in repeated point-reaching tasks. To the best of our knowledge, this work presents the first fully markerless pose-estimation-driven position-based visual servoing framework for continuum manipulators, enabling precise closed-loop control without physical markers or embedded sensing.",
          "authors": [
            "Junhyun Park",
            "Chunggil An",
            "Myeongbo Park",
            "Ihsan Ullah",
            "Sihyeong Park",
            "Minho Hwang"
          ],
          "published": "2026-02-18T11:08:32Z",
          "updated": "2026-02-18T11:08:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16365v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16360v1",
          "title": "Docking and Persistent Operations for a Resident Underwater Vehicle",
          "summary": "Our understanding of the oceans remains limited by sparse and infrequent observations, primarily because current methods are constrained by the high cost and logistical effort of underwater monitoring, relying either on sporadic surveys across broad areas or on long-term measurements at fixed locations. To overcome these limitations, monitoring systems must enable persistent and autonomous operations without the need for continuous surface support. Despite recent advances, resident underwater vehicles remain uncommon due to persistent challenges in autonomy, robotic resilience, and mechanical robustness, particularly under long-term deployment in harsh and remote environments. This work addresses these problems by presenting the development, deployment, and operation of a resident infrastructure using a docking station with a mini-class Remotely Operated Vehicle (ROV) at 90m depth. The ROVis equipped with enhanced onboard processing and perception, allowing it to autonomously navigate using USBL signals, dock via ArUco marker-based visual localisation fused through an Extended Kalman Filter, and carry out local inspection routines. The system demonstrated a 90% autonomous docking success rate and completed full inspection missions within four minutes, validating the integration of acoustic and visual navigation in real-world conditions. These results show that reliable, untethered operations at depth are feasible, highlighting the potential of resident ROV systems for scalable, cost-effective underwater monitoring.",
          "authors": [
            "Leonard Günzel",
            "Gabrielė Kasparavičiūtė",
            "Ambjørn Grimsrud Waldum",
            "Bjørn-Magnus Moslått",
            "Abubakar Aliyu Badawi",
            "Celil Yılmaz",
            "Md Shamin Yeasher Yousha",
            "Robert Staven",
            "Martin Ludvigsen"
          ],
          "published": "2026-02-18T10:50:04Z",
          "updated": "2026-02-18T10:50:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16360v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16358v1",
          "title": "System Identification under Constraints and Disturbance: A Bayesian Estimation Approach",
          "summary": "We introduce a Bayesian system identification (SysID) framework for jointly estimating robot's state trajectories and physical parameters with high accuracy. It embeds physically consistent inverse dynamics, contact and loop-closure constraints, and fully featured joint friction models as hard, stage-wise equality constraints. It relies on energy-based regressors to enhance parameter observability, supports both equality and inequality priors on inertial and actuation parameters, enforces dynamically consistent disturbance projections, and augments proprioceptive measurements with energy observations to disambiguate nonlinear friction effects. To ensure scalability, we derive a parameterized equality-constrained Riccati recursion that preserves the banded structure of the problem, achieving linear complexity in the time horizon, and develop computationally efficient derivatives. Simulation studies on representative robotic systems, together with hardware experiments on a Unitree B1 equipped with a Z1 arm, demonstrate faster convergence, lower inertial and friction estimation errors, and improved contact consistency compared to forward-dynamics and decoupled identification baselines. When deployed within model predictive control frameworks, the resulting models yield measurable improvements in tracking performance during locomotion over challenging environments.",
          "authors": [
            "Sergi Martinez",
            "Steve Tonneau",
            "Carlos Mastalli"
          ],
          "published": "2026-02-18T10:45:30Z",
          "updated": "2026-02-18T10:45:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16358v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16356v1",
          "title": "Articulated 3D Scene Graphs for Open-World Mobile Manipulation",
          "summary": "Semantics has enabled 3D scene understanding and affordance-driven object interaction. However, robots operating in real-world environments face a critical limitation: they cannot anticipate how objects move. Long-horizon mobile manipulation requires closing the gap between semantics, geometry, and kinematics. In this work, we present MoMa-SG, a novel framework for building semantic-kinematic 3D scene graphs of articulated scenes containing a myriad of interactable objects. Given RGB-D sequences containing multiple object articulations, we temporally segment object interactions and infer object motion using occlusion-robust point tracking. We then lift point trajectories into 3D and estimate articulation models using a novel unified twist estimation formulation that robustly estimates revolute and prismatic joint parameters in a single optimization pass. Next, we associate objects with estimated articulations and detect contained objects by reasoning over parent-child relations at identified opening states. We also introduce the novel Arti4D-Semantic dataset, which uniquely combines hierarchical object semantics including parent-child relation labels with object axis annotations across 62 in-the-wild RGB-D sequences containing 600 object interactions and three distinct observation paradigms. We extensively evaluate the performance of MoMa-SG on two datasets and ablate key design choices of our approach. In addition, real-world experiments on both a quadruped and a mobile manipulator demonstrate that our semantic-kinematic scene graphs enable robust manipulation of articulated objects in everyday home environments. We provide code and data at: https://momasg.cs.uni-freiburg.de.",
          "authors": [
            "Martin Büchner",
            "Adrian Röfer",
            "Tim Engelbracht",
            "Tim Welschehold",
            "Zuria Bauer",
            "Hermann Blum",
            "Marc Pollefeys",
            "Abhinav Valada"
          ],
          "published": "2026-02-18T10:40:35Z",
          "updated": "2026-02-18T10:40:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16356v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16353v1",
          "title": "Dual-Quadruped Collaborative Transportation in Narrow Environments via Safe Reinforcement Learning",
          "summary": "Collaborative transportation, where multiple robots collaboratively transport a payload, has garnered significant attention in recent years. While ensuring safe and high-performance inter-robot collaboration is critical for effective task execution, it is difficult to pursue in narrow environments where the feasible region is extremely limited. To address this challenge, we propose a novel approach for dual-quadruped collaborative transportation via safe reinforcement learning (RL). Specifically, we model the task as a fully cooperative constrained Markov game, where collision avoidance is formulated as constraints. We introduce a cost-advantage decomposition method that enforces the sum of team constraints to remain below an upper bound, thereby guaranteeing task safety within an RL framework. Furthermore, we propose a constraint allocation method that assigns shared constraints to individual robots to maximize the overall task reward, encouraging autonomous task-assignment among robots, thereby improving collaborative task performance. Simulation and real-time experimental results demonstrate that the proposed approach achieves superior performance and a higher success rate in dual-quadruped collaborative transportation compared to existing methods.",
          "authors": [
            "Zhezhi Lei",
            "Zhihai Bi",
            "Wenxin Wang",
            "Jun Ma"
          ],
          "published": "2026-02-18T10:35:45Z",
          "updated": "2026-02-18T10:35:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16353v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16349v1",
          "title": "SCAR: Satellite Imagery-Based Calibration for Aerial Recordings",
          "summary": "We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.",
          "authors": [
            "Henry Hölzemann",
            "Michael Schleiss"
          ],
          "published": "2026-02-18T10:33:24Z",
          "updated": "2026-02-18T10:33:24Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16349v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16330v1",
          "title": "Machine Learning Driven Prediction of the Behavior of Biohybrid Actuators",
          "summary": "Skeletal muscle-based biohybrid actuators have proved to be a promising component in soft robotics, offering efficient movement. However, their intrinsic biological variability and nonlinearity pose significant challenges for controllability and predictability. To address these issues, this study investigates the application of supervised learning, a form of machine learning, to model and predict the behavior of biohybrid machines (BHMs), focusing on a muscle ring anchored on flexible polymer pillars. First, static prediction models (i.e., random forest and neural network regressors) are trained to estimate the maximum exerted force achieved from input variables such as muscle sample, electrical stimulation parameters, and baseline exerted force. Second, a dynamic modeling framework, based on Long Short-Term Memory networks, is developed to serve as a digital twin, replicating the time series of exerted forces observed in response to electrical stimulation. Both modeling approaches demonstrate high predictive accuracy. The best performance of the static models is characterized by R2 of 0.9425, whereas the dynamic model achieves R2 of 0.9956. The static models can enable optimization of muscle actuator performance for targeted applications and required force outcomes, while the dynamic model provides a foundation for developing robustly adaptive control strategies in future biohybrid robotic systems.",
          "authors": [
            "Michail-Antisthenis Tsompanas",
            "Marco Perez Hernandez",
            "Faisal Abdul-Fattah",
            "Karim Elhakim",
            "Mostafa Ibrahim",
            "Judith Fuentes",
            "Florencia Lezcano",
            "Riccardo Collu",
            "Massimo Barbaro",
            "Stefano Lai",
            "Samuel Sanchez",
            "Andrew Adamatzky"
          ],
          "published": "2026-02-18T10:11:55Z",
          "updated": "2026-02-18T10:11:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.ET"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16330v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16308v1",
          "title": "Markerless Robot Detection and 6D Pose Estimation for Multi-Agent SLAM",
          "summary": "The capability of multi-robot SLAM approaches to merge localization history and maps from different observers is often challenged by the difficulty in establishing data association. Loop closure detection between perceptual inputs of different robotic agents is easily compromised in the context of perceptual aliasing, or when perspectives differ significantly. For this reason, direct mutual observation among robots is a powerful way to connect partial SLAM graphs, but often relies on the presence of calibrated arrays of fiducial markers (e.g., AprilTag arrays), which severely limits the range of observations and frequently fails under sharp lighting conditions, e.g., reflections or overexposure. In this work, we propose a novel solution to this problem leveraging recent advances in Deep-Learning-based 6D pose estimation. We feature markerless pose estimation as part of a decentralized multi-robot SLAM system and demonstrate the benefit to the relative localization accuracy among the robotic team. The solution is validated experimentally on data recorded in a test field campaign on a planetary analogous environment.",
          "authors": [
            "Markus Rueggeberg",
            "Maximilian Ulmer",
            "Maximilian Durner",
            "Wout Boerdijk",
            "Marcus Gerhard Mueller",
            "Rudolph Triebel",
            "Riccardo Giubilato"
          ],
          "published": "2026-02-18T09:38:06Z",
          "updated": "2026-02-18T09:38:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16308v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16206v1",
          "title": "Nonplanar Model Predictive Control for Autonomous Vehicles with Recursive Sparse Gaussian Process Dynamics",
          "summary": "This paper proposes a nonplanar model predictive control (MPC) framework for autonomous vehicles operating on nonplanar terrain. To approximate complex vehicle dynamics in such environments, we develop a geometry-aware modeling approach that learns a residual Gaussian Process (GP). By utilizing a recursive sparse GP, the framework enables real-time adaptation to varying terrain geometry. The effectiveness of the learned model is demonstrated in a reference-tracking task using a Model Predictive Path Integral (MPPI) controller. Validation within a custom Isaac Sim environment confirms the framework's capability to maintain high tracking accuracy on challenging 3D surfaces.",
          "authors": [
            "Ahmad Amine",
            "Kabir Puri",
            "Viet-Anh Le",
            "Rahul Mangharam"
          ],
          "published": "2026-02-18T05:59:16Z",
          "updated": "2026-02-18T05:59:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16206v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16187v1",
          "title": "SIT-LMPC: Safe Information-Theoretic Learning Model Predictive Control for Iterative Tasks",
          "summary": "Robots executing iterative tasks in complex, uncertain environments require control strategies that balance robustness, safety, and high performance. This paper introduces a safe information-theoretic learning model predictive control (SIT-LMPC) algorithm for iterative tasks. Specifically, we design an iterative control framework based on an information-theoretic model predictive control algorithm to address a constrained infinite-horizon optimal control problem for discrete-time nonlinear stochastic systems. An adaptive penalty method is developed to ensure safety while balancing optimality. Trajectories from previous iterations are utilized to learn a value function using normalizing flows, which enables richer uncertainty modeling compared to Gaussian priors. SIT-LMPC is designed for highly parallel execution on graphics processing units, allowing efficient real-time optimization. Benchmark simulations and hardware experiments demonstrate that SIT-LMPC iteratively improves system performance while robustly satisfying system constraints.",
          "authors": [
            "Zirui Zang",
            "Ahmad Amine",
            "Nick-Marios T. Kokolakis",
            "Truong X. Nghiem",
            "Ugo Rosolia",
            "Rahul Mangharam"
          ],
          "published": "2026-02-18T05:13:45Z",
          "updated": "2026-02-18T05:13:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16187v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16182v1",
          "title": "World Model Failure Classification and Anomaly Detection for Autonomous Inspection",
          "summary": "Autonomous inspection robots for monitoring industrial sites can reduce costs and risks associated with human-led inspection. However, accurate readings can be challenging due to occlusions, limited viewpoints, or unexpected environmental conditions. We propose a hybrid framework that combines supervised failure classification with anomaly detection, enabling classification of inspection tasks as a success, known failure, or anomaly (i.e., out-of-distribution) case. Our approach uses a world model backbone with compressed video inputs. This policy-agnostic, distribution-free framework determines classifications based on two decision functions set by conformal prediction (CP) thresholds before a human observer does. We evaluate the framework on gauge inspection feeds collected from office and industrial sites and demonstrate real-time deployment on a Boston Dynamics Spot. Experiments show over 90% accuracy in distinguishing between successes, failures, and OOD cases, with classifications occurring earlier than a human observer. These results highlight the potential for robust, anticipatory failure detection in autonomous inspection tasks or as a feedback signal for model training to assess and improve the quality of training data. Project website: https://autoinspection-classification.github.io",
          "authors": [
            "Michelle Ho",
            "Muhammad Fadhil Ginting",
            "Isaac R. Ward",
            "Andrzej Reinke",
            "Mykel J. Kochenderfer",
            "Ali-akbar Agha-Mohammadi",
            "Shayegan Omidshafiei"
          ],
          "published": "2026-02-18T04:41:14Z",
          "updated": "2026-02-18T04:41:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16182v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16178v1",
          "title": "Image Measurement Method for Automatic Insertion of Forks into Inclined Pallet",
          "summary": "In order to insert a fork into a hole of a pallet by a forklift located in front of a pallet, it is necessary to control the height position, reach position, and tilt angle of the fork to match the position and orientation of the hole of the pallet. In order to make AGF (Autonomous Guided Forklift) do this automatically, we propose an image measurement method to measure the pitch inclination of the pallet in the camera coordinate system from an image obtained by using a wide-angle camera. In addition, we propose an image measurement method to easily acquire the calibration information between the camera coordinate system and the fork coordinate system necessary to apply the measurements in the camera coordinate system to the fork control. In the experiment space, a wide-angle camera was fixed at the backrest of a reach type forklift. The wide-angle images taken by placing a pallet in front of the camera were processed. As a result of evaluating the error by comparing the image measurement value with the hand measurement value when changing the pitch inclination angle of the pallet, the relative height of the pallet and the fork, and whether the pallet is loaded or not, it was confirmed that the error was within the allowable range for safely inserting the fork.",
          "authors": [
            "Nobuyuki Kita",
            "Takuro Kato"
          ],
          "published": "2026-02-18T04:32:31Z",
          "updated": "2026-02-18T04:32:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16178v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16744v1",
          "title": "ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts",
          "summary": "This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.",
          "authors": [
            "Takuro Kato",
            "Mitsuharu Morisawa"
          ],
          "published": "2026-02-18T03:10:00Z",
          "updated": "2026-02-18T03:10:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16744v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16127v1",
          "title": "Reactive Slip Control in Multifingered Grasping: Hybrid Tactile Sensing and Internal-Force Optimization",
          "summary": "We present a hybrid learning and model-based approach that adapts internal grasp forces to halt in-hand slip on a multifingered robotic gripper. A multimodal tactile stack combines piezoelectric (PzE) sensing for fast slip cues with piezoresistive (PzR) arrays for contact localization, enabling online construction of the grasp matrix. Upon slip, we update internal forces computed in the null space of the grasp via a quadratic program that preserves the object wrench while enforcing actuation limits. The pipeline yields a theoretical sensing-to-command latency of 35-40 ms, with 5 ms for PzR-based contact and geometry updates and about 4 ms for the quadratic program solve. In controlled trials, slip onset is detected at 20ms. We demonstrate closed-loop stabilization on multifingered grasps under external perturbations. Augmenting efficient analytic force control with learned tactile cues yields both robustness and rapid reactions, as confirmed in our end-to-end evaluation. Measured delays are dominated by the experimental data path rather than actual computation. The analysis outlines a clear route to sub-50 ms closed-loop stabilization.",
          "authors": [
            "Théo Ayral",
            "Saifeddine Aloui",
            "Mathieu Grossard"
          ],
          "published": "2026-02-18T01:37:07Z",
          "updated": "2026-02-18T01:37:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16127v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16073v1",
          "title": "ScenicRules: An Autonomous Driving Benchmark with Multi-Objective Specifications and Abstract Scenarios",
          "summary": "Developing autonomous driving systems for complex traffic environments requires balancing multiple objectives, such as avoiding collisions, obeying traffic rules, and making efficient progress. In many situations, these objectives cannot be satisfied simultaneously, and explicit priority relations naturally arise. Also, driving rules require context, so it is important to formally model the environment scenarios within which such rules apply. Existing benchmarks for evaluating autonomous vehicles lack such combinations of multi-objective prioritized rules and formal environment models. In this work, we introduce ScenicRules, a benchmark for evaluating autonomous driving systems in stochastic environments under prioritized multi-objective specifications. We first formalize a diverse set of objectives to serve as quantitative evaluation metrics. Next, we design a Hierarchical Rulebook framework that encodes multiple objectives and their priority relations in an interpretable and adaptable manner. We then construct a compact yet representative collection of scenarios spanning diverse driving contexts and near-accident situations, formally modeled in the Scenic language. Experimental results show that our formalized objectives and Hierarchical Rulebooks align well with human driving judgments and that our benchmark effectively exposes agent failures with respect to the prioritized objectives. Our benchmark can be accessed at https://github.com/BerkeleyLearnVerify/ScenicRules/.",
          "authors": [
            "Kevin Kai-Chun Chang",
            "Ekin Beyazit",
            "Alberto Sangiovanni-Vincentelli",
            "Tichakorn Wongpiromsarn",
            "Sanjit A. Seshia"
          ],
          "published": "2026-02-17T22:57:43Z",
          "updated": "2026-02-17T22:57:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16073v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16035v1",
          "title": "The Impact of Class Uncertainty Propagation in Perception-Based Motion Planning",
          "summary": "Autonomous vehicles (AVs) are being increasingly deployed in urban environments. In order to operate safely and reliably, AVs need to account for the inherent uncertainty associated with perceiving the world through sensor data and incorporate that into their decision-making process. Uncertainty-aware planners have recently been developed to account for upstream perception and prediction uncertainty. However, such planners may be sensitive to prediction uncertainty miscalibration, the magnitude of which has not yet been characterized. Towards this end, we perform a detailed analysis on the impact that perceptual uncertainty propagation and calibration has on perception-based motion planning. We do so by comparing two novel prediction-planning pipelines with varying levels of uncertainty propagation on the recently-released nuPlan planning benchmark. We study the impact of upstream uncertainty calibration using closed-loop evaluation on the nuPlan challenge scenarios. We find that the method incorporating upstream uncertainty propagation demonstrates superior generalization to complex closed-loop scenarios.",
          "authors": [
            "Jibran Iqbal Shah",
            "Andrei Ivanovic",
            "Kelly Zhu",
            "Masha Itkina",
            "Rowan McAllister",
            "Igor Gilitschenski",
            "Florian Shkurti"
          ],
          "published": "2026-02-17T21:42:37Z",
          "updated": "2026-02-17T21:42:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16035v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.16005v1",
          "title": "ODYN: An All-Shifted Non-Interior-Point Method for Quadratic Programming in Robotics and AI",
          "summary": "We introduce ODYN, a novel all-shifted primal-dual non-interior-point quadratic programming (QP) solver designed to efficiently handle challenging dense and sparse QPs. ODYN combines all-shifted nonlinear complementarity problem (NCP) functions with proximal method of multipliers to robustly address ill-conditioned and degenerate problems, without requiring linear independence of the constraints. It exhibits strong warm-start performance and is well suited to both general-purpose optimization, and robotics and AI applications, including model-based control, estimation, and kernel-based learning methods. We provide an open-source implementation and benchmark ODYN on the Maros-Mészáros test set, demonstrating state-of-the-art convergence performance in small-to-high-scale problems. The results highlight ODYN's superior warm-starting capabilities, which are critical in sequential and real-time settings common in robotics and AI. These advantages are further demonstrated by deploying ODYN as the backend of an SQP-based predictive control framework (OdynSQP), as the implicitly differentiable optimization layer for deep learning (ODYNLayer), and the optimizer of a contact-dynamics simulation (ODYNSim).",
          "authors": [
            "Jose Rojas",
            "Aristotelis Papatheodorou",
            "Sergi Martinez",
            "Ioannis Havoutis",
            "Carlos Mastalli"
          ],
          "published": "2026-02-17T20:52:32Z",
          "updated": "2026-02-17T20:52:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.16005v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15963v1",
          "title": "The human intention. A taxonomy attempt and its applications to robotics",
          "summary": "Despite a surge in robotics research dedicated to inferring and understanding human intent, a universally accepted definition remains elusive since existing works often equate human intention with specific task-related goals. This article seeks to address this gap by examining the multifaceted nature of intention. Drawing on insights from psychology, it attempts to consolidate a definition of intention into a comprehensible framework for a broader audience. The article classifies different types of intention based on psychological and communication studies, offering guidance to researchers shifting from pure technical enhancements to a more human-centric perspective in robotics. It then demonstrates how various robotics studies can be aligned with these intention categories. Finally, through in-depth analyses of collaborative search and object transport use cases, the article underscores the significance of considering the diverse facets of human intention.",
          "authors": [
            "J. E. Domínguez-Vidal",
            "Alberto Sanfeliu"
          ],
          "published": "2026-02-17T19:26:16Z",
          "updated": "2026-02-17T19:26:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15963v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15954v1",
          "title": "Hybrid Model Predictive Control with Physics-Informed Neural Network for Satellite Attitude Control",
          "summary": "Reliable spacecraft attitude control depends on accurate prediction of attitude dynamics, particularly when model-based strategies such as Model Predictive Control (MPC) are employed, where performance is limited by the quality of the internal system model. For spacecraft with complex dynamics, obtaining accurate physics-based models can be difficult, time-consuming, or computationally heavy. Learning-based system identification presents a compelling alternative; however, models trained exclusively on data frequently exhibit fragile stability properties and limited extrapolation capability. This work explores Physics-Informed Neural Networks (PINNs) for modeling spacecraft attitude dynamics and contrasts it with a conventional data-driven approach. A comprehensive dataset is generated using high-fidelity numerical simulations, and two learning methodologies are investigated: a purely data-driven pipeline and a physics-regularized approach that incorporates prior knowledge into the optimization process. The results indicate that embedding physical constraints during training leads to substantial improvements in predictive reliability, achieving a 68.17% decrease in mean relative error relative. When deployed within an MPC architecture, the physics-informed models yield superior closed-loop tracking performance and improved robustness to uncertainty. Furthermore, a hybrid control formulation that merges the learned nonlinear dynamics with a nominal linear model enables consistent steady-state convergence and significantly faster response, reducing settling times by 61.52%-76.42% under measurement noise and reaction wheel friction.",
          "authors": [
            "Carlo Cena",
            "Mauro Martini",
            "Marcello Chiaberge"
          ],
          "published": "2026-02-17T19:08:48Z",
          "updated": "2026-02-17T19:08:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15954v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15828v1",
          "title": "Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation",
          "summary": "Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.",
          "authors": [
            "Yuxuan Kuang",
            "Sungjae Park",
            "Katerina Fragkiadaki",
            "Shubham Tulsiani"
          ],
          "published": "2026-02-17T18:59:31Z",
          "updated": "2026-02-17T18:59:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15828v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15827v1",
          "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching",
          "summary": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.",
          "authors": [
            "Zhen Wu",
            "Xiaoyu Huang",
            "Lujie Yang",
            "Yuanhang Zhang",
            "Koushil Sreenath",
            "Xi Chen",
            "Pieter Abbeel",
            "Rocky Duan",
            "Angjoo Kanazawa",
            "Carmelo Sferrazza",
            "Guanya Shi",
            "C. Karen Liu"
          ],
          "published": "2026-02-17T18:59:11Z",
          "updated": "2026-02-17T18:59:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15827v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15817v1",
          "title": "Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning",
          "summary": "Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.",
          "authors": [
            "Oswin So",
            "Eric Yang Yu",
            "Songyuan Zhang",
            "Matthew Cleaveland",
            "Mitchell Black",
            "Chuchu Fan"
          ],
          "published": "2026-02-17T18:53:31Z",
          "updated": "2026-02-17T18:53:31Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15817v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15813v1",
          "title": "FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy",
          "summary": "Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.",
          "authors": [
            "Haochen Zhang",
            "Nirav Savaliya",
            "Faizan Siddiqui",
            "Enna Sachdeva"
          ],
          "published": "2026-02-17T18:49:43Z",
          "updated": "2026-02-17T18:49:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15813v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15767v1",
          "title": "Robot-Assisted Social Dining as a White Glove Service",
          "summary": "Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.",
          "authors": [
            "Atharva S Kashyap",
            "Ugne Aleksandra Morkute",
            "Patricia Alves-Oliveira"
          ],
          "published": "2026-02-17T17:58:25Z",
          "updated": "2026-02-17T17:58:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15767v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15755v1",
          "title": "RaCo: Ranking and Covariance for Practical Learned Keypoints",
          "summary": "This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.",
          "authors": [
            "Abhiram Shenoi",
            "Philipp Lindenberger",
            "Paul-Edouard Sarlin",
            "Marc Pollefeys"
          ],
          "published": "2026-02-17T17:39:52Z",
          "updated": "2026-02-17T17:39:52Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15755v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15733v1",
          "title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction",
          "summary": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.",
          "authors": [
            "Qiang Zhang",
            "Jiahao Ma",
            "Peiran Liu",
            "Shuai Shi",
            "Zeran Su",
            "Zifan Wang",
            "Jingkai Sun",
            "Wei Cui",
            "Jialin Yu",
            "Gang Han",
            "Wen Zhao",
            "Pihai Sun",
            "Kangning Yin",
            "Jiaxu Wang",
            "Jiahang Cao",
            "Lingfeng Zhang",
            "Hao Cheng",
            "Xiaoshuai Hao",
            "Yiding Ji",
            "Junwei Liang",
            "Jian Tang",
            "Renjing Xu",
            "Yijie Guo"
          ],
          "published": "2026-02-17T17:09:45Z",
          "updated": "2026-02-17T17:09:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15733v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15721v1",
          "title": "Lifelong Scalable Multi-Agent Realistic Testbed and A Comprehensive Study on Design Choices in Lifelong AGV Fleet Management Systems",
          "summary": "We present Lifelong Scalable Multi-Agent Realistic Testbed (LSMART), an open-source simulator to evaluate any Multi-Agent Path Finding (MAPF) algorithm in a Fleet Management System (FMS) with Automated Guided Vehicles (AGVs). MAPF aims to move a group of agents from their corresponding starting locations to their goals. Lifelong MAPF (LMAPF) is a variant of MAPF that continuously assigns new goals for agents to reach. LMAPF applications, such as autonomous warehouses, often require a centralized, lifelong system to coordinate the movement of a fleet of robots, typically AGVs. However, existing works on MAPF and LMAPF often assume simplified kinodynamic models, such as pebble motion, as well as perfect execution and communication for AGVs. Prior work has presented SMART, a software capable of evaluating any MAPF algorithms while considering agent kinodynamics, communication delays, and execution uncertainties. However, SMART is designed for MAPF, not LMAPF. Generalizing SMART to an FMS requires many more design choices. First, an FMS parallelizes planning and execution, raising the question of when to plan. Second, given planners with varying optimality and differing agent-model assumptions, one must decide how to plan. Third, when the planner fails to return valid solutions, the system must determine how to recover. In this paper, we first present LSMART, an open-source simulator that incorporates all these considerations to evaluate any MAPF algorithms in an FMS. We then provide experiment results based on state-of-the-art methods for each design choice, offering guidance on how to effectively design centralized lifelong AGV Fleet Management Systems. LSMART is available at https://smart-mapf.github.io/lifelong-smart.",
          "authors": [
            "Jingtian Yan",
            "Yulun Zhang",
            "Zhenting Liu",
            "Han Zhang",
            "He Jiang",
            "Jingkai Chen",
            "Stephen F. Smith",
            "Jiaoyang Li"
          ],
          "published": "2026-02-17T16:53:20Z",
          "updated": "2026-02-17T16:53:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15721v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15684v1",
          "title": "Estimating Human Muscular Fatigue in Dynamic Collaborative Robotic Tasks with Learning-Based Models",
          "summary": "Assessing human muscle fatigue is critical for optimizing performance and safety in physical human-robot interaction(pHRI). This work presents a data-driven framework to estimate fatigue in dynamic, cyclic pHRI using arm-mounted surface electromyography(sEMG). Subject-specific machine-learning regression models(Random Forest, XGBoost, and Linear Regression predict the fraction of cycles to fatigue(FCF) from three frequency-domain and one time-domain EMG features, and are benchmarked against a convolutional neural network(CNN) that ingests spectrograms of filtered EMG. Framing fatigue estimation as regression (rather than classification) captures continuous progression toward fatigue, supporting earlier detection, timely intervention, and adaptive robot control. In experiments with ten participants, a collaborative robot under admittance control guided repetitive lateral (left-right) end-effector motions until muscular fatigue. Average FCF RMSE across participants was 20.8+/-4.3% for the CNN, 23.3+/-3.8% for Random Forest, 24.8+/-4.5% for XGBoost, and 26.9+/-6.1% for Linear Regression. To probe cross-task generalization, one participant additionally performed unseen vertical (up-down) and circular repetitions; models trained only on lateral data were tested directly and largely retained accuracy, indicating robustness to changes in movement direction, arm kinematics, and muscle recruitment, while Linear Regression deteriorated. Overall, the study shows that both feature-based ML and spectrogram-based DL can estimate remaining work capacity during repetitive pHRI, with the CNN delivering the lowest error and the tree-based models close behind. The reported transfer to new motion patterns suggests potential for practical fatigue monitoring without retraining for every task, improving operator protection and enabling fatigue-aware shared autonomy, for safer fatigue-adaptive pHRI control.",
          "authors": [
            "Feras Kiki",
            "Pouya P. Niaz",
            "Alireza Madani",
            "Cagatay Basdogan"
          ],
          "published": "2026-02-17T16:08:11Z",
          "updated": "2026-02-17T16:08:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC",
            "eess.SP",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15684v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15642v1",
          "title": "Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing",
          "summary": "We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.",
          "authors": [
            "Alexander Wachter",
            "Alexander Willert",
            "Marc-Philip Ecker",
            "Christian Hartl-Nesic"
          ],
          "published": "2026-02-17T15:10:44Z",
          "updated": "2026-02-17T15:10:44Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15642v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15922v1",
          "title": "World Action Models are Zero-shot Policies",
          "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.",
          "authors": [
            "Seonghyeon Ye",
            "Yunhao Ge",
            "Kaiyuan Zheng",
            "Shenyuan Gao",
            "Sihyun Yu",
            "George Kurian",
            "Suneel Indupuru",
            "You Liang Tan",
            "Chuning Zhu",
            "Jiannan Xiang",
            "Ayaan Malik",
            "Kyungmin Lee",
            "William Liang",
            "Nadun Ranawaka",
            "Jiasheng Gu",
            "Yinzhen Xu",
            "Guanzhi Wang",
            "Fengyuan Hu",
            "Avnish Narayan",
            "Johan Bjorck",
            "Jing Wang",
            "Gwanghyun Kim",
            "Dantong Niu",
            "Ruijie Zheng",
            "Yuqi Xie",
            "Jimmy Wu",
            "Qi Wang",
            "Ryan Julian",
            "Danfei Xu",
            "Yilun Du",
            "Yevgen Chebotar",
            "Scott Reed",
            "Jan Kautz",
            "Yuke Zhu",
            "Linxi \"Jim\" Fan",
            "Joel Jang"
          ],
          "published": "2026-02-17T15:04:02Z",
          "updated": "2026-02-17T15:04:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15922v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15633v1",
          "title": "SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms",
          "summary": "Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.",
          "authors": [
            "Haichao Liu",
            "Yufeng Hu",
            "Shuang Wang",
            "Kangjun Guo",
            "Jun Ma",
            "Jinni Zhou"
          ],
          "published": "2026-02-17T15:03:05Z",
          "updated": "2026-02-17T15:03:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15633v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15608v1",
          "title": "Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion",
          "summary": "Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between \"grip\" and \"slip\" states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.",
          "authors": [
            "Mostafa A. Atalla",
            "Daan van Bemmel",
            "Jack Cummings",
            "Paul Breedveld",
            "Michaël Wiertlewski",
            "Aimée Sakes"
          ],
          "published": "2026-02-17T14:33:17Z",
          "updated": "2026-02-17T14:33:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "physics.app-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15608v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15567v1",
          "title": "Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions",
          "summary": "Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.",
          "authors": [
            "Jieting Long",
            "Dechuan Liu",
            "Weidong Cai",
            "Ian Manchester",
            "Weiming Zhi"
          ],
          "published": "2026-02-17T13:27:05Z",
          "updated": "2026-02-17T13:27:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15567v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15549v1",
          "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing",
          "summary": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.",
          "authors": [
            "Guoqin Tang",
            "Qingxuan Jia",
            "Gang Chen",
            "Tong Li",
            "Zeyuan Huang",
            "Zihang Lv",
            "Ning Ji"
          ],
          "published": "2026-02-17T12:54:18Z",
          "updated": "2026-02-17T12:54:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15549v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15543v1",
          "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA",
          "summary": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.",
          "authors": [
            "Young-Chae Son",
            "Jung-Woo Lee",
            "Yoon-Ji Choi",
            "Dae-Kwan Ko",
            "Soo-Chul Lim"
          ],
          "published": "2026-02-17T12:48:59Z",
          "updated": "2026-02-17T12:48:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15543v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15533v1",
          "title": "Efficient Knowledge Transfer for Jump-Starting Control Policy Learning of Multirotors through Physics-Aware Neural Architectures",
          "summary": "Efficiently training control policies for robots is a major challenge that can greatly benefit from utilizing knowledge gained from training similar systems through cross-embodiment knowledge transfer. In this work, we focus on accelerating policy training using a library-based initialization scheme that enables effective knowledge transfer across multirotor configurations. By leveraging a physics-aware neural control architecture that combines a reinforcement learning-based controller and a supervised control allocation network, we enable the reuse of previously trained policies. To this end, we utilize a policy evaluation-based similarity measure that identifies suitable policies for initialization from a library. We demonstrate that this measure correlates with the reduction in environment interactions needed to reach target performance and is therefore suited for initialization. Extensive simulation and real-world experiments confirm that our control architecture achieves state-of-the-art control performance, and that our initialization scheme saves on average up to $73.5\\%$ of environment interactions (compared to training a policy from scratch) across diverse quadrotor and hexarotor designs, paving the way for efficient cross-embodiment transfer in reinforcement learning.",
          "authors": [
            "Welf Rehberg",
            "Mihir Kulkarni",
            "Philipp Weiss",
            "Kostas Alexis"
          ],
          "published": "2026-02-17T12:31:53Z",
          "updated": "2026-02-17T12:31:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15533v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15513v1",
          "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling",
          "summary": "Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.",
          "authors": [
            "Ji Li",
            "Jing Xia",
            "Mingyi Li",
            "Shiyan Hu"
          ],
          "published": "2026-02-17T11:41:28Z",
          "updated": "2026-02-17T11:41:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15513v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15424v1",
          "title": "Lyapunov-Based $\\mathcal{L}_2$-Stable PI-Like Control of a Four-Wheel Independently Driven and Steered Robot",
          "summary": "In this letter, Lyapunov-based synthesis of a PI-like controller is proposed for $\\mathcal{L}_2$-stable motion control of an independently driven and steered four-wheel mobile robot. An explicit, structurally verified model is used to enable systematic controller design with stability and performance guarantees suitable for real-time operation. A Lyapunov function is constructed to yield explicit bounds and $\\mathcal{L}_2$ stability results, supporting feedback synthesis that reduces configuration dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation while preserving rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.",
          "authors": [
            "Branimir Ćaran",
            "Vladimir Milić",
            "Bojan Jerbić"
          ],
          "published": "2026-02-17T08:36:13Z",
          "updated": "2026-02-17T08:36:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15424v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15400v1",
          "title": "One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation",
          "summary": "A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\\% Success Rate (SR) in R2R-CE and 42.2\\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.",
          "authors": [
            "Zerui Li",
            "Hongpei Zheng",
            "Fangguo Zhao",
            "Aidan Chan",
            "Jian Zhou",
            "Sihao Lin",
            "Shijie Li",
            "Qi Wu"
          ],
          "published": "2026-02-17T07:13:48Z",
          "updated": "2026-02-17T07:13:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15400v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15398v1",
          "title": "Hybrid F' and ROS2 Architecture for Vision-Based Autonomous Flight: Design and Experimental Validation",
          "summary": "Autonomous aerospace systems require architectures that balance deterministic real-time control with advanced perception capabilities. This paper presents an integrated system combining NASA's F' flight software framework with ROS2 middleware via Protocol Buffers bridging. We evaluate the architecture through a 32.25-minute indoor quadrotor flight test using vision-based navigation. The vision system achieved 87.19 Hz position estimation with 99.90\\% data continuity and 11.47 ms mean latency, validating real-time performance requirements. All 15 ground commands executed successfully with 100 % success rate, demonstrating robust F'--PX4 integration. System resource utilization remained low (15.19 % CPU, 1,244 MB RAM) with zero stale telemetry messages, confirming efficient operation on embedded platforms. Results validate the feasibility of hybrid flight-software architectures combining certification-grade determinism with flexible autonomy for autonomous aerial vehicles.",
          "authors": [
            "Abdelrahman Metwally",
            "Monijesu James",
            "Aleksey Fedoseev",
            "Miguel Altamirano Cabrera",
            "Dzmitry Tsetserukou",
            "Andrey Somov"
          ],
          "published": "2026-02-17T07:12:06Z",
          "updated": "2026-02-17T07:12:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15398v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15397v1",
          "title": "ActionCodec: What Makes for Good Action Tokenizers",
          "summary": "Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \\textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \\textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.",
          "authors": [
            "Zibin Dong",
            "Yicheng Liu",
            "Shiduo Zhang",
            "Baijun Ye",
            "Yifu Yuan",
            "Fei Ni",
            "Jingjing Gong",
            "Xipeng Qiu",
            "Hang Zhao",
            "Yinchuan Li",
            "Jianye Hao"
          ],
          "published": "2026-02-17T07:07:15Z",
          "updated": "2026-02-17T07:07:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15397v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15357v1",
          "title": "Fluoroscopy-Constrained Magnetic Robot Control via Zernike-Based Field Modeling and Nonlinear MPC",
          "summary": "Magnetic actuation enables surgical robots to navigate complex anatomical pathways while reducing tissue trauma and improving surgical precision. However, clinical deployment is limited by the challenges of controlling such systems under fluoroscopic imaging, which provides low frame rate and noisy pose feedback. This paper presents a control framework that remains accurate and stable under such conditions by combining a nonlinear model predictive control (NMPC) framework that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter to estimate the robot state. Experimental validation is conducted with two magnetic robots in a 3D-printed fluid workspace and a spine phantom replicating drug delivery in the epidural space. Results show the proposed control method remains highly accurate when feedback is downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm), mimicking clinical fluoroscopy. In the spine phantom experiments, the proposed method successfully executed a drug delivery trajectory with a root mean square (RMS) position error of 1.18 mm while maintaining safe clearance from critical anatomical boundaries.",
          "authors": [
            "Xinhao Chen",
            "Hongkun Yao",
            "Anuruddha Bhattacharjee",
            "Suraj Raval",
            "Lamar O. Mair",
            "Yancy Diaz-Mercado",
            "Axel Krieger"
          ],
          "published": "2026-02-17T04:50:18Z",
          "updated": "2026-02-17T04:50:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15357v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15354v1",
          "title": "A Comparison of Bayesian Prediction Techniques for Mobile Robot Trajectory Tracking",
          "summary": "This paper presents a performance comparison of different estimation and prediction techniques applied to the problem of tracking multiple robots. The main performance criteria are the magnitude of the estimation or prediction error, the computational effort and the robustness of each method to non-Gaussian noise. Among the different techniques compared are the well known Kalman filters and their different variants (e.g. extended and unscented), and the more recent techniques relying on Sequential Monte Carlo Sampling methods, such as particle filters and Gaussian Mixture Sigma Point Particle Filter.",
          "authors": [
            "Jose Luis Peralta-Cabezas",
            "Miguel Torres-Torriti",
            "Marcelo Guarini-Hermann"
          ],
          "published": "2026-02-17T04:47:31Z",
          "updated": "2026-02-17T04:47:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15354v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15351v1",
          "title": "Feasibility-aware Imitation Learning from Observation with Multimodal Feedback",
          "summary": "Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.",
          "authors": [
            "Kei Takahashi",
            "Hikaru Sasaki",
            "Takamitsu Matsubara"
          ],
          "published": "2026-02-17T04:34:29Z",
          "updated": "2026-02-17T04:34:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15351v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15309v1",
          "title": "OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy",
          "summary": "Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule's predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.",
          "authors": [
            "Mostafa A. Atalla",
            "Anand S. Sekar",
            "Remi van Starkenburg",
            "David J. Jager",
            "Aimée Sakes",
            "Michaël Wiertlewski",
            "Paul Breedveld"
          ],
          "published": "2026-02-17T02:14:30Z",
          "updated": "2026-02-17T02:14:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "physics.med-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15309v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15258v1",
          "title": "SEG-JPEG: Simple Visual Semantic Communications for Remote Operation of Automated Vehicles over Unreliable Wireless Networks",
          "summary": "Remote Operation is touted as being key to the rapid deployment of automated vehicles. Streaming imagery to control connected vehicles remotely currently requires a reliable, high throughput network connection, which can be limited in real-world remote operation deployments relying on public network infrastructure. This paper investigates how the application of computer vision assisted semantic communication can be used to circumvent data loss and corruption associated with traditional image compression techniques. By encoding the segmentations of detected road users into colour coded highlights within low resolution greyscale imagery, the required data rate can be reduced by 50 \\% compared with conventional techniques, while maintaining visual clarity. This enables a median glass-to-glass latency of below 200ms even when the network data rate is below 500kbit/s, while clearly outlining salient road users to enhance situational awareness of the remote operator. The approach is demonstrated in an area of variable 4G mobile connectivity using an automated last-mile delivery vehicle. With this technique, the results indicate that large-scale deployment of remotely operated automated vehicles could be possible even on the often constrained public 4G/5G mobile network, providing the potential to expedite the nationwide roll-out of automated vehicles.",
          "authors": [
            "Sebastian Donnelly",
            "Ruth Anderson",
            "George Economides",
            "James Broughton",
            "Peter Ball",
            "Alexander Rast",
            "Andrew Bradley"
          ],
          "published": "2026-02-16T23:28:10Z",
          "updated": "2026-02-16T23:28:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15258v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15201v1",
          "title": "DexEvolve: Evolutionary Optimization for Robust and Diverse Dexterous Grasp Synthesis",
          "summary": "Dexterous grasping is fundamental to robotics, yet data-driven grasp prediction heavily relies on large, diverse datasets that are costly to generate and typically limited to a narrow set of gripper morphologies. Analytical grasp synthesis can be used to scale data collection, but necessary simplifying assumptions often yield physically infeasible grasps that need to be filtered in high-fidelity simulators, significantly reducing the total number of grasps and their diversity. We propose a scalable generate-and-refine pipeline for synthesizing large-scale, diverse, and physically feasible grasps. Instead of using high-fidelity simulators solely for verification and filtering, we leverage them as an optimization stage that continuously improves grasp quality without discarding precomputed candidates. More specifically, we initialize an evolutionary search with a seed set of analytically generated, potentially suboptimal grasps. We then refine these proposals directly in a high-fidelity simulator (Isaac Sim) using an asynchronous, gradient-free evolutionary algorithm, improving stability while maintaining diversity. In addition, this refinement stage can be guided toward human preferences and/or domain-specific quality metrics without requiring a differentiable objective. We further distill the refined grasp distribution into a diffusion model for robust real-world deployment, and highlight the role of diversity for both effective training and during deployment. Experiments on a newly introduced Handles dataset and a DexGraspNet subset demonstrate that our approach achieves over 120 distinct stable grasps per object (a 1.7-6x improvement over unrefined analytical methods) while outperforming diffusion-based alternatives by 46-60\\% in unique grasp coverage.",
          "authors": [
            "René Zurbrügg",
            "Andrei Cramariuc",
            "Marco Hutter"
          ],
          "published": "2026-02-16T21:31:45Z",
          "updated": "2026-02-16T21:31:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15201v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15181v1",
          "title": "Time-Archival Camera Virtualization for Sports and Visual Performances",
          "summary": "Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...",
          "authors": [
            "Yunxiao Zhang",
            "William Stone",
            "Suryansh Kumar"
          ],
          "published": "2026-02-16T20:39:51Z",
          "updated": "2026-02-16T20:39:51Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15181v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15162v1",
          "title": "A ROS2 Benchmarking Framework for Hierarchical Control Strategies in Mobile Robots for Mediterranean Greenhouses",
          "summary": "Mobile robots operating in agroindustrial environments, such as Mediterranean greenhouses, are subject to challenging conditions, including uneven terrain, variable friction, payload changes, and terrain slopes, all of which significantly affect control performance and stability. Despite the increasing adoption of robotic platforms in agriculture, the lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluations of control strategies under realistic operating conditions. This paper presents a comprehensive benchmarking framework for evaluating mobile robot controllers in greenhouse environments. The proposed framework integrates an accurate three dimensional model of the environment, a physics based simulator, and a hierarchical control architecture comprising low, mid, and high level control layers. Three benchmark categories are defined to enable modular assessment, ranging from actuator level control to full autonomous navigation. Additionally, three disturbance scenarios payload variation, terrain type, and slope are explicitly modeled to replicate real world agricultural conditions. To ensure objective and reproducible evaluation, standardized performance metrics are introduced, including the Squared Absolute Error (SAE), the Squared Control Input (SCI), and composite performance indices. Statistical analysis based on repeated trials is employed to mitigate the influence of sensor noise and environmental variability. The framework is further enhanced by a plugin based architecture that facilitates seamless integration of user defined controllers and planners. The proposed benchmark provides a robust and extensible tool for the quantitative comparison of classical, predictive, and planning based control strategies in realistic conditions, bridging the gap between simulation based analysis and real world agroindustrial applications.",
          "authors": [
            "Fernando Cañadas-Aránega",
            "Francisco J. Mañas-Álvarez",
            "José L- Guzmán",
            "José C. Moreno",
            "José L. Blanco-Claraco"
          ],
          "published": "2026-02-16T20:04:50Z",
          "updated": "2026-02-16T20:04:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15162v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15018v1",
          "title": "Neurosim: A Fast Simulator for Neuromorphic Robot Perception",
          "summary": "Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .",
          "authors": [
            "Richeek Das",
            "Pratik Chaudhari"
          ],
          "published": "2026-02-16T18:57:04Z",
          "updated": "2026-02-16T18:57:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15018v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15010v2",
          "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames",
          "summary": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations. Videos are available at https://bigpicturepolicies.github.io/",
          "authors": [
            "Max Sobol Mark",
            "Jacky Liang",
            "Maria Attarian",
            "Chuyuan Fu",
            "Debidatta Dwibedi",
            "Dhruv Shah",
            "Aviral Kumar"
          ],
          "published": "2026-02-16T18:49:56Z",
          "updated": "2026-02-18T07:07:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15010v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14974v1",
          "title": "DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI",
          "summary": "Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.",
          "authors": [
            "En Yu",
            "Haoran Lv",
            "Jianjian Sun",
            "Kangheng Lin",
            "Ruitao Zhang",
            "Yukang Shi",
            "Yuyang Chen",
            "Ze Chen",
            "Ziheng Zhang",
            "Fan Jia",
            "Kaixin Liu",
            "Meng Zhang",
            "Ruitao Hao",
            "Saike Huang",
            "Songhan Xie",
            "Yu Liu",
            "Zhao Wu",
            "Bin Xie",
            "Pengwei Zhang",
            "Qi Yang",
            "Xianchi Deng",
            "Yunfei Wei",
            "Enwen Zhang",
            "Hongyang Peng",
            "Jie Zhao",
            "Kai Liu",
            "Wei Sun",
            "Yajun Wei",
            "Yi Yang",
            "Yunqiao Zhang",
            "Ziwei Yan",
            "Haitao Yang",
            "Hao Liu",
            "Haoqiang Fan",
            "Haowei Zhang",
            "Junwen Huang",
            "Yang Chen",
            "Yunchao Ma",
            "Yunhuan Yang",
            "Zhengyuan Du",
            "Ziming Liu",
            "Jiahui Niu",
            "Yucheng Zhao",
            "Daxin Jiang",
            "Wenbin Tang",
            "Xiangyu Zhang",
            "Zheng Ge",
            "Erjin Zhou",
            "Tiancai Wang"
          ],
          "published": "2026-02-16T17:59:16Z",
          "updated": "2026-02-16T17:59:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14974v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14968v1",
          "title": "PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement",
          "summary": "Automatically generating interactive 3D environments is crucial for scaling up robotic data collection in simulation. While prior work has primarily focused on 3D asset placement, it often overlooks the physical relationships between objects (e.g., contact, support, balance, and containment), which are essential for creating complex and realistic manipulation scenarios such as tabletop arrangements, shelf organization, or box packing. Compared to classical 3D layout generation, producing complex physical scenes introduces additional challenges: (a) higher object density and complexity (e.g., a small shelf may hold dozens of books), (b) richer supporting relationships and compact spatial layouts, and (c) the need to accurately model both spatial placement and physical properties. To address these challenges, we propose PhyScensis, an LLM agent-based framework powered by a physics engine, to produce physically plausible scene configurations with high complexity. Specifically, our framework consists of three main components: an LLM agent iteratively proposes assets with spatial and physical predicates; a solver, equipped with a physics engine, realizes these predicates into a 3D scene; and feedback from the solver informs the agent to refine and enrich the configuration. Moreover, our framework preserves strong controllability over fine-grained textual descriptions and numerical parameters (e.g., relative positions, scene stability), enabled through probabilistic programming for stability and a complementary heuristic that jointly regulates stability and spatial relations. Experimental results show that our method outperforms prior approaches in scene complexity, visual quality, and physical accuracy, offering a unified pipeline for generating complex physical scene layouts for robotic manipulation.",
          "authors": [
            "Yian Wang",
            "Han Yang",
            "Minghao Guo",
            "Xiaowen Qiu",
            "Tsun-Hsuan Wang",
            "Wojciech Matusik",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
          ],
          "published": "2026-02-16T17:55:25Z",
          "updated": "2026-02-16T17:55:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14968v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14965v1",
          "title": "PAct: Part-Decomposed Single-View Articulated Object Generation",
          "summary": "Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.",
          "authors": [
            "Qingming Liu",
            "Xinyue Yao",
            "Shuyuan Zhang",
            "Yueci Deng",
            "Guiliang Liu",
            "Zhen Liu",
            "Kui Jia"
          ],
          "published": "2026-02-16T17:45:44Z",
          "updated": "2026-02-16T17:45:44Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14965v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14958v1",
          "title": "Morphing of and writing with a scissor linkage mechanism",
          "summary": "Kinematics of mechanisms is intricately coupled to their geometry and their utility often arises out of the ability to perform reproducible motion with fewer actuating degrees of freedom. In this article, we explore the assembly of scissor-units, each made of two rigid linear members connected by a pin joint. The assembly has a single degree of freedom, where actuating any single unit results in a shape change of the entire assembly. We derive expressions for the effective curvature of the unit and the trajectory of the mechanism's tip as a function of the geometric variables which we then use as the basis to program two tasks in the mechanism: shape morphing and writing. By phrasing these tasks as optimization problems and utilizing the differentiable simulation framework, we arrive at solutions that are then tested in table-top experiments. Our results show that the geometry of scissor assemblies can be leveraged for automated navigation and inspection in complex domains, in light of the optimization framework. However, we highlight that the challenges associated with rapid programming and error-free implementation in experiments without feedback still remain.",
          "authors": [
            "Mohanraj A",
            "S Ganga Prasath"
          ],
          "published": "2026-02-16T17:37:43Z",
          "updated": "2026-02-16T17:37:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14958v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15092v1",
          "title": "Augmenting Human Balance with Generic Supernumerary Robotic Limbs",
          "summary": "Supernumerary robotic limbs (SLs) have the potential to transform a wide range of human activities, yet their usability remains limited by key technical challenges, particularly in ensuring safety and achieving versatile control. Here, we address the critical problem of maintaining balance in the human-SLs system, a prerequisite for safe and comfortable augmentation tasks. Unlike previous approaches that developed SLs specifically for stability support, we propose a general framework for preserving balance with SLs designed for generic use. Our hierarchical three-layer architecture consists of: (i) a prediction layer that estimates human trunk and center of mass (CoM) dynamics, (ii) a planning layer that generates optimal CoM trajectories to counteract trunk movements and computes the corresponding SL control inputs, and (iii) a control layer that executes these inputs on the SL hardware. We evaluated the framework with ten participants performing forward and lateral bending tasks. The results show a clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance. This work paves the path towards safe and versatile human-SLs interactions. [This paper has been submitted for publication to IEEE.]",
          "authors": [
            "Xuanyun Qiu",
            "Dorian Verdel",
            "Hector Cervantes-Culebro",
            "Alexis Devillard",
            "Etienne Burdet"
          ],
          "published": "2026-02-16T17:29:34Z",
          "updated": "2026-02-16T17:29:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15092v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14948v1",
          "title": "Kalman Filtering Based Flight Management System Modeling for AAM Aircraft",
          "summary": "Advanced Aerial Mobility (AAM) operations require strategic flight planning services that predict both spatial and temporal uncertainties to safely validate flight plans against hazards such as weather cells, restricted airspaces, and CNS disruption areas. Current uncertainty estimation methods for AAM vehicles rely on conservative linear models due to limited real-world performance data. This paper presents a novel Kalman Filter-based uncertainty propagation method that models AAM Flight Management System (FMS) architectures through sigmoid-blended measurement noise covariance. Unlike existing approaches with fixed uncertainty thresholds, our method continuously adapts the filter's measurement trust based on progress toward waypoints, enabling FMS correction behavior to emerge naturally. The approach scales proportionally with control inputs and is tunable to match specific aircraft characteristics or route conditions. We validate the method using real ADS-B data from general aviation aircraft divided into training and verification sets. Uncertainty propagation parameters were tuned on the training set, achieving 76% accuracy in predicting arrival times when compared against the verification dataset, demonstrating the method's effectiveness for strategic flight plan validation in AAM operations.",
          "authors": [
            "Balram Kandoria",
            "Aryaman Singh Samyal"
          ],
          "published": "2026-02-16T17:29:05Z",
          "updated": "2026-02-16T17:29:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14948v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14874v1",
          "title": "Affordance Transfer Across Object Instances via Semantically Anchored Functional Map",
          "summary": "Traditional learning from demonstration (LfD) generally demands a cumbersome collection of physical demonstrations, which can be time-consuming and challenging to scale. Recent advances show that robots can instead learn from human videos by extracting interaction cues without direct robot involvement. However, a fundamental challenge remains: how to generalize demonstrated interactions across different object instances that share similar functionality but vary significantly in geometry. In this work, we propose \\emph{Semantic Anchored Functional Maps} (SemFM), a framework for transferring affordances across objects from a single visual demonstration. Starting from a coarse mesh reconstructed from an image, our method identifies semantically corresponding functional regions between objects, selects mutually exclusive semantic anchors, and propagates these constraints over the surface using a functional map to obtain a dense, semantically consistent correspondence. This enables demonstrated interaction regions to be transferred across geometrically diverse objects in a lightweight and interpretable manner. Experiments on synthetic object categories and real-world robotic manipulation tasks show that our approach enables accurate affordance transfer with modest computational cost, making it well-suited for practical robotic perception-to-action pipelines.",
          "authors": [
            "Xiaoxiang Dong",
            "Weiming Zhi"
          ],
          "published": "2026-02-16T16:04:47Z",
          "updated": "2026-02-16T16:04:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14874v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14799v1",
          "title": "Scalable Multi-Robot Path Planning via Quadratic Unconstrained Binary Optimization",
          "summary": "Multi-Agent Path Finding (MAPF) remains a fundamental challenge in robotics, where classical centralized approaches exhibit exponential growth in joint-state complexity as the number of agents increases. This paper investigates Quadratic Unconstrained Binary Optimization (QUBO) as a structurally scalable alternative for simultaneous multi-robot path planning. This approach is a robotics-oriented QUBO formulation incorporating BFS-based logical pre-processing (achieving over 95% variable reduction), adaptive penalty design for collision and constraint enforcement, and a time-windowed decomposition strategy that enables execution within current hardware limitations. An experimental evaluation in grid environments with up to four robots demonstrated near-optimal solutions in dense scenarios and favorable scaling behavior compared to sequential classical planning. These results establish a practical and reproducible baseline for future quantum and quantum-inspired multi-robot coordinations.",
          "authors": [
            "Javier González Villasmil"
          ],
          "published": "2026-02-16T14:50:04Z",
          "updated": "2026-02-16T14:50:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "quant-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14799v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14794v1",
          "title": "Analysis of a Cuspidal 6R Robot",
          "summary": "We present a theoretical and numerical analysis of the kinematics for the \"Transpressor\", a cuspidal 6R robot. It admits up to 16 inverse kinematics solutions which are described geometrically. For special target poses, we provide the solutions analytically and present a simple numerical solver for the general case. Moreover, an analytical estimate of the Jacobian determinant on a path between two solutions proves cuspidality for a class of robots similar to the transpressor.",
          "authors": [
            "Alexander Feeß",
            "Martin Weiß"
          ],
          "published": "2026-02-16T14:41:26Z",
          "updated": "2026-02-16T14:41:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14794v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14780v1",
          "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic",
          "summary": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.",
          "authors": [
            "Anna-Lena Schlamp",
            "Jeremias Gerner",
            "Klaus Bogenberger",
            "Werner Huber",
            "Stefanie Schmidtner"
          ],
          "published": "2026-02-16T14:30:01Z",
          "updated": "2026-02-16T14:30:01Z",
          "primary_category": "cs.MA",
          "categories": [
            "cs.MA",
            "cs.CY",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14780v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14726v1",
          "title": "ManeuverNet: A Soft Actor-Critic Framework for Precise Maneuvering of Double-Ackermann-Steering Robots with Optimized Reward Functions",
          "summary": "Autonomous control of double-Ackermann-steering robots is essential in agricultural applications, where robots must execute precise and complex maneuvers within a limited space. Classical methods, such as the Timed Elastic Band (TEB) planner, can address this problem, but they rely on parameter tuning, making them highly sensitive to changes in robot configuration or environment and impractical to deploy without constant recalibration. At the same time, end-to-end deep reinforcement learning (DRL) methods often fail due to unsuitable reward functions for non-holonomic constraints, resulting in sub-optimal policies and poor generalization. To address these challenges, this paper presents ManeuverNet, a DRL framework tailored for double-Ackermann systems, combining Soft Actor-Critic with CrossQ. Furthermore, ManeuverNet introduces four specifically designed reward functions to support maneuver learning. Unlike prior work, ManeuverNet does not depend on expert data or handcrafted guidance. We extensively evaluate ManeuverNet against both state-of-the-art DRL baselines and the TEB planner. Experimental results demonstrate that our framework substantially improves maneuverability and success rates, achieving more than a 40% gain over DRL baselines. Moreover, ManeuverNet effectively mitigates the strong parameter sensitivity observed in the TEB planner. In real-world trials, ManeuverNet achieved up to a 90% increase in maneuvering trajectory efficiency, highlighting its robustness and practical applicability.",
          "authors": [
            "Kohio Deflesselle",
            "Mélodie Daniel",
            "Aly Magassouba",
            "Miguel Aranda",
            "Olivier Ly"
          ],
          "published": "2026-02-16T13:19:04Z",
          "updated": "2026-02-16T13:19:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14726v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14666v1",
          "title": "Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments",
          "summary": "Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.",
          "authors": [
            "Ruofeng Wei",
            "Kai Chen",
            "Yui Lun Ng",
            "Yiyao Ma",
            "Justin Di-Lang Ho",
            "Hon Sing Tong",
            "Xiaomei Wang",
            "Jing Dai",
            "Ka-Wai Kwok",
            "Qi Dou"
          ],
          "published": "2026-02-16T11:46:14Z",
          "updated": "2026-02-16T11:46:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14666v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14662v1",
          "title": "Advances in Global Solvers for 3D Vision",
          "summary": "Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.",
          "authors": [
            "Zhenjun Zhao",
            "Heng Yang",
            "Bangyan Liao",
            "Yingping Zeng",
            "Shaocheng Yan",
            "Yingdong Gu",
            "Peidong Liu",
            "Yi Zhou",
            "Haoang Li",
            "Javier Civera"
          ],
          "published": "2026-02-16T11:40:32Z",
          "updated": "2026-02-16T11:40:32Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14662v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14561v1",
          "title": "Simulation-based Learning of Electrical Cabinet Assembly Using Robot Skills",
          "summary": "This paper presents a simulation-driven approach for automating the force-controlled assembly of electrical terminals on DIN-rails, a task traditionally hindered by high programming effort and product variability. The proposed method integrates deep reinforcement learning (DRL) with parameterizable robot skills in a physics-based simulation environment. To realistically model the snap-fit assembly process, we develop and evaluate two types of joining models: analytical models based on beam theory and rigid-body models implemented in the MuJoCo physics engine. These models enable accurate simulation of interaction forces, essential for training DRL agents. The robot skills are structured using the pitasc framework, allowing modular, reusable control strategies. Training is conducted in simulation using Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithms. Domain randomization is applied to improve robustness. The trained policies are transferred to a physical UR10e robot system without additional tuning. Experimental results demonstrate high success rates (up to 100%) in both simulation and real-world settings, even under significant positional and rotational deviations. The system generalizes well to new terminal types and positions, significantly reducing manual programming effort. This work highlights the potential of combining simulation-based learning with modular robot skills for flexible, scalable automation in small-batch manufacturing. Future work will explore hybrid learning methods, automated environment parameterization, and further refinement of joining models for design integration.",
          "authors": [
            "Arik Laemmle",
            "Balázs András Bálint",
            "Philipp Tenbrock",
            "Frank Naegele",
            "David Traunecker",
            "József Váncza",
            "Marco F. Huber"
          ],
          "published": "2026-02-16T08:45:54Z",
          "updated": "2026-02-16T08:45:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14561v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14551v1",
          "title": "Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction",
          "summary": "Human-Robot Collaboration (HRC) plays an important role in assembly tasks by enabling robots to plan and adjust their motions based on interactive, real-time human instructions. However, such instructions are often linguistically ambiguous and underspecified, making it difficult to generate physically feasible and cooperative robot behaviors. To address this challenge, many studies have applied Vision-Language Models (VLMs) to interpret high-level instructions and generate corresponding actions. Nevertheless, VLM-based approaches still suffer from hallucinated reasoning and an inability to anticipate physical execution failures. To address these challenges, we propose an HRC framework that augments a VLM-based reasoning with a dual-correction mechanism: an internal correction model that verifies logical consistency and task feasibility prior to action execution, and an external correction model that detects and rectifies physical failures through post-execution feedback. Simulation ablation studies demonstrate that the proposed method improves the success rate compared to baselines without correction models. Our real-world experiments in collaborative assembly tasks supported by object fixation or tool preparation by an upper body humanoid robot further confirm the framewor's effectiveness in enabling interactive replanning across different collaborative tasks in response to human instructions, validating its practical feasibility.",
          "authors": [
            "Taichi Kato",
            "Takuya Kiyokawa",
            "Namiko Saito",
            "Kensuke Harada"
          ],
          "published": "2026-02-16T08:24:19Z",
          "updated": "2026-02-16T08:24:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14551v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14540v1",
          "title": "Multimodal Covariance Steering in Belief Space with Active Probing and Influence for Autonomous Driving",
          "summary": "Autonomous driving in complex traffic requires reasoning under uncertainty. Common approaches rely on prediction-based planning or risk-aware control, but these are typically treated in isolation, limiting their ability to capture the coupled nature of action and inference in interactive settings. This gap becomes especially critical in uncertain scenarios, where simply reacting to predictions can lead to unsafe maneuvers or overly conservative behavior. Our central insight is that safe interaction requires not only estimating human behavior but also shaping it when ambiguity poses risks. To this end, we introduce a hierarchical belief model that structures human behavior across coarse discrete intents and fine motion modes, updated via Bayesian inference for interpretable multi-resolution reasoning. On top of this, we develop an active probing strategy that identifies when multimodal ambiguity in human predictions may compromise safety and plans disambiguating actions that both reveal intent and gently steer human decisions toward safer outcomes. Finally, a runtime risk-evaluation layer based on Conditional Value-at-Risk (CVaR) ensures that all probing actions remain within human risk tolerance during influence. Our simulations in lane-merging and unsignaled intersection scenarios demonstrate that our approach achieves higher success rates and shorter completion times compared to existing methods. These results highlight the benefit of coupling belief inference, probing, and risk monitoring, yielding a principled and interpretable framework for planning under uncertainty.",
          "authors": [
            "Devodita Chakravarty",
            "John Dolan",
            "Yiwei Lyu"
          ],
          "published": "2026-02-16T08:04:16Z",
          "updated": "2026-02-16T08:04:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14540v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14526v1",
          "title": "TWISTED-RL: Hierarchical Skilled Agents for Knot-Tying without Human Demonstrations",
          "summary": "Robotic knot-tying represents a fundamental challenge in robotics due to the complex interactions between deformable objects and strict topological constraints. We present TWISTED-RL, a framework that improves upon the previous state-of-the-art in demonstration-free knot-tying (TWISTED), which smartly decomposed a single knot-tying problem into manageable subproblems, each addressed by a specialized agent. Our approach replaces TWISTED's single-step inverse model that was learned via supervised learning with a multi-step Reinforcement Learning policy conditioned on abstract topological actions rather than goal states. This change allows more delicate topological state transitions while avoiding costly and ineffective data collection protocols, thus enabling better generalization across diverse knot configurations. Experimental results demonstrate that TWISTED-RL manages to solve previously unattainable knots of higher complexity, including commonly used knots such as the Figure-8 and the Overhand. Furthermore, the increase in success rates and drop in planning time establishes TWISTED-RL as the new state-of-the-art in robotic knot-tying without human demonstrations.",
          "authors": [
            "Guy Freund",
            "Tom Jurgenson",
            "Matan Sudry",
            "Erez Karpas"
          ],
          "published": "2026-02-16T07:21:02Z",
          "updated": "2026-02-16T07:21:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14526v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14473v1",
          "title": "Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots' Performance in U-Shaped Stair Climbing",
          "summary": "Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot's performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab's pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.",
          "authors": [
            "Baixiao Huang",
            "Baiyu Huang",
            "Yu Hou"
          ],
          "published": "2026-02-16T05:19:06Z",
          "updated": "2026-02-16T05:19:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14473v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14438v1",
          "title": "RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems",
          "summary": "This study proposes an intelligent multi-agent framework built on LLMs and VLMs and specifically tailored to robotics. The goal is to integrate the strengths of LLMs and VLMs with computational tools to automatically analyze and solve problems related to robotic manipulators. Our developed framework accepts both textual and visual inputs and can automatically perform forward and inverse kinematics, compute velocities and accelerations of key points, generate 3D simulations of the robot, and ultimately execute motion control within the simulated environment, all according to the user's query. To evaluate the framework, three benchmark tests were designed, each consisting of ten questions. In the first benchmark test, the framework was evaluated while connected to GPT-4o, DeepSeek-V3.2, and Claude-Sonnet-4.5, as well as their corresponding raw models. The objective was to extract the forward kinematics of robots directly from textual descriptions. The results showed that the framework integrated with GPT-4o achieved the highest accuracy, reaching 0.97 in computing the final solution, whereas the raw model alone attained an accuracy of only 0.30 for the same task. Similarly, for the other two models, the framework consistently outperformed the corresponding raw models in terms of accuracy. The second benchmark test was identical to the first, except that the input was provided in visual form. In this test, the GPT-4o LLM was used alongside the Gemini 2.5 Pro VLM. The results showed that the framework achieved an accuracy of 0.93 in obtaining the final answer, which is approximately 20% higher than that of the corresponding raw model. The third benchmark test encompassed a range of robotic tasks, including simulation, control, velocity and acceleration computation, as well as inverse kinematics and Jacobian calculation, for which the framework achieved an accuracy of 0.97.",
          "authors": [
            "Hamid Khabazi",
            "Ali F. Meghdari",
            "Alireza Taheri"
          ],
          "published": "2026-02-16T03:49:17Z",
          "updated": "2026-02-16T03:49:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14438v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14434v1",
          "title": "A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation",
          "summary": "Contact-rich manipulation tasks in unstructured environments pose significant robustness challenges for robot learning, where unexpected collisions can cause damage and hinder policy acquisition. Existing soft end-effectors face fundamental limitations: they either provide a limited deformation range, lack directional stiffness control, or require complex actuation systems that compromise practicality. This study introduces CLAW (Compliant Leaf-spring Anisotropic soft Wrist), a novel soft wrist mechanism that addresses these limitations through a simple yet effective design using two orthogonal leaf springs and rotary joints with a locking mechanism. CLAW provides large 6-degree-of-freedom deformation (40mm lateral, 20mm vertical), anisotropic stiffness that is tunable across three distinct modes, while maintaining lightweight construction (330g) at low cost ($550). Experimental evaluations using imitation learning demonstrate that CLAW achieves 76% success rate in benchmark peg-insertion tasks, outperforming both the Fin Ray gripper (43%) and rigid gripper alternatives (36%). CLAW successfully handles diverse contact-rich scenarios, including precision assembly with tight tolerances and delicate object manipulation, demonstrating its potential to enable robust robot learning in contact-rich domains. Project page: https://project-page-manager.github.io/CLAW/",
          "authors": [
            "Steven Oh",
            "Tomoya Takahashi",
            "Cristian C. Beltran-Hernandez",
            "Yuki Kuroda",
            "Masashi Hamaya"
          ],
          "published": "2026-02-16T03:45:04Z",
          "updated": "2026-02-16T03:45:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14434v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14413v1",
          "title": "Understanding Sensor Vulnerabilities in Industrial XR Tracking",
          "summary": "Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.",
          "authors": [
            "Sourya Saha",
            "Md. Nurul Absur"
          ],
          "published": "2026-02-16T02:42:15Z",
          "updated": "2026-02-16T02:42:15Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14413v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14363v1",
          "title": "AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation",
          "summary": "This paper presents Adaptive Whole-body Loco-Manipulation, AdaptManip, a fully autonomous framework for humanoid robots to perform integrated navigation, object lifting, and delivery. Unlike prior imitation learning-based approaches that rely on human demonstrations and are often brittle to disturbances, AdaptManip aims to train a robust loco-manipulation policy via reinforcement learning without human demonstrations or teleoperation data. The proposed framework consists of three coupled components: (1) a recurrent object state estimator that tracks the manipulated object in real time under limited field-of-view and occlusions; (2) a whole-body base policy for robust locomotion with residual manipulation control for stable object lifting and delivery; and (3) a LiDAR-based robot global position estimator that provides drift-robust localization. All components are trained in simulation using reinforcement learning and deployed on real hardware in a zero-shot manner. Experimental results show that AdaptManip significantly outperforms baseline methods, including imitation learning-based approaches, in adaptability and overall success rate, while accurate object state estimation improves manipulation performance even under occlusion. We further demonstrate fully autonomous real-world navigation, object lifting, and delivery on a humanoid robot.",
          "authors": [
            "Morgan Byrd",
            "Donghoon Baek",
            "Kartik Garg",
            "Hyunyoung Jung",
            "Daesol Cho",
            "Maks Sorokin",
            "Robert Wright",
            "Sehoon Ha"
          ],
          "published": "2026-02-16T00:29:53Z",
          "updated": "2026-02-16T00:29:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14363v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15904v1",
          "title": "A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving",
          "summary": "LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.",
          "authors": [
            "June Moh Goo",
            "Zichao Zeng",
            "Jan Boehm"
          ],
          "published": "2026-02-15T22:34:28Z",
          "updated": "2026-02-15T22:34:28Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15904v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14311v1",
          "title": "Exploiting Structure-from-Motion for Robust Vision-Based Map Matching for Aircraft Surface Movement",
          "summary": "In this paper we introduce a vision-aided navigation (VAN) pipeline designed to support ground navigation of autonomous aircraft. The proposed algorithm combines the computational efficiency of indirect methods with the robustness of direct image-based techniques to enhance solution integrity. The pipeline starts by processing ground images (e.g., acquired by a taxiing aircraft) and relates them via a feature-based structure-from-motion (SfM) solution. A ground plane mosaic is then constructed via homography transforms and matched to satellite imagery using a sum of squares differences (SSD) of intensities. Experimental results reveal that drift within the SfM solution, similar to that observed in dead-reckoning systems, challenges the expected accuracy benefits of map-matching with a wide-baseline ground-plane mosaic. However, the proposed algorithm demonstrates key integrity features, such as the ability to identify registration anomalies and ambiguous matches. These characteristics of the pipeline can mitigate outlier behaviors and contribute toward a robust, certifiable solution for autonomous surface movement of aircraft.",
          "authors": [
            "Daniel Choate",
            "Jason Rife"
          ],
          "published": "2026-02-15T21:08:48Z",
          "updated": "2026-02-15T21:08:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14311v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14287v1",
          "title": "Autonomous Robotic Tissue Palpation and Abnormalities Characterisation via Ergodic Exploration",
          "summary": "We propose a novel autonomous robotic palpation framework for real-time elastic mapping during tissue exploration using a viscoelastic tissue model. The method combines force-based parameter estimation using a commercial force/torque sensor with an ergodic control strategy driven by a tailored Expected Information Density, which explicitly biases exploration toward diagnostically relevant regions by jointly considering model uncertainty, stiffness magnitude, and spatial gradients. An Extended Kalman Filter is employed to estimate viscoelastic model parameters online, while Gaussian Process Regression provides spatial modelling of the estimated elasticity, and a Heat Equation Driven Area Coverage controller enables adaptive, continuous trajectory planning. Simulations on synthetic stiffness maps demonstrate that the proposed approach achieves better reconstruction accuracy, enhanced segmentation capability, and improved robustness in detecting stiff inclusions compared to Bayesian Optimisation-based techniques. Experimental validation on a silicone phantom with embedded inclusions emulating pathological tissue regions further corroborates the potential of the method for autonomous tissue characterisation in diagnostic and screening applications.",
          "authors": [
            "Luca Beber",
            "Edoardo Lamon",
            "Matteo Saveriano",
            "Daniele Fontanelli",
            "Luigi Palopoli"
          ],
          "published": "2026-02-15T19:37:21Z",
          "updated": "2026-02-15T19:37:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14287v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14255v1",
          "title": "A Latency-Aware Framework for Visuomotor Policy Learning on Industrial Robots",
          "summary": "Industrial robots are increasingly deployed in contact-rich construction and manufacturing tasks that involve uncertainty and long-horizon execution. While learning-based visuomotor policies offer a promising alternative to open-loop control, their deployment on industrial platforms is challenged by a large observation-execution gap caused by sensing, inference, and control latency. This gap is significantly greater than on low-latency research robots due to high-level interfaces and slower closed-loop dynamics, making execution timing a critical system-level issue. This paper presents a latency-aware framework for deploying and evaluating visuomotor policies on industrial robotic arms under realistic timing constraints. The framework integrates calibrated multimodal sensing, temporally consistent synchronization, a unified communication pipeline, and a teleoperation interface for demonstration collection. Within this framework, we introduce a latency-aware execution strategy that schedules finite-horizon, policy-predicted action sequences based on temporal feasibility, enabling asynchronous inference and execution without modifying policy architectures or training. We evaluate the framework on a contact-rich industrial assembly task while systematically varying inference latency. Using identical policies and sensing pipelines, we compare latency-aware execution with blocking and naive asynchronous baselines. Results show that latency-aware execution maintains smooth motion, compliant contact behavior, and consistent task progression across a wide range of latencies while reducing idle time and avoiding instability observed in baseline methods. These findings highlight the importance of explicitly handling latency for reliable closed-loop deployment of visuomotor policies on industrial robots.",
          "authors": [
            "Daniel Ruan",
            "Salma Mozaffari",
            "Sigrid Adriaenssens",
            "Arash Adel"
          ],
          "published": "2026-02-15T17:53:57Z",
          "updated": "2026-02-15T17:53:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14255v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14252v1",
          "title": "GRAIL: Goal Recognition Alignment through Imitation Learning",
          "summary": "Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.",
          "authors": [
            "Osher Elhadad",
            "Felipe Meneguzzi",
            "Reuth Mirsky"
          ],
          "published": "2026-02-15T17:45:03Z",
          "updated": "2026-02-15T17:45:03Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14252v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14247v1",
          "title": "Path Planning Optimisation for SParse, AwaRe and Cooperative Networked Aerial Robot Teams (SpArC-NARTs): Optimisation Tool and Ground Sensing Coverage Use Cases",
          "summary": "A networked aerial robot team (NART) comprises a group of agents (e.g., unmanned aerial vehicles (UAVs), ground control stations, etc.) interconnected by wireless links. Inter-agent connectivity, even if intermittent (i.e. sparse), enables data exchanges between agents and supports cooperative behaviours in several NART missions. It can benefit online decentralised decision-making and group resilience, particularly when prior knowledge is inaccurate or incomplete. These requirements can be accounted for in the offline mission planning stages to incentivise cooperative behaviours and improve mission efficiency during the NART deployment. This paper proposes a novel path planning tool for a Sparse, Aware, and Cooperative Networked Aerial Robot Team (SpArC-NART) in exploration missions. It simultaneously considers different levels of prior information regarding the environment, limited agent energy, sensing, and communication, as well as distinct NART constitutions. The communication model takes into account the limitations of user-defined radio technology and physical phenomena. The proposed tool aims to maximise the mission goals (e.g., finding one or multiple targets, covering the full area of the environment, etc.), while cooperating with other agents to reduce agent reporting times, increase their global situational awareness (e.g., their knowledge of the environment), and facilitate mission replanning, if required. The developed cooperation mechanism leverages soft-motion constraints and dynamic rewards based on the Value of Movement and the expected communication availability between the agents at each time step. A ground sensing coverage use case was chosen to illustrate the current capabilities of this tool.",
          "authors": [
            "Maria Conceição",
            "António Grilo",
            "Meysam Basiri"
          ],
          "published": "2026-02-15T17:40:45Z",
          "updated": "2026-02-15T17:40:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14247v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14222v1",
          "title": "Muscle Coactivation in the Sky: Geometry and Pareto Optimality of Energy vs. Promptness in Multirotors",
          "summary": "In robotics and human biomechanics, the tension between energy economy and kinematic readiness is well recognized; this work brings that fundamental principle to aerial multirotors. We show that the limited torque of the motors and the nonlinear aerodynamic map from rotor speed to thrust naturally give rise to the novel concept of promptness-a metric akin to dynamic aerodynamic manipulability. By treating energy consumption as a competing objective and introducing a geometric fiber-bundle formulation, we turn redundancy resolution into a principled multi-objective program on affine fibers. The use of the diffeomorphic transformation linearizing the signed-quadratic propulsion model allows us to lay the foundations for a rigorous study of the interplay between these costs. Through an illustrative case study on 4-DoF allocation on the hexarotor, we reveal that this interplay is fiber-dependent and physically shaped by hardware inequalities. For unidirectional thrusters, the feasible fibers are compact, yielding interior allocations and a short Pareto arc, while torque demands break symmetry and separate the optima. Conversely, with reversible propellers, the null space enables antagonistic rotor co-contraction that drives promptness to hardware limits, making optimal endurance and agility fundamentally incompatible in those regimes. Ultimately, rather than relying on heuristic tuning or black box algorithms to empirically improve task execution, this framework provides a foundational understanding of why and how to achieve agility through geometry-aware control allocation, offering possible guidance for vehicle design, certification metrics, and threat-aware flight operation.",
          "authors": [
            "Antonio Franchi"
          ],
          "published": "2026-02-15T16:35:46Z",
          "updated": "2026-02-15T16:35:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14222v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14193v1",
          "title": "Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation",
          "summary": "Articulated object manipulation is essential for various real-world robotic tasks, yet generalizing across diverse objects remains a major challenge. A key to generalization lies in understanding functional parts (e.g., door handles and knobs), which indicate where and how to manipulate across diverse object categories and shapes. Previous works attempted to achieve generalization by introducing foundation features, while these features are mostly 2D-based and do not specifically consider functional parts. When lifting these 2D features to geometry-profound 3D space, challenges arise, such as long runtimes, multi-view inconsistencies, and low spatial resolution with insufficient geometric information. To address these issues, we propose Part-Aware 3D Feature Field (PA3FF), a novel dense 3D feature with part awareness for generalizable articulated object manipulation. PA3FF is trained by 3D part proposals from a large-scale labeled dataset, via a contrastive learning formulation. Given point clouds as input, PA3FF predicts a continuous 3D feature field in a feedforward manner, where the distance between point features reflects the proximity of functional parts: points with similar features are more likely to belong to the same part. Building on this feature, we introduce the Part-Aware Diffusion Policy (PADP), an imitation learning framework aimed at enhancing sample efficiency and generalization for robotic manipulation. We evaluate PADP on several simulated and real-world tasks, demonstrating that PA3FF consistently outperforms a range of 2D and 3D representations in manipulation scenarios, including CLIP, DINOv2, and Grounded-SAM. Beyond imitation learning, PA3FF enables diverse downstream methods, including correspondence learning and segmentation tasks, making it a versatile foundation for robotic manipulation. Project page: https://pa3ff.github.io",
          "authors": [
            "Yue Chen",
            "Muqing Jiang",
            "Kaifeng Zheng",
            "Jiaqi Liang",
            "Chenrui Tie",
            "Haoran Lu",
            "Ruihai Wu",
            "Hao Dong"
          ],
          "published": "2026-02-15T15:39:05Z",
          "updated": "2026-02-15T15:39:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14193v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14174v1",
          "title": "Direction Matters: Learning Force Direction Enables Sim-to-Real Contact-Rich Manipulation",
          "summary": "Sim-to-real transfer for contact-rich manipulation remains challenging due to the inherent discrepancy in contact dynamics. While existing methods often rely on costly real-world data or utilize blind compliance through fixed controllers, we propose a framework that leverages expert-designed controller logic for transfer. Inspired by the success of privileged supervision in kinematic tasks, we employ a human-designed finite state machine based position/force controller in simulation to provide privileged guidance. The resulting policy is trained to predict the end-effector pose, contact state, and crucially the desired contact force direction. Unlike force magnitudes, which are highly sensitive to simulation inaccuracies, force directions encode high-level task geometry and remain robust across the sim-to-real gap. At deployment, these predictions configure a force-aware admittance controller. By combining the policy's directional intent with a constant, low-cost manually tuned force magnitude, the system generates adaptive, task-aligned compliance. This tuning is lightweight, typically requiring only a single scalar per contact state. We provide theoretical analysis for stability and robustness to disturbances. Experiments on four real-world tasks, i.e., microwave opening, peg-in-hole, whiteboard wiping, and door opening, demonstrate that our approach significantly outperforms strong baselines in both success rate and robustness. Videos are available at: https://yifei-y.github.io/project-pages/DirectionMatters/.",
          "authors": [
            "Yifei Yang",
            "Anzhe Chen",
            "Zhenjie Zhu",
            "Kechun Xu",
            "Yunxuan Mao",
            "Yufei Wei",
            "Lu Chen",
            "Rong Xiong",
            "Yue Wang"
          ],
          "published": "2026-02-15T14:57:13Z",
          "updated": "2026-02-15T14:57:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14174v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14104v1",
          "title": "Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects",
          "summary": "Precise in-hand manipulation of force-sensitive objects typically requires judicious coordinated force planning as well as accurate contact force feedback and control. Unlike multi-arm platforms with gripper end effectors, multi-fingered hands rely solely on fingertip point contacts and are not able to apply pull forces, therefore poses a more challenging problem. Furthermore, calibrated torque sensors are lacking in most commercial dexterous hands, adding to the difficulty. To address these challenges, we propose a dual-layer framework for multi-finger coordination, enabling high-precision manipulation of force-sensitive objects through joint control without tactile feedback. This approach solves coordinated contact force planning by incorporating graph rigidity and force closure constraints. By employing a force-to-position mapping, the planned force trajectory is converted to a joint trajectory. We validate the framework on a custom dexterous hand, demonstrating the capability to manipulate fragile objects-including a soft yarn, a plastic cup, and a raw egg-with high precision and safety.",
          "authors": [
            "Xinan Rong",
            "Changhuang Wan",
            "Aochen He",
            "Xiaolong Li",
            "Gangshan Jing"
          ],
          "published": "2026-02-15T11:40:39Z",
          "updated": "2026-02-15T11:40:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14104v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14099v1",
          "title": "SemanticFeels: Semantic Labeling during In-Hand Manipulation",
          "summary": "As robots become increasingly integrated into everyday tasks, their ability to perceive both the shape and properties of objects during in-hand manipulation becomes critical for adaptive and intelligent behavior. We present SemanticFeels, an extension of the NeuralFeels framework that integrates semantic labeling with neural implicit shape representation, from vision and touch. To illustrate its application, we focus on material classification: high-resolution Digit tactile readings are processed by a fine-tuned EfficientNet-B0 convolutional neural network (CNN) to generate local material predictions, which are then embedded into an augmented signed distance field (SDF) network that jointly predicts geometry and continuous material regions. Experimental results show that the system achieves a high correspondence between predicted and actual materials on both single- and multi-material objects, with an average matching accuracy of 79.87% across multiple manipulation trials on a multi-material object.",
          "authors": [
            "Anas Al Shikh Khalil",
            "Haozhi Qi",
            "Roberto Calandra"
          ],
          "published": "2026-02-15T11:22:05Z",
          "updated": "2026-02-15T11:22:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14099v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14092v1",
          "title": "Simultaneous State Estimation and Online Model Learning in a Soft Robotic System",
          "summary": "Operating complex real-world systems, such as soft robots, can benefit from precise predictive control schemes that require accurate state and model knowledge. This knowledge is typically not available in practical settings and must be inferred from noisy measurements. In particular, it is challenging to simultaneously estimate unknown states and learn a model online from sequentially arriving measurements. In this paper, we show how a recently proposed gray-box system identification tool enables the estimation of a soft robot's current pose while at the same time learning a bending stiffness model. For estimation and learning, we rely solely on a nominal constant-curvature robot model and measurements of the robot's base reactions (e.g., base forces). The estimation scheme -- relying on a marginalized particle filter -- allows us to conveniently interface nominal constant-curvature equations with a Gaussian Process (GP) bending stiffness model to be learned. This, in contrast to estimation via a random walk over stiffness values, enables prediction of bending stiffness and improves overall model quality. We demonstrate, using real-world soft-robot data, that the method learns a bending stiffness model online while accurately estimating the robot's pose. Notably, reduced multi-step forward-prediction errors indicate that the learned bending-stiffness GP improves overall model quality.",
          "authors": [
            "Jan-Hendrik Ewering",
            "Max Bartholdt",
            "Simon F. G. Ehlers",
            "Niklas Wahlström",
            "Thomas B. Schön",
            "Thomas Seel"
          ],
          "published": "2026-02-15T10:44:58Z",
          "updated": "2026-02-15T10:44:58Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14092v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14048v1",
          "title": "ProAct: A Dual-System Framework for Proactive Embodied Social Agents",
          "summary": "Embodied social agents have recently advanced in generating synchronized speech and gestures. However, most interactive systems remain fundamentally reactive, responding only to current sensory inputs within a short temporal window. Proactive social behavior, in contrast, requires deliberation over accumulated context and intent inference, which conflicts with the strict latency budget of real-time interaction. We present \\emph{ProAct}, a dual-system framework that reconciles this time-scale conflict by decoupling a low-latency \\emph{Behavioral System} for streaming multimodal interaction from a slower \\emph{Cognitive System} which performs long-horizon social reasoning and produces high-level proactive intentions. To translate deliberative intentions into continuous non-verbal behaviors without disrupting fluency, we introduce a streaming flow-matching model conditioned on intentions via ControlNet. This mechanism supports asynchronous intention injection, enabling seamless transitions between reactive and proactive gestures within a single motion stream. We deploy ProAct on a physical humanoid robot and evaluate both motion quality and interactive effectiveness. In real-world interaction user studies, participants and observers consistently prefer ProAct over reactive variants in perceived proactivity, social presence, and overall engagement, demonstrating the benefits of dual-system proactive control for embodied social interaction.",
          "authors": [
            "Zeyi Zhang",
            "Zixi Kang",
            "Ruijie Zhao",
            "Yusen Feng",
            "Biao Jiang",
            "Libin Liu"
          ],
          "published": "2026-02-15T08:27:34Z",
          "updated": "2026-02-15T08:27:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.GR"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14048v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14032v1",
          "title": "RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation",
          "summary": "Enhancing the generalization capability of robotic learning to enable robots to operate effectively in diverse, unseen scenes is a fundamental and challenging problem. Existing approaches often depend on pretraining with large-scale data collection, which is labor-intensive and time-consuming, or on semantic data augmentation techniques that necessitate an impractical assumption of flawless upstream object detection in real-world scenarios. In this work, we propose RoboAug, a novel generative data augmentation framework that significantly minimizes the reliance on large-scale pretraining and the perfect visual recognition assumption by requiring only the bounding box annotation of a single image during training. Leveraging this minimal information, RoboAug employs pre-trained generative models for precise semantic data augmentation and integrates a plug-and-play region-contrastive loss to help models focus on task-relevant regions, thereby improving generalization and boosting task success rates. We conduct extensive real-world experiments on three robots, namely UR-5e, AgileX, and Tien Kung 2.0, spanning over 35k rollouts. Empirical results demonstrate that RoboAug significantly outperforms state-of-the-art data augmentation baselines. Specifically, when evaluating generalization capabilities in unseen scenes featuring diverse combinations of backgrounds, distractors, and lighting conditions, our method achieves substantial gains over the baseline without augmentation. The success rates increase from 0.09 to 0.47 on UR-5e, from 0.16 to 0.60 on AgileX, and from 0.19 to 0.67 on Tien Kung 2.0. These results highlight the superior generalization and effectiveness of RoboAug in real-world manipulation tasks. Our project is available at https://x-roboaug.github.io/.",
          "authors": [
            "Xinhua Wang",
            "Kun Wu",
            "Zhen Zhao",
            "Hu Cao",
            "Yinuo Zhao",
            "Zhiyuan Xu",
            "Meng Li",
            "Shichao Fan",
            "Di Wu",
            "Yixue Zhang",
            "Ning Liu",
            "Zhengping Che",
            "Jian Tang"
          ],
          "published": "2026-02-15T07:40:00Z",
          "updated": "2026-02-15T07:40:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14032v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13999v1",
          "title": "It Takes Two to Tango: A Holistic Simulator for Joint Order Scheduling and Multi-Agent Path Finding in Robotic Warehouses",
          "summary": "The prevailing paradigm in Robotic Mobile Fulfillment Systems (RMFS) typically treats order scheduling and multi-agent pathfinding as isolated sub-problems. We argue that this decoupling is a fundamental bottleneck, masking the critical dependencies between high-level dispatching and low-level congestion. Existing simulators fail to bridge this gap, often abstracting away heterogeneous kinematics and stochastic execution failures. We propose WareRover, a holistic simulation platform that enforces a tight coupling between OS and MAPF via a unified, closed-loop optimization interface. Unlike standard benchmarks, WareRover integrates dynamic order streams, physics-aware motion constraints, and non-nominal recovery mechanisms into a single evaluation loop. Experiments reveal that SOTA algorithms often falter under these realistic coupled constraints, demonstrating that WareRover provides a necessary and challenging testbed for robust, next-generation warehouse coordination. The project and video is available at https://hhh-x.github.io/WareRover/.",
          "authors": [
            "Haozheng Xu",
            "Wenhao Li",
            "Zifan Wei",
            "Bo Jin",
            "Hongxing Bai",
            "Ben Yang",
            "Xiangfeng Wang"
          ],
          "published": "2026-02-15T05:51:58Z",
          "updated": "2026-02-15T05:51:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13999v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13977v1",
          "title": "WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL",
          "summary": "Reinforcement learning (RL) promises to unlock capabilities beyond imitation learning for Vision-Language-Action (VLA) models, but its requirement for massive real-world interaction prevents direct deployment on physical robots. Recent work attempts to use learned world models as simulators for policy optimization, yet closed-loop imagined rollouts inevitably suffer from hallucination and long-horizon error accumulation. Such errors do not merely degrade visual fidelity; they corrupt the optimization signal, encouraging policies to exploit model inaccuracies rather than genuine task progress. We propose WoVR, a reliable world-model-based reinforcement learning framework for post-training VLA policies. Instead of assuming a faithful world model, WoVR explicitly regulates how RL interacts with imperfect imagined dynamics. It improves rollout stability through a controllable action-conditioned video world model, reshapes imagined interaction to reduce effective error depth via Keyframe-Initialized Rollouts, and maintains policy-simulator alignment through World Model-Policy co-evolution. Extensive experiments on LIBERO benchmarks and real-world robotic manipulation demonstrate that WoVR enables stable long-horizon imagined rollouts and effective policy optimization, improving average LIBERO success from 39.95% to 69.2% (+29.3 points) and real-robot success from 61.7% to 91.7% (+30.0 points). These results show that learned world models can serve as practical simulators for reinforcement learning when hallucination is explicitly controlled.",
          "authors": [
            "Zhennan Jiang",
            "Shangqing Zhou",
            "Yutong Jiang",
            "Zefang Huang",
            "Mingjie Wei",
            "Yuhui Chen",
            "Tianxing Zhou",
            "Zhen Guo",
            "Hao Lin",
            "Quanlu Zhang",
            "Yu Wang",
            "Haoran Li",
            "Chao Yu",
            "Dongbin Zhao"
          ],
          "published": "2026-02-15T03:48:20Z",
          "updated": "2026-02-15T03:48:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13977v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13932v1",
          "title": "Joint Task Assistance Planning via Nested Branch and Bound (Extended Version)",
          "summary": "We introduce and study the Joint Task Assistance Planning problem which generalizes prior work on optimizing assistance in robotic collaboration. In this setting, two robots operate over predefined roadmaps, each represented as a graph corresponding to its configuration space. One robot, the task robot, must execute a timed mission, while the other, the assistance robot, provides sensor-based support that depends on their spatial relationship. The objective is to compute a path for both robots that maximizes the total duration of assistance given. Solving this problem is challenging due to the combinatorial explosion of possible path combinations together with the temporal nature of the problem (time needs to be accounted for as well). To address this, we propose a nested branch-and-bound framework that efficiently explores the space of robot paths in a hierarchical manner. We empirically evaluate our algorithm and demonstrate a speedup of up to two orders of magnitude when compared to a baseline approach.",
          "authors": [
            "Omer Daube",
            "Oren Salzman"
          ],
          "published": "2026-02-15T00:04:51Z",
          "updated": "2026-02-15T00:04:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13932v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13932v2",
          "title": "Joint Task Assistance Planning via Nested Branch and Bound (Extended Version)",
          "summary": "We introduce and study the Joint Task Assistance Planning problem which generalizes prior work on optimizing assistance in robotic collaboration. In this setting, two robots operate over predefined roadmaps, each represented as a graph corresponding to its configuration space. One robot, the task robot, must execute a timed mission, while the other, the assistance robot, provides sensor-based support that depends on their spatial relationship. The objective is to compute a path for both robots that maximizes the total duration of assistance given. Solving this problem is challenging due to the combinatorial explosion of possible path combinations together with the temporal nature of the problem (time needs to be accounted for as well). To address this, we propose a nested branch-and-bound framework that efficiently explores the space of robot paths in a hierarchical manner. We empirically evaluate our algorithm and demonstrate a speedup of up to two orders of magnitude when compared to a baseline approach.",
          "authors": [
            "Omer Daube",
            "Oren Salzman"
          ],
          "published": "2026-02-15T00:04:51Z",
          "updated": "2026-02-24T17:45:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13932v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13909v1",
          "title": "High-fidelity 3D reconstruction for planetary exploration",
          "summary": "Planetary exploration increasingly relies on autonomous robotic systems capable of perceiving, interpreting, and reconstructing their surroundings in the absence of global positioning or real-time communication with Earth. Rovers operating on planetary surfaces must navigate under sever environmental constraints, limited visual redundancy, and communication delays, making onboard spatial awareness and visual localization key components for mission success. Traditional techniques based on Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) provide geometric consistency but struggle to capture radiometric detail or to scale efficiently in unstructured, low-texture terrains typical of extraterrestrial environments. This work explores the integration of radiance field-based methods - specifically Neural Radiance Fields (NeRF) and Gaussian Splatting - into a unified, automated environment reconstruction pipeline for planetary robotics. Our system combines the Nerfstudio and COLMAP frameworks with a ROS2-compatible workflow capable of processing raw rover data directly from rosbag recordings. This approach enables the generation of dense, photorealistic, and metrically consistent 3D representations from minimal visual input, supporting improved perception and planning for autonomous systems operating in planetary-like conditions. The resulting pipeline established a foundation for future research in radiance field-based mapping, bridging the gap between geometric and neural representations in planetary exploration.",
          "authors": [
            "Alfonso Martínez-Petersen",
            "Levin Gerdes",
            "David Rodríguez-Martínez",
            "C. J. Pérez-del-Pulgar"
          ],
          "published": "2026-02-14T22:07:03Z",
          "updated": "2026-02-14T22:07:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13909v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13901v1",
          "title": "RPGD: RANSAC-P3P Gradient Descent for Extrinsic Calibration in 3D Human Pose Estimation",
          "summary": "In this paper, we propose RPGD (RANSAC-P3P Gradient Descent), a human-pose-driven extrinsic calibration framework that robustly aligns MoCap-based 3D skeletal data with monocular or multi-view RGB cameras using only natural human motion. RPGD formulates extrinsic calibration as a coarse-to-fine problem tailored to human poses, combining the global robustness of RANSAC-P3P with Gradient-Descent-based refinement. We evaluate RPGD on three large-scale public 3D HPE datasets as well as on a self-collected in-the-wild dataset. Experimental results demonstrate that RPGD consistently recovers extrinsic parameters with accuracy comparable to the provided ground truth, achieving sub-pixel MPJPE reprojection error even in challenging, noisy settings. These results indicate that RPGD provides a practical and automatic solution for reliable extrinsic calibration of large-scale 3D HPE dataset collection.",
          "authors": [
            "Zhanyu Tuo"
          ],
          "published": "2026-02-14T21:49:51Z",
          "updated": "2026-02-14T21:49:51Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13901v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13900v1",
          "title": "UAV-SEAD: State Estimation Anomaly Dataset for UAVs",
          "summary": "Accurate state estimation in Unmanned Aerial Vehicles (UAVs) is crucial for ensuring reliable and safe operation, as anomalies occurring during mission execution may induce discrepancies between expected and observed system behaviors, thereby compromising mission success or posing potential safety hazards. It is essential to continuously monitor and detect such conditions in order to ensure a timely response and maintain system reliability. In this work, we focus on UAV state estimation anomalies and provide a large-scale real-world UAV dataset to facilitate research aimed at improving the development of anomaly detection. Unlike existing datasets that primarily rely on injected faults into simulated data, this dataset comprises 1396 real flight logs totaling over 52 hours of flight time, collected across diverse indoor and outdoor environments using a collection of PX4-based UAVs equipped with a variety of sensor configurations. The dataset comprises both normal and anomalous flights without synthetic manipulation, making it uniquely suitable for realistic anomaly detection tasks. A structured classification is proposed that categorizes UAV state estimation anomalies into four classes: mechanical and electrical, external position, global position, and altitude anomalies. These classifications reflect collective, contextual, and outlier anomalies observed in multivariate sensor data streams, including IMU, GPS, barometer, magnetometer, distance sensors, visual odometry, and optical flow, that can be found in the PX4 logging mechanism. It is anticipated that this dataset will play a key role in the development, training, and evaluation of anomaly detection and isolation systems to address the critical gap in UAV reliability research.",
          "authors": [
            "Aykut Kabaoglu",
            "Sanem Sariel"
          ],
          "published": "2026-02-14T21:45:30Z",
          "updated": "2026-02-14T21:45:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13900v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13866v1",
          "title": "Modeling and Optimizing the Provisioning of Exhaustible Capabilities for Simultaneous Task Allocation and Scheduling",
          "summary": "Deploying heterogeneous robot teams to accomplish multiple tasks over extended time horizons presents significant computational challenges for task allocation and planning. In this paper, we present a comprehensive, time-extended, offline heterogeneous multi-robot task allocation framework, TRAITS, which we believe to be the first that can cope with the provisioning of exhaustible traits under battery and temporal constraints. Specifically, we introduce a nonlinear programming-based trait distribution module that can optimize the trait-provisioning rate of coalitions to yield feasible and time-efficient solutions. TRAITS provides a more accurate feasibility assessment and estimation of task execution times and makespan by leveraging trait-provisioning rates while optimizing battery consumption -- an advantage that state-of-the-art frameworks lack. We evaluate TRAITS against two state-of-the-art frameworks, with results demonstrating its advantage in satisfying complex trait and battery requirements while remaining computationally tractable.",
          "authors": [
            "Jinwoo Park",
            "Harish Ravichandar",
            "Seth Hutchinson"
          ],
          "published": "2026-02-14T19:55:15Z",
          "updated": "2026-02-14T19:55:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13866v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13865v1",
          "title": "Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay",
          "summary": "Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.",
          "authors": [
            "Gabriel Romio",
            "Mateus Begnini Melchiades",
            "Bruno Castro da Silva",
            "Gabriel de Oliveira Ramos"
          ],
          "published": "2026-02-14T19:55:11Z",
          "updated": "2026-02-14T19:55:11Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13865v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13850v2",
          "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement",
          "summary": "We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce \\emph{Humanoid Hanoi}, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines.",
          "authors": [
            "Minku Kim",
            "Kuan-Chia Chen",
            "Aayam Shrestha",
            "Li Fuxin",
            "Stefan Lee",
            "Alan Fern"
          ],
          "published": "2026-02-14T19:11:02Z",
          "updated": "2026-02-18T23:55:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13850v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13850v3",
          "title": "Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement",
          "summary": "We investigate a skill-based framework for humanoid box rearrangement that enables long-horizon execution by sequencing reusable skills at the task level. In our architecture, all skills execute through a shared, task-agnostic whole-body controller (WBC), providing a consistent closed-loop interface for skill composition, in contrast to non-shared designs that use separate low-level controllers per skill. We find that naively reusing the same pretrained WBC can reduce robustness over long horizons, as new skills and their compositions induce shifted state and command distributions. We address this with a simple data aggregation procedure that augments shared-WBC training with rollouts from closed-loop skill execution under domain randomization. To evaluate the approach, we introduce Humanoid Hanoi, a long-horizon Tower-of-Hanoi box rearrangement benchmark, and report results in simulation and on the Digit V3 humanoid robot, demonstrating fully autonomous rearrangement over extended horizons and quantifying the benefits of the shared-WBC approach over non-shared baselines. Project page: https://osudrl.github.io/Humanoid_Hanoi/",
          "authors": [
            "Minku Kim",
            "Kuan-Chia Chen",
            "Aayam Shrestha",
            "Li Fuxin",
            "Stefan Lee",
            "Alan Fern"
          ],
          "published": "2026-02-14T19:11:02Z",
          "updated": "2026-02-23T07:14:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13850v3",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13849v1",
          "title": "Push-Placement: A Hybrid Approach Integrating Prehensile and Non-Prehensile Manipulation for Object Rearrangement",
          "summary": "Efficient tabletop rearrangement remains challenging due to collisions and the need for temporary buffering when target poses are obstructed. Prehensile pick-and-place provides precise control but often requires extra moves, whereas non-prehensile pushing can be more efficient but suffers from complex, imprecise dynamics. This paper proposes push-placement, a hybrid action primitive that uses the grasped object to displace obstructing items while being placed, thereby reducing explicit buffering. The method is integrated into a physics-in-the-loop Monte Carlo Tree Search (MCTS) planner and evaluated in the PyBullet simulator. Empirical results show push-placement reduces the manipulator travel cost by up to 11.12% versus a baseline MCTS planner and 8.56% versus dynamic stacking. These findings indicate that hybrid prehensile/non-prehensile action primitives can substantially improve efficiency in long-horizon rearrangement tasks.",
          "authors": [
            "Majid Sadeghinejad",
            "Arman Barghi",
            "Hamed Hosseini",
            "Mehdi Tale Masouleh",
            "Ahmad Kalhor"
          ],
          "published": "2026-02-14T18:58:00Z",
          "updated": "2026-02-14T18:58:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13849v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13833v1",
          "title": "Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation",
          "summary": "Generalizing tool manipulation requires both semantic planning and precise physical control. Modern generalist robot policies, such as Vision-Language-Action (VLA) models, often lack the high-fidelity physical grounding required for contact-rich tool manipulation. Conversely, existing contact-aware policies that leverage tactile or haptic sensing are typically instance-specific and fail to generalize across diverse tool geometries. Bridging this gap requires learning unified contact representations from diverse data, yet a fundamental barrier remains: diverse real-world tactile data are prohibitive at scale, while direct zero-shot sim-to-real transfer is challenging due to the complex dynamics of nonlinear deformation of soft sensors. To address this, we propose Semantic-Contact Fields (SCFields), a unified 3D representation fusing visual semantics with dense contact estimates. We enable this via a two-stage Sim-to-Real Contact Learning Pipeline: first, we pre-train on a large simulation data set to learn general contact physics; second, we fine-tune on a small set of real data, pseudo-labeled via geometric heuristics and force optimization, to align sensor characteristics. This allows physical generalization to unseen tools. We leverage SCFields as the dense observation input for a diffusion policy to enable robust execution of contact-rich tool manipulation tasks. Experiments on scraping, crayon drawing, and peeling demonstrate robust category-level generalization, significantly outperforming vision-only and raw-tactile baselines.",
          "authors": [
            "Kevin Yuchen Ma",
            "Heng Zhang",
            "Weisi Lin",
            "Mike Zheng Shou",
            "Yan Wu"
          ],
          "published": "2026-02-14T16:05:08Z",
          "updated": "2026-02-14T16:05:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13833v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13806v1",
          "title": "Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos",
          "summary": "Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.",
          "authors": [
            "Can Li",
            "Jie Gu",
            "Jingmin Chen",
            "Fangzhou Qiu",
            "Lei Sun"
          ],
          "published": "2026-02-14T14:30:25Z",
          "updated": "2026-02-14T14:30:25Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13806v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13800v1",
          "title": "Ontological grounding for sound and natural robot explanations via large language models",
          "summary": "Building effective human-robot interaction requires robots to derive conclusions from their experiences that are both logically sound and communicated in ways aligned with human expectations. This paper presents a hybrid framework that blends ontology-based reasoning with large language models (LLMs) to produce semantically grounded and natural robot explanations. Ontologies ensure logical consistency and domain grounding, while LLMs provide fluent, context-aware and adaptive language generation. The proposed method grounds data from human-robot experiences, enabling robots to reason about whether events are typical or atypical based on their properties. We integrate a state-of-the-art algorithm for retrieving and constructing static contrastive ontology-based narratives with an LLM agent that uses them to produce concise, clear, interactive explanations. The approach is validated through a laboratory study replicating an industrial collaborative task. Empirical results show significant improvements in the clarity and brevity of ontology-based narratives while preserving their semantic accuracy. Initial evaluations further demonstrate the system's ability to adapt explanations to user feedback. Overall, this work highlights the potential of ontology-LLM integration to advance explainable agency, and promote more transparent human-robot collaboration.",
          "authors": [
            "Alberto Olivares-Alarcos",
            "Muhammad Ahsan",
            "Satrio Sanjaya",
            "Hsien-I Lin",
            "Guillem Alenyà"
          ],
          "published": "2026-02-14T14:23:11Z",
          "updated": "2026-02-14T14:23:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13800v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13764v1",
          "title": "MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer",
          "summary": "While vision-language-action (VLA) models have advanced generalist robotic learning, cross-embodiment transfer remains challenging due to kinematic heterogeneity and the high cost of collecting sufficient real-world demonstrations to support fine-tuning. Existing cross-embodiment policies typically rely on shared-private architectures, which suffer from limited capacity of private parameters and lack explicit adaptation mechanisms. To address these limitations, we introduce MOTIF for efficient few-shot cross-embodiment transfer that decouples embodiment-agnostic spatiotemporal patterns, termed action motifs, from heterogeneous action data. Specifically, MOTIF first learns unified motifs via vector quantization with progress-aware alignment and embodiment adversarial constraints to ensure temporal and cross-embodiment consistency. We then design a lightweight predictor that predicts these motifs from real-time inputs to guide a flow-matching policy, fusing them with robot-specific states to enable action generation on new embodiments. Evaluations across both simulation and real-world environments validate the superiority of MOTIF, which significantly outperforms strong baselines in few-shot transfer scenarios by 6.5% in simulation and 43.7% in real-world settings. Code is available at https://github.com/buduz/MOTIF.",
          "authors": [
            "Heng Zhi",
            "Wentao Tan",
            "Lei Zhu",
            "Fengling Li",
            "Jingjing Li",
            "Guoli Yang",
            "Heng Tao Shen"
          ],
          "published": "2026-02-14T13:21:40Z",
          "updated": "2026-02-14T13:21:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13764v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13762v1",
          "title": "Impact-Robust Posture Optimization for Aerial Manipulation",
          "summary": "We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot's state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot's configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot's state w.r.t. nominal TSID.",
          "authors": [
            "Amr Afifi",
            "Ahmad Gazar",
            "Javier Alonso-Mora",
            "Paolo Robuffo Giordano",
            "Antonio Franchi"
          ],
          "published": "2026-02-14T13:16:46Z",
          "updated": "2026-02-14T13:16:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13762v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13762v2",
          "title": "Impact-Robust Posture Optimization for Aerial Manipulation",
          "summary": "We present a novel method for optimizing the posture of kinematically redundant torque-controlled robots to improve robustness during impacts. A rigid impact model is used as the basis for a configuration-dependent metric that quantifies the variation between pre- and post-impact velocities. By finding configurations (postures) that minimize the aforementioned metric, spikes in the robot's state and input commands can be significantly reduced during impacts, improving safety and robustness. The problem of identifying impact-robust postures is posed as a min-max optimization of the aforementioned metric. To overcome the real-time intractability of the problem, we reformulate it as a gradient-based motion task that iteratively guides the robot towards configurations that minimize the proposed metric. This task is embedded within a task-space inverse dynamics (TSID) whole-body controller, enabling seamless integration with other control objectives. The method is applied to a kinematically redundant aerial manipulator performing repeated point contact tasks. We test our method inside a realistic physics simulator and compare it with the nominal TSID. Our method leads to a reduction (up to 51% w.r.t. standard TSID) of post-impact spikes in the robot's configuration and successfully avoids actuator saturation. Moreover, we demonstrate the importance of kinematic redundancy for impact robustness using additional numerical simulations on a quadruped and a humanoid robot, resulting in up to 45% reduction of post-impact spikes in the robot's state w.r.t. nominal TSID.",
          "authors": [
            "Amr Afifi",
            "Ahmad Gazar",
            "Javier Alonso-Mora",
            "Paolo Robuffo Giordano",
            "Antonio Franchi"
          ],
          "published": "2026-02-14T13:16:46Z",
          "updated": "2026-02-22T10:55:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13762v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13747v1",
          "title": "The More the Merrier: Running Multiple Neuromorphic Components On-Chip for Robotic Control",
          "summary": "It has long been realized that neuromorphic hardware offers benefits for the domain of robotics such as low energy, low latency, as well as unique methods of learning. In aiming for more complex tasks, especially those incorporating multimodal data, one hurdle continuing to prevent their realization is an inability to orchestrate multiple networks on neuromorphic hardware without resorting to off-chip process management logic. To address this, we show a first example of a pipeline for vision-based robot control in which numerous complex networks can be run entirely on hardware via the use of a spiking neural state machine for process orchestration. The pipeline is validated on the Intel Loihi 2 research chip. We show that all components can run concurrently on-chip in the milli Watt regime at latencies competitive with the state-of-the-art. An equivalent network on simulated hardware is shown to accomplish robotic arm plug insertion in simulation, and the core elements of the pipeline are additionally tested on a real robotic arm.",
          "authors": [
            "Evan Eames",
            "Priyadarshini Kannan",
            "Ronan Sangouard",
            "Philipp Plank",
            "Elvin Hajizada",
            "Gintautas Palinauskas",
            "Lana Amaya",
            "Michael Neumeier",
            "Sai Thejeshwar Sharma",
            "Marcella Toth",
            "Prottush Sarkar",
            "Axel von Arnim"
          ],
          "published": "2026-02-14T12:39:15Z",
          "updated": "2026-02-14T12:39:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13747v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13739v1",
          "title": "XIT: Exploration and Exploitation Informed Trees for Active Gas Distribution Mapping in Unknown Environments",
          "summary": "Mobile robotic gas distribution mapping (GDM) provides critical situational awareness during emergency responses to hazardous gas releases. However, most systems still rely on teleoperation, limiting scalability and response speed. Autonomous active GDM is challenging in unknown and cluttered environments, because the robot must simultaneously explore traversable space, map the environment, and infer the gas distribution belief from sparse chemical measurements. We address this by formulating active GDM as a next-best-trajectory informative path planning (IPP) problem and propose XIT (Exploration-Exploitation Informed Trees), a sampling-based planner that balances exploration and exploitation by generating concurrent trajectories toward exploration-rich goals while collecting informative gas measurements en route. XIT draws batches of samples from an Upper Confidence Bound (UCB) information field derived from the current gas posterior and expands trees using a cost that trades off travel effort against gas concentration and uncertainty. To enable plume-aware exploration, we introduce the gas frontier concept, defined as unobserved regions adjacent to high gas concentrations, and propose the Wavefront Gas Frontier Detection (WGFD) algorithm for their identification. High-fidelity simulations and real-world experiments demonstrate the benefits of XIT in terms of GDM quality and efficiency. Although developed for active GDM, XIT is readily applicable to other robotic information-gathering tasks in unknown environments that face the exploration and exploitation trade-off.",
          "authors": [
            "Mal Fazliu",
            "Matthew Coombes",
            "Sen Wang",
            "Cunjia Liu"
          ],
          "published": "2026-02-14T12:11:48Z",
          "updated": "2026-02-14T12:11:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13739v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13733v1",
          "title": "Improving Driver Satisfaction with a Driving Function Learning from Implicit Human Feedback -- a Test Group Study",
          "summary": "During the use of advanced driver assistance systems, drivers frequently intervene into the active driving function and adjust the system's behavior to their personal wishes. These active driver-initiated takeovers contain feedback about deviations in the driving function's behavior from the drivers' personal preferences. This feedback should be utilized to optimize and personalize the driving function's behavior. In this work, the adjustment of the speed profile of a Predictive Longitudinal Driving Function (PLDF) on a pre-defined route is highlighted. An algorithm is introduced which iteratively adjusts the PLDF's speed profile by taking into account both the original speed profile of the PLDF and the driver demonstration. This approach allows for personalization in a traded control scenario during active use of the PLDF. The applicability of the proposed algorithm is tested in a driving simulator-based test group study with 43 participants. The study finds a significant increase in driver satisfaction and a significant reduction in the intervention frequency when using the proposed adaptive PLDF. Additionally, feedback by the participants was gathered to identify further optimization potentials of the proposed system.",
          "authors": [
            "Robin Schwager",
            "Andrea Anastasio",
            "Simon Hartmann",
            "Andreas Ronellenfitsch",
            "Michael Grimm",
            "Tim Brühl",
            "Tin Stribor Sohn",
            "Tim Dieter Eberhardt",
            "Sören Hohmann"
          ],
          "published": "2026-02-14T11:57:11Z",
          "updated": "2026-02-14T11:57:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13733v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13720v1",
          "title": "FC-Vision: Real-Time Visibility-Aware Replanning for Occlusion-Free Aerial Target Structure Scanning in Unknown Environments",
          "summary": "Autonomous aerial scanning of target structures is crucial for practical applications, requiring online adaptation to unknown obstacles during flight. Existing methods largely emphasize collision avoidance and efficiency, but overlook occlusion-induced visibility degradation, severely compromising scanning quality. In this study, we propose FC-Vision, an on-the-fly visibility-aware replanning framework that proactively and safely prevents target occlusions while preserving the intended coverage and efficiency of the original plan. Our approach explicitly enforces dense surface-visibility constraints to regularize replanning behavior in real-time via an efficient two-level decomposition: occlusion-free viewpoint repair that maintains coverage with minimal deviation from the nominal scan intent, followed by segment-wise clean-sensing connection in 5-DoF space. A plug-in integration strategy is also presented to seamlessly interface FC-Vision with existing UAV scanning systems without architectural changes. Comprehensive simulation and real-world evaluations show that FC-Vision consistently improves scanning quality under unexpected occluders, delivering a maximum coverage gain of 55.32% and a 73.17% reduction in the occlusion ratio, while achieving real-time performance with a moderate increase in flight time. The source code will be made publicly available.",
          "authors": [
            "Chen Feng",
            "Yang Xu",
            "Shaojie Shen"
          ],
          "published": "2026-02-14T11:01:40Z",
          "updated": "2026-02-14T11:01:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13720v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13718v1",
          "title": "HybridFlow: A Two-Step Generative Policy for Robotic Manipulation",
          "summary": "Limited by inference latency, existing robot manipulation policies lack sufficient real-time interaction capability with the environment. Although faster generation methods such as flow matching are gradually replacing diffusion methods, researchers are pursuing even faster generation suitable for interactive robot control. MeanFlow, as a one-step variant of flow matching, has shown strong potential in image generation, but its precision in action generation does not meet the stringent requirements of robotic manipulation. We therefore propose \\textbf{HybridFlow}, a \\textbf{3-stage method} with \\textbf{2-NFE}: Global Jump in MeanFlow mode, ReNoise for distribution alignment, and Local Refine in ReFlow mode. This method balances inference speed and generation quality by leveraging the rapid advantage of MeanFlow one-step generation while ensuring action precision with minimal generation steps. Through real-world experiments, HybridFlow outperforms the 16-step Diffusion Policy by \\textbf{15--25\\%} in success rate while reducing inference time from 152ms to 19ms (\\textbf{8$\\times$ speedup}, \\textbf{$\\sim$52Hz}); it also achieves 70.0\\% success on unseen-color OOD grasping and 66.3\\% on deformable object folding. We envision HybridFlow as a practical low-latency method to enhance real-world interaction capabilities of robotic manipulation policies.",
          "authors": [
            "Zhenchen Dong",
            "Jinna Fu",
            "Jiaming Wu",
            "Shengyuan Yu",
            "Fulin Chen",
            "Yide Liu"
          ],
          "published": "2026-02-14T10:50:23Z",
          "updated": "2026-02-14T10:50:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13718v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13689v1",
          "title": "Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation",
          "summary": "Insertion tasks in robotic manipulation demand precise, contact-rich interactions that vision alone cannot resolve. While tactile feedback is intuitively valuable, existing studies have shown that naïve visuo-tactile fusion often fails to deliver consistent improvements. In this work, we propose a Cross-Modal Transformer (CMT) for visuo-tactile fusion that integrates wrist-camera observations with tactile signals through structured self- and cross-attention. To stabilize tactile embeddings, we further introduce a physics-informed regularization that encourages bilateral force balance, reflecting principles of human motor control. Experiments on the TacSL benchmark show that CMT with symmetry regularization achieves a 96.59% insertion success rate, surpassing naïve and gated fusion baselines and closely matching the privileged \"wrist + contact force\" configuration (96.09%). These results highlight two central insights: (i) tactile sensing is indispensable for precise alignment, and (ii) principled multimodal fusion, further strengthened by physics-informed regularization, unlocks complementary strengths of vision and touch, approaching privileged performance under realistic sensing.",
          "authors": [
            "Wonju Lee",
            "Matteo Grimaldi",
            "Tao Yu"
          ],
          "published": "2026-02-14T09:19:48Z",
          "updated": "2026-02-14T09:19:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13689v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13656v1",
          "title": "A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking",
          "summary": "Current humanoid motion tracking systems can execute routine and moderately dynamic behaviors, yet significant gaps remain near hardware performance limits and algorithmic robustness boundaries. Martial arts represent an extreme case of highly dynamic human motion, characterized by rapid center-of-mass shifts, complex coordination, and abrupt posture transitions. However, datasets tailored to such high-intensity scenarios remain scarce. To address this gap, we construct KungFuAthlete, a high-dynamic martial arts motion dataset derived from professional athletes' daily training videos. The dataset includes ground and jump subsets covering representative complex motion patterns. The jump subset exhibits substantially higher joint, linear, and angular velocities compared to commonly used datasets such as LAFAN1, PHUMA, and AMASS, indicating significantly increased motion intensity and complexity. Importantly, even professional athletes may fail during highly dynamic movements. Similarly, humanoid robots are prone to instability and falls under external disturbances or execution errors. Most prior work assumes motion execution remains within safe states and lacks a unified strategy for modeling unsafe states and enabling reliable autonomous recovery. We propose a novel training paradigm that enables a single policy to jointly learn high-dynamic motion tracking and fall recovery, unifying agile execution and stabilization within one framework. This framework expands robotic capability from pure motion tracking to recovery-enabled execution, promoting more robust and autonomous humanoid performance in real-world high-dynamic scenarios.",
          "authors": [
            "Zhongxiang Lei",
            "Lulu Cao",
            "Xuyang Wang",
            "Tianyi Qian",
            "Jinyan Liu",
            "Xuesong Li"
          ],
          "published": "2026-02-14T07:49:56Z",
          "updated": "2026-02-14T07:49:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13656v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13641v1",
          "title": "SPLIT: Sparse Incremental Learning of Error Dynamics for Control-Oriented Modeling in Autonomous Vehicles",
          "summary": "Accurate, computationally efficient, and adaptive vehicle models are essential for autonomous vehicle control. Hybrid models that combine a nominal model with a Gaussian Process (GP)-based residual model have emerged as a promising approach. However, the GP-based residual model suffers from the curse of dimensionality, high evaluation complexity, and the inefficiency of online learning, which impede the deployment in real-time vehicle controllers. To address these challenges, we propose SPLIT, a sparse incremental learning framework for control-oriented vehicle dynamics modeling. SPLIT integrates three key innovations: (i) Model Decomposition. We decompose the vehicle model into invariant elements calibrated by experiments, and variant elements compensated by the residual model to reduce feature dimensionality. (ii) Local Incremental Learning. We define the valid region in the feature space and partition it into subregions, enabling efficient online learning from streaming data. (iii) GP Sparsification. We use bayesian committee machine to ensure scalable online evaluation. Integrated into model-based controllers, SPLIT is evaluated in aggressive simulations and real-vehicle experiments. Results demonstrate that SPLIT improves model accuracy and control performance online. Moreover, it enables rapid adaptation to vehicle dynamics deviations and exhibits robust generalization to previously unseen scenarios.",
          "authors": [
            "Yaoyu Li",
            "Chaosheng Huang",
            "Jun Li"
          ],
          "published": "2026-02-14T07:12:05Z",
          "updated": "2026-02-14T07:12:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13641v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13640v1",
          "title": "Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation",
          "summary": "Existing robotic manipulation methods primarily rely on visual and proprioceptive observations, which may struggle to infer contact-related interaction states in partially observable real-world environments. Acoustic cues, by contrast, naturally encode rich interaction dynamics during contact, yet remain underexploited in current multimodal fusion literature. Most multimodal fusion approaches implicitly assume homogeneous roles across modalities, and thus design flat and symmetric fusion structures. However, this assumption is ill-suited for acoustic signals, which are inherently sparse and contact-driven. To achieve precise robotic manipulation through acoustic-informed perception, we propose a hierarchical representation fusion framework that progressively integrates audio, vision, and proprioception. Our approach first conditions visual and proprioceptive representations on acoustic cues, and then explicitly models higher-order cross-modal interactions to capture complementary dependencies among modalities. The fused representation is leveraged by a diffusion-based policy to directly generate continuous robot actions from multimodal observations. The combination of end-to-end learning and hierarchical fusion structure enables the policy to exploit task-relevant acoustic information while mitigating interference from less informative modalities. The proposed method has been evaluated on real-world robotic manipulation tasks, including liquid pouring and cabinet opening. Extensive experiment results demonstrate that our approach consistently outperforms state-of-the-art multimodal fusion frameworks, particularly in scenarios where acoustic cues provide task-relevant information not readily available from visual observations alone. Furthermore, a mutual information analysis is conducted to interpret the effect of audio cues in robotic manipulation via multimodal fusion.",
          "authors": [
            "Siyuan Li",
            "Jiani Lu",
            "Yu Song",
            "Xianren Li",
            "Bo An",
            "Peng Liu"
          ],
          "published": "2026-02-14T07:11:29Z",
          "updated": "2026-02-14T07:11:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13640v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13591v1",
          "title": "AgentRob: From Virtual Forum Agents to Hijacked Physical Robots",
          "summary": "Large Language Model (LLM)-powered autonomous agents have demonstrated significant capabilities in virtual environments, yet their integration with the physical world remains narrowly confined to direct control interfaces. We present AgentRob, a framework that bridges online community forums, LLM-powered agents, and physical robots through the Model Context Protocol (MCP). AgentRob enables a novel paradigm where autonomous agents participate in online forums--reading posts, extracting natural language commands, dispatching physical robot actions, and reporting results back to the community. The system comprises three layers: a Forum Layer providing asynchronous, persistent, multi-agent interaction; an Agent Layer with forum agents that poll for @mention-targeted commands; and a Robot Layer with VLM-driven controllers and Unitree Go2/G1 hardware that translate commands into robot primitives via iterative tool calling. The framework supports multiple concurrent agents with distinct identities and physical embodiments coexisting in the same forum, establishing the feasibility of forum-mediated multi-agent robot orchestration.",
          "authors": [
            "Wenrui Liu",
            "Yaxuan Wang",
            "Xun Zhang",
            "Yanshu Wang",
            "Jiashen Wei",
            "Yifan Xiang",
            "Yuhang Wang",
            "Mingshen Ye",
            "Elsie Dai",
            "Zhiqi Liu",
            "Yingjie Xu",
            "Xinyang Chen",
            "Hengzhe Sun",
            "Jiyu Shen",
            "Jingjing He",
            "Tong Yang"
          ],
          "published": "2026-02-14T04:14:59Z",
          "updated": "2026-02-14T04:14:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13591v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13579v1",
          "title": "TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment",
          "summary": "Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).",
          "authors": [
            "Youngsun Wi",
            "Jessica Yin",
            "Elvis Xiang",
            "Akash Sharma",
            "Jitendra Malik",
            "Mustafa Mukadam",
            "Nima Fazeli",
            "Tess Hellebrekers"
          ],
          "published": "2026-02-14T03:31:32Z",
          "updated": "2026-02-14T03:31:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13579v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13577v1",
          "title": "ONRAP: Occupancy-driven Noise-Resilient Autonomous Path Planning",
          "summary": "Dynamic path planning must remain reliable in the presence of sensing noise, uncertain localization, and incomplete semantic perception. We propose a practical, implementation-friendly planner that operates on occupancy grids and optionally incorporates occupancy-flow predictions to generate ego-centric, kinematically feasible paths that safely navigate through static and dynamic obstacles. The core is a nonlinear program in the spatial domain built on a modified bicycle model with explicit feasibility and collision-avoidance penalties. The formulation naturally handles unknown obstacle classes and heterogeneous agent motion by operating purely in occupancy space. The pipeline runs in real-time (faster than 10 Hz on average), requires minimal tuning, and interfaces cleanly with standard control stacks. We validate our approach in simulation with severe localization and perception noises, and on an F1TENTH platform, demonstrating smooth and safe maneuvering through narrow passages and rough routes. The approach provides a robust foundation for noise-resilient, prediction-aware planning, eliminating the need for handcrafted heuristics. The project website can be accessed at https://honda-research-institute.github.io/onrap/",
          "authors": [
            "Faizan M. Tariq",
            "Avinash Singh",
            "Vipul Ramtekkar",
            "Jovin D'sa",
            "David Isele",
            "Yosuke Sakamoto",
            "Sangjae Bae"
          ],
          "published": "2026-02-14T03:27:46Z",
          "updated": "2026-02-14T03:27:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13577v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13554v1",
          "title": "From Snapshot Sensing to Persistent EM World Modeling: A Generative-Space Perspective for ISAC",
          "summary": "Electromagnetic (EM) world modeling is emerging as a foundational capability for environment-aware and embodiment-enabled wireless systems. However, most existing mmWave sensing solutions are designed for snapshot-based parameter estimation and rely on hardware-intensive architectures, making scalable and persistent world modeling difficult to achieve. This article rethinks mmWave sensing from a system-level perspective and introduces a generative-space framework, in which sensing is realized through controlled traversal of a low-dimensional excitation space spanning frequency, waveform, and physical embodiment. This perspective decouples spatial observability from rigid antenna arrays and transmit-time multiplexing, enabling flexible and scalable sensing-by-design radios. To illustrate the practicality of this framework, we present a representative realization called Multi-RF Chain Frequency-as-Aperture Clip-on Aperture Fabric (MRC-FaA-CAF), where multiple FMCW sources coordinate frequency-selective modules distributed along guided-wave backbones. This architecture enables interference-free excitation, preserves beat-frequency separability, and maintains low calibration overhead. Case studies show that generative-space-driven sensing can achieve update rates comparable to phased arrays while avoiding dense RF replication and the latency penalties of TDM-MIMO systems. Overall, this work positions generative-space-driven sensing as a practical architectural foundation for mmWave systems that move beyond snapshot sensing toward persistent EM world modeling.",
          "authors": [
            "Pin-Han Ho",
            "Haoran Mei",
            "Limei Peng",
            "Yiming Miao",
            "Kairan Liang",
            "Yan Jiao"
          ],
          "published": "2026-02-14T02:07:31Z",
          "updated": "2026-02-14T02:07:31Z",
          "primary_category": "cs.ET",
          "categories": [
            "cs.ET",
            "cs.IT",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13554v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13476v1",
          "title": "AsyncVLA: An Asynchronous VLA for Fast and Robust Navigation on the Edge",
          "summary": "Robotic foundation models achieve strong generalization by leveraging internet-scale vision-language representations, but their massive computational cost creates a fundamental bottleneck: high inference latency. In dynamic environments, this latency breaks the control loop, rendering powerful models unsafe for real-time deployment. We propose AsyncVLA, an asynchronous control framework that decouples semantic reasoning from reactive execution. Inspired by hierarchical control, AsyncVLA runs a large foundation model on a remote workstation to provide high-level guidance, while a lightweight, onboard Edge Adapter continuously refines actions at high frequency. To bridge the domain gap between these asynchronous streams, we introduce an end-to-end finetuning protocol and a trajectory re-weighting strategy that prioritizes dynamic interactions. We evaluate our approach on real-world vision-based navigation tasks with communication delays up to 6 seconds. AsyncVLA achieves a 40% higher success rate than state-of-the-art baselines, effectively bridging the gap between the semantic intelligence of large models and the reactivity required for edge robotics.",
          "authors": [
            "Noriaki Hirose",
            "Catherine Glossop",
            "Dhruv Shah",
            "Sergey Levine"
          ],
          "published": "2026-02-13T21:31:19Z",
          "updated": "2026-02-13T21:31:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13476v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13457v1",
          "title": "Inferring Turn-Rate-Limited Engagement Zones with Sacrificial Agents for Safe Trajectory Planning",
          "summary": "This paper presents a learning-based framework for estimating pursuer parameters in turn-rate-limited pursuit-evasion scenarios using sacrificial agents. Each sacrificial agent follows a straight-line trajectory toward an adversary and reports whether it was intercepted or survived. These binary outcomes are related to the pursuer's parameters through a geometric reachable-region (RR) model. Two formulations are introduced: a boundary-interception case, where capture occurs at the RR boundary, and an interior-interception case, which allows capture anywhere within it. The pursuer's parameters are inferred using a gradient-based multi-start optimization with custom loss functions tailored to each case. Two trajectory-selection strategies are proposed for the sacrificial agents: a geometric heuristic that maximizes the spread of expected interception points, and a Bayesian experimental-design method that maximizes the D-score of the expected Gauss-Newton information matrix, thereby selecting trajectories that yield maximal information gain. Monte Carlo experiments demonstrate accurate parameter recovery with five to twelve sacrificial agents. The learned engagement models are then used to generate safe, time-optimal paths for high-value agents that avoid all feasible pursuer engagement regions.",
          "authors": [
            "Grant Stagg",
            "Cameron K. Peterson"
          ],
          "published": "2026-02-13T21:03:26Z",
          "updated": "2026-02-13T21:03:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13457v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13444v1",
          "title": "FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation",
          "summary": "Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots. We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and employing a motion-text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7$\\times$ higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40$\\times$ inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.",
          "authors": [
            "Huajian Zeng",
            "Lingyun Chen",
            "Jiaqi Yang",
            "Yuantai Zhang",
            "Fan Shi",
            "Peidong Liu",
            "Xingxing Zuo"
          ],
          "published": "2026-02-13T20:46:08Z",
          "updated": "2026-02-13T20:46:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13444v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13440v1",
          "title": "Learning on the Fly: Replay-Based Continual Object Perception for Indoor Drones",
          "summary": "Autonomous agents such as indoor drones must learn new object classes in real-time while limiting catastrophic forgetting, motivating Class-Incremental Learning (CIL). However, most unmanned aerial vehicle (UAV) datasets focus on outdoor scenes and offer limited temporally coherent indoor videos. We introduce an indoor dataset of $14,400$ frames capturing inter-drone and ground vehicle footage, annotated via a semi-automatic workflow with a $98.6\\%$ first-pass labeling agreement before final manual verification. Using this dataset, we benchmark 3 replay-based CIL strategies: Experience Replay (ER), Maximally Interfered Retrieval (MIR), and Forgetting-Aware Replay (FAR), using YOLOv11-nano as a resource-efficient detector for deployment-constrained UAV platforms. Under tight memory budgets ($5-10\\%$ replay), FAR performs better than the rest, achieving an average accuracy (ACC, $mAP_{50-95}$ across increments) of $82.96\\%$ with $5\\%$ replay. Gradient-weighted class activation mapping (Grad-CAM) analysis shows attention shifts across classes in mixed scenes, which is associated with reduced localization quality for drones. The experiments further demonstrate that replay-based continual learning can be effectively applied to edge aerial systems. Overall, this work contributes an indoor UAV video dataset with preserved temporal coherence and an evaluation of replay-based CIL under limited replay budgets. Project page: https://spacetime-vision-robotics-laboratory.github.io/learning-on-the-fly-cl",
          "authors": [
            "Sebastian-Ion Nae",
            "Mihai-Eugen Barbu",
            "Sebastian Mocanu",
            "Marius Leordeanu"
          ],
          "published": "2026-02-13T20:34:01Z",
          "updated": "2026-02-13T20:34:01Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13440v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13436v1",
          "title": "High-Fidelity, Customizable Force Sensing for the Wearable Human-Robot Interface",
          "summary": "Mechanically characterizing the human-machine interface is essential to understanding user behavior and optimizing wearable robot performance. This interface has been challenging to sensorize due to manufacturing complexity and non-linear sensor responses. Here, we measure human limb-device interaction via fluidic innervation, creating a 3D-printed silicone pad with embedded air channels to measure forces. As forces are applied to the pad, the air channels compress, resulting in a pressure change measurable by off-the-shelf pressure transducers. We demonstrate in benchtop testing that pad pressure is highly linearly related to applied force ($R^2 = 0.998$). This is confirmed with clinical dynamometer correlations with isometric knee torque, where above-knee pressure was highly correlated with flexion torque ($R^2 = 0.95$), while below-knee pressure was highly correlated with extension torque ($R^2 = 0.75$). We build on these idealized settings to test pad performance in more unconstrained settings. We place the pad over \\textit{biceps brachii} during cyclic curls and stepwise isometric holds, observing a correlation between pressure and elbow angle. Finally, we integrated the sensor into the strap of a lower-extremity robotic exoskeleton and recorded pad pressure during repeated squats with the device unpowered. Pad pressure tracked squat phase and overall task dynamics consistently. Overall, our preliminary results suggest fluidic innervation is a readily customizable sensing modality with high signal-to-noise ratio and temporal resolution for capturing human-machine mechanical interaction. In the long-term, this modality may provide an alternative real-time sensing input to control / optimize wearable robotic systems and to capture user function during device use.",
          "authors": [
            "Noah Rubin",
            "Ava Schraeder",
            "Hrishikesh Sahu",
            "Thomas C. Bulea",
            "Lillian Chin"
          ],
          "published": "2026-02-13T20:23:36Z",
          "updated": "2026-02-13T20:23:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13436v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.14979v1",
          "title": "RynnBrain: Open Embodied Foundation Models",
          "summary": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.",
          "authors": [
            "Ronghao Dang",
            "Jiayan Guo",
            "Bohan Hou",
            "Sicong Leng",
            "Kehan Li",
            "Xin Li",
            "Jiangpin Liu",
            "Yunxuan Mao",
            "Zhikai Wang",
            "Yuqian Yuan",
            "Minghao Zhu",
            "Xiao Lin",
            "Yang Bai",
            "Qian Jiang",
            "Yaxi Zhao",
            "Minghua Zeng",
            "Junlong Gao",
            "Yuming Jiang",
            "Jun Cen",
            "Siteng Huang",
            "Liuyi Wang",
            "Wenqiao Zhang",
            "Chengju Liu",
            "Jianfei Yang",
            "Shijian Lu",
            "Deli Zhao"
          ],
          "published": "2026-02-13T18:59:56Z",
          "updated": "2026-02-13T18:59:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.14979v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13197v1",
          "title": "Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos",
          "summary": "The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.",
          "authors": [
            "Albert J. Zhai",
            "Kuo-Hao Zeng",
            "Jiasen Lu",
            "Ali Farhadi",
            "Shenlong Wang",
            "Wei-Chiu Ma"
          ],
          "published": "2026-02-13T18:59:10Z",
          "updated": "2026-02-13T18:59:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13197v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13193v1",
          "title": "Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control",
          "summary": "Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks. Website: steerable-policies.github.io",
          "authors": [
            "William Chen",
            "Jagdeep Singh Bhatia",
            "Catherine Glossop",
            "Nikhil Mathihalli",
            "Ria Doshi",
            "Andy Tang",
            "Danny Driess",
            "Karl Pertsch",
            "Sergey Levine"
          ],
          "published": "2026-02-13T18:57:56Z",
          "updated": "2026-02-13T18:57:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13193v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13163v1",
          "title": "Human Emotion-Mediated Soft Robotic Arts: Exploring the Intersection of Human Emotions, Soft Robotics and Arts",
          "summary": "Soft robotics has emerged as a versatile field with applications across various domains, from healthcare to industrial automation, and more recently, art and interactive installations. The inherent flexibility, adaptability, and safety of soft robots make them ideal for applications that require delicate, organic, and lifelike movement, allowing for immersive and responsive interactions. This study explores the intersection of human emotions, soft robotics, and art to establish and create new forms of human emotion-mediated soft robotic art. In this paper, we introduce two soft embodiments: a soft character and a soft flower as an art display that dynamically responds to brain signals based on alpha waves, reflecting different emotion levels. We present how human emotions can be measured as alpha waves based on brain/EEG signals, how we map the alpha waves to the dynamic movements of the two soft embodiments, and demonstrate our proposed concept using experiments. The findings of this study highlight how soft robotics can embody human emotional states, offering a new medium for insightful artistic expression and interaction, and demonstrating how art displays can be embodied.",
          "authors": [
            "Saitarun Nadipineni",
            "Chenhao Hong",
            "Tanishtha Ramlall",
            "Chapa Sirithunge",
            "Kaspar Althoefer",
            "Fumiya Iida",
            "Thilina Dulantha Lalitharatne"
          ],
          "published": "2026-02-13T18:23:35Z",
          "updated": "2026-02-13T18:23:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13163v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13159v1",
          "title": "Temporally-Sampled Efficiently Adaptive State Lattices for Autonomous Ground Robot Navigation in Partially Observed Environments",
          "summary": "Due to sensor limitations, environments that off-road mobile robots operate in are often only partially observable. As the robots move throughout the environment and towards their goal, the optimal route is continuously revised as the sensors perceive new information. In traditional autonomous navigation architectures, a regional motion planner will consume the environment map and output a trajectory for the local motion planner to use as a reference. Due to the continuous revision of the regional plan guidance as a result of changing map information, the reference trajectories which are passed down to the local planner can differ significantly across sequential planning cycles. This rapidly changing guidance can result in unsafe navigation behavior, often requiring manual safety interventions during autonomous traversals in off-road environments. To remedy this problem, we propose Temporally-Sampled Efficiently Adaptive State Lattices (TSEASL), which is a regional planner arbitration architecture that considers updated and optimized versions of previously generated trajectories against the currently generated trajectory. When tested on a Clearpath Robotics Warthog Unmanned Ground Vehicle as well as real map data collected from the Warthog, results indicate that when running TSEASL, the robot did not require manual interventions in the same locations where the robot was running the baseline planner. Additionally, higher levels of planner stability were recorded with TSEASL over the baseline. The paper concludes with a discussion of further improvements to TSEASL in order to make it more generalizable to various off-road autonomy scenarios.",
          "authors": [
            "Ashwin Satish Menon",
            "Eric R. Damm",
            "Eli S. Lancaster",
            "Felix A. Sanchez",
            "Jason M. Gregory",
            "Thomas M. Howard"
          ],
          "published": "2026-02-13T18:14:56Z",
          "updated": "2026-02-13T18:14:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13159v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13157v1",
          "title": "A Data-Driven Algorithm for Model-Free Control Synthesis",
          "summary": "Presented is an algorithm to synthesize the optimal infinite-horizon LQR feedback controller for continuous-time systems. The algorithm does not require knowledge of the system dynamics but instead uses only a finite-length sampling of arbitrary input-output data. The algorithm is based on a constrained optimization problem that enforces a necessary condition on the dynamics of the optimal value function along any trajectory. In addition to calculating the standard LQR gain matrix, a feedforward gain can be found to implement a reference tracking controller. This paper presents a theoretical justification for the method and shows several examples, including a validation test on a real scale aircraft.",
          "authors": [
            "Sean Bowerfind",
            "Matthew R. Kirchner",
            "Gary Hewer"
          ],
          "published": "2026-02-13T18:11:33Z",
          "updated": "2026-02-13T18:11:33Z",
          "primary_category": "math.OC",
          "categories": [
            "math.OC",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13157v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13086v1",
          "title": "UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph",
          "summary": "Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.",
          "authors": [
            "Haichao Liu",
            "Yuanjiang Xue",
            "Yuheng Zhou",
            "Haoyuan Deng",
            "Yinan Liang",
            "Lihua Xie",
            "Ziwei Wang"
          ],
          "published": "2026-02-13T16:47:26Z",
          "updated": "2026-02-13T16:47:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13086v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13081v1",
          "title": "Agentic AI for Robot Control: Flexible but still Fragile",
          "summary": "Recent work leverages the capabilities and commonsense priors of generative models for robot control. In this paper, we present an agentic control system in which a reasoning-capable language model plans and executes tasks by selecting and invoking robot skills within an iterative planner and executor loop. We deploy the system on two physical robot platforms in two settings: (i) tabletop grasping, placement, and box insertion in indoor mobile manipulation (Mobipick) and (ii) autonomous agricultural navigation and sensing (Valdemar). Both settings involve uncertainty, partial observability, sensor noise, and ambiguous natural-language commands. The system exposes structured introspection of its planning and decision process, reacts to exogenous events via explicit event checks, and supports operator interventions that modify or redirect ongoing execution. Across both platforms, our proof-of-concept experiments reveal substantial fragility, including non-deterministic suboptimal behavior, instruction-following errors, and high sensitivity to prompt specification. At the same time, the architecture is flexible: transfer to a different robot and task domain largely required updating the system prompt (domain model, affordances, and action catalogue) and re-binding the same tool interface to the platform-specific skill API.",
          "authors": [
            "Oscar Lima",
            "Marc Vinci",
            "Martin Günther",
            "Marian Renz",
            "Alexander Sung",
            "Sebastian Stock",
            "Johannes Brust",
            "Lennart Niecksch",
            "Zongyao Yi",
            "Felix Igelbrink",
            "Benjamin Kisliuk",
            "Martin Atzmueller",
            "Joachim Hertzberg"
          ],
          "published": "2026-02-13T16:43:34Z",
          "updated": "2026-02-13T16:43:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13081v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13078v1",
          "title": "SENSE-STEP: Learning Sim-to-Real Locomotion for a Sensory-Enabled Soft Quadruped Robot",
          "summary": "Robust closed-loop locomotion remains challenging for soft quadruped robots due to high-dimensional dynamics, actuator hysteresis, and difficult-to-model contact interactions, while conventional proprioception provides limited information about ground contact. In this paper, we present a learning-based control framework for a pneumatically actuated soft quadruped equipped with tactile suction-cup feet, and we validate the approach experimentally on physical hardware. The control policy is trained in simulation through a staged learning process that starts from a reference gait and is progressively refined under randomized environmental conditions. The resulting controller maps proprioceptive and tactile feedback to coordinated pneumatic actuation and suction-cup commands, enabling closed-loop locomotion on flat and inclined surfaces. When deployed on the real robot, the closed-loop policy outperforms an open-loop baseline, increasing forward speed by 41% on a flat surface and by 91% on a 5-degree incline. Ablation studies further demonstrate the role of tactile force estimates and inertial feedback in stabilizing locomotion, with performance improvements of up to 56% compared to configurations without sensory feedback.",
          "authors": [
            "Storm de Kam",
            "Ebrahim Shahabi",
            "Cosimo Della Santina"
          ],
          "published": "2026-02-13T16:37:29Z",
          "updated": "2026-02-13T16:37:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13078v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13016v1",
          "title": "How Swarms Differ: Challenges in Collective Behaviour Comparison",
          "summary": "Collective behaviours often need to be expressed through numerical features, e.g., for classification or imitation learning. This problem is often addressed by proposing an ad-hoc feature set for a particular swarm behaviour context, usually without further consideration of the solution's resilience outside of the conceived context. Yet, the development of automatic methods to design swarm behaviours is dependent on the ability to measure quantitatively the similarity of swarm behaviours. Hence, we investigate the impact of feature sets for collective behaviours. We select swarm feature sets and similarity measures from prior swarm robotics works, which mainly considered a narrow behavioural context and assess their robustness. We demonstrate that the interplay of feature set and similarity measure makes some combinations more suitable to distinguish groups of similar behaviours. We also propose a self-organised map-based approach to identify regions of the feature space where behaviours cannot be easily distinguished.",
          "authors": [
            "André Fialho Jesus",
            "Jonas Kuckling"
          ],
          "published": "2026-02-13T15:23:28Z",
          "updated": "2026-02-13T15:23:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13016v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12978v1",
          "title": "Learning Native Continuation for Action Chunking Flow Policies",
          "summary": "Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.",
          "authors": [
            "Yufeng Liu",
            "Hang Yu",
            "Juntu Zhao",
            "Bocheng Li",
            "Di Zhang",
            "Mingzhu Li",
            "Wenxuan Wu",
            "Yingdong Hu",
            "Junyuan Xie",
            "Junliang Guo",
            "Dequan Wang",
            "Yang Gao"
          ],
          "published": "2026-02-13T14:56:06Z",
          "updated": "2026-02-13T14:56:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12978v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12971v1",
          "title": "INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval",
          "summary": "Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/",
          "authors": [
            "YukTungSamuel Fang",
            "Zhikang Shi",
            "Jiabin Qiu",
            "Zixuan Chen",
            "Jieqi Shi",
            "Hao Xu",
            "Jing Huo",
            "Yang Gao"
          ],
          "published": "2026-02-13T14:45:55Z",
          "updated": "2026-02-13T14:45:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12971v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15063v1",
          "title": "How Do We Research Human-Robot Interaction in the Age of Large Language Models? A Systematic Review",
          "summary": "Advances in large language models (LLMs) are profoundly reshaping the field of human-robot interaction (HRI). While prior work has highlighted the technical potential of LLMs, few studies have systematically examined their human-centered impact (e.g., human-oriented understanding, user modeling, and levels of autonomy), making it difficult to consolidate emerging challenges in LLM-driven HRI systems. Therefore, we conducted a systematic literature search following the PRISMA guideline, identifying 86 articles that met our inclusion criteria. Our findings reveal that: (1) LLMs are transforming the fundamentals of HRI by reshaping how robots sense context, generate socially grounded interactions, and maintain continuous alignment with human needs in embodied settings; and (2) current research is largely exploratory, with different studies focusing on different facets of LLM-driven HRI, resulting in wide-ranging choices of experimental setups, study methods, and evaluation metrics. Finally, we identify key design considerations and challenges, offering a coherent overview and guidelines for future research at the intersection of LLMs and HRI.",
          "authors": [
            "Yufeng Wang",
            "Yuan Xu",
            "Anastasia Nikolova",
            "Yuxuan Wang",
            "Jianyu Wang",
            "Chongyang Wang",
            "Xin Tong"
          ],
          "published": "2026-02-13T13:55:43Z",
          "updated": "2026-02-13T13:55:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15063v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12918v1",
          "title": "Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips",
          "summary": "Distinguishing the feel of smooth silk from coarse cotton is a trivial everyday task for humans. When exploring such fabrics, fingertip skin senses both spatio-temporal force patterns and texture-induced vibrations that are integrated to form a haptic representation of the explored material. It is challenging to reproduce this rich, dynamic perceptual capability in robots because tactile sensors typically cannot achieve both high spatial resolution and high temporal sampling rate. In this work, we present a system that can sense both types of haptic information, and we investigate how each type influences robotic tactile perception of fabrics. Our robotic hand's middle finger and thumb each feature a soft tactile sensor: one is the open-source Minsight sensor that uses an internal camera to measure fingertip deformation and force at 50 Hz, and the other is our new sensor Minsound that captures vibrations through an internal MEMS microphone with a bandwidth from 50 Hz to 15 kHz. Inspired by the movements humans make to evaluate fabrics, our robot actively encloses and rubs folded fabric samples between its two sensitive fingers. Our results test the influence of each sensing modality on overall classification performance, showing high utility for the audio-based sensor. Our transformer-based method achieves a maximum fabric classification accuracy of 97 % on a dataset of 20 common fabrics. Incorporating an external microphone away from Minsound increases our method's robustness in loud ambient noise conditions. To show that this audio-visual tactile sensing approach generalizes beyond the training data, we learn general representations of fabric stretchiness, thickness, and roughness.",
          "authors": [
            "Iris Andrussow",
            "Jans Solano",
            "Benjamin A. Richardson",
            "Georg Martius",
            "Katherine J. Kuchenbecker"
          ],
          "published": "2026-02-13T13:23:52Z",
          "updated": "2026-02-13T13:23:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12918v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15061v1",
          "title": "Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories",
          "summary": "The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.",
          "authors": [
            "Zihan Zhang",
            "Haohui Que",
            "Junhan Chang",
            "Xin Zhang",
            "Hao Wei",
            "Tong Zhu"
          ],
          "published": "2026-02-13T12:42:48Z",
          "updated": "2026-02-13T12:42:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15061v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15060v1",
          "title": "CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation",
          "summary": "Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.",
          "authors": [
            "Tengjie Zhu",
            "Guanyu Cai",
            "Yang Zhaohui",
            "Guanzhu Ren",
            "Haohui Xie",
            "ZiRui Wang",
            "Junsong Wu",
            "Jingbo Wang",
            "Xiaokang Yang",
            "Yao Mu",
            "Yichao Yan",
            "Yichao Yan"
          ],
          "published": "2026-02-13T12:03:13Z",
          "updated": "2026-02-13T12:03:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15060v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15060v2",
          "title": "CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation",
          "summary": "Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.",
          "authors": [
            "Tengjie Zhu",
            "Guanyu Cai",
            "Yang Zhaohui",
            "Guanzhu Ren",
            "Haohui Xie",
            "ZiRui Wang",
            "Junsong Wu",
            "Jingbo Wang",
            "Xiaokang Yang",
            "Yao Mu",
            "Yichao Yan"
          ],
          "published": "2026-02-13T12:03:13Z",
          "updated": "2026-02-20T09:51:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15060v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12838v1",
          "title": "SKYSURF: A Self-learning Framework for Persistent Surveillance using Cooperative Aerial Gliders",
          "summary": "The success of surveillance applications involving small unmanned aerial vehicles (UAVs) depends on how long the limited on-board power would persist. To cope with this challenge, alternative renewable sources of lift are sought. One promising solution is to extract energy from rising masses of buoyant air. This paper proposes a local-global behavioral management and decision-making approach for the autonomous deployment of soaring-capable UAVs. The cooperative UAVs are modeled as non-deterministic finite state-based rational agents. In addition to a mission planning module for assigning tasks and issuing dynamic navigation waypoints for a new path planning scheme, in which the concepts of visibility and prediction are applied to avoid the collisions. Moreover, a delayed learning and tuning strategy is employed optimize the gains of the path tracking controller. Rigorous comparative analyses carried out with three benchmarking baselines and 15 evolutionary algorithms highlight the adequacy of the proposed approach for maintaining the surveillance persistency (staying aloft for longer periods without landing) and maximizing the detection of targets (two times better than non-cooperative and semi-cooperative approaches) with less power consumption (almost 6% of battery consumed in six hours).",
          "authors": [
            "Houssem Eddine Mohamadi",
            "Nadjia Kara"
          ],
          "published": "2026-02-13T11:42:26Z",
          "updated": "2026-02-13T11:42:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12838v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12794v2",
          "title": "SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies",
          "summary": "The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.",
          "authors": [
            "Thies Oelerich",
            "Gerald Ebmer",
            "Christian Hartl-Nesic",
            "Andreas Kugi"
          ],
          "published": "2026-02-13T10:23:43Z",
          "updated": "2026-02-17T09:36:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12794v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12785v1",
          "title": "Media Framing Moderates Risk-Benefit Perceptions and Value Tradeoffs in Human-Robot Collaboration",
          "summary": "Public acceptance of industrial human-robot collaboration (HRC) is shaped by how risks and benefits are perceived by affected employees. Positive or negative media framing may shape and shift how individuals evaluate HRC. This study examines how message framing moderates the effects of perceived risks and perceived benefits on overall attributed value. In a pre-registered study, participants (N = 1150) were randomly assigned to read either a positively or negatively framed newspaper article in one of three industrial contexts (autonomy, employment, safety) about HRC in production. Subsequently, perceived risks, benefits, and value were measured using reliable and publicly available psychometric scales. Two multiple regressions (one per framing condition) tested for main and interaction effects. Framing influenced absolute evaluations of risk, benefits, and value. In both frames, risks and benefits significantly predicted attributed value. Under positive framing, only main effects were observed (risks: beta = -0.52; benefits: beta = 0.45). Under negative framing, both predictors had stronger main effects (risks: beta = -0.69; benefits: beta = 0.63) along with a significant negative interaction (beta = -0.32), indicating that higher perceived risk diminishes the positive effect of perceived benefits. Model fit was higher for the positive frame (R^2 = 0.715) than for the negative frame (R^2 = 0.583), indicating greater explained variance in value attributions. Framing shapes the absolute evaluation of HRC and how risks and benefits are cognitively integrated in trade-offs. Negative framing produces stronger but interdependent effects, whereas positive framing supports additive evaluations. These findings highlight the role of strategic communication in fostering acceptance of HRC and underscore the need to consider framing in future HRC research.",
          "authors": [
            "Philipp Brauner",
            "Felix Glawe",
            "Luisa Vervier",
            "Martina Ziefle"
          ],
          "published": "2026-02-13T10:09:21Z",
          "updated": "2026-02-13T10:09:21Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12785v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12734v1",
          "title": "Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models",
          "summary": "Imitation learning is a popular paradigm to teach robots new tasks, but collecting robot demonstrations through teleoperation or kinesthetic teaching is tedious and time-consuming. In contrast, directly demonstrating a task using our human embodiment is much easier and data is available in abundance, yet transfer to the robot can be non-trivial. In this work, we propose Real2Gen to train a manipulation policy from a single human demonstration. Real2Gen extracts required information from the demonstration and transfers it to a simulation environment, where a programmable expert agent can demonstrate the task arbitrarily many times, generating an unlimited amount of data to train a flow matching policy. We evaluate Real2Gen on human demonstrations from three different real-world tasks and compare it to a recent baseline. Real2Gen shows an average increase in the success rate of 26.6% and better generalization of the trained policy due to the abundance and diversity of training data. We further deploy our purely simulation-trained policy zero-shot in the real world. We make the data, code, and trained models publicly available at real2gen.cs.uni-freiburg.de.",
          "authors": [
            "Nick Heppert",
            "Minh Quang Nguyen",
            "Abhinav Valada"
          ],
          "published": "2026-02-13T09:04:18Z",
          "updated": "2026-02-13T09:04:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12734v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13358v1",
          "title": "Location as a service with a MEC architecture",
          "summary": "In recent years, automated driving has become viable, and advanced driver assistance systems (ADAS) are now part of modern cars. These systems require highly precise positioning. In this paper, a cooperative approach to localization is presented. The GPS information from several road users is collected in a Mobile Edge Computing cloud, and the characteristics of GNSS positioning are used to provide lane-precise positioning for all participants by applying probabilistic filters and HD maps.",
          "authors": [
            "Christopher Schahn",
            "Jorin Kouril",
            "Bernd Schaeufele",
            "Ilja Radusch"
          ],
          "published": "2026-02-13T09:03:12Z",
          "updated": "2026-02-13T09:03:12Z",
          "primary_category": "cs.NI",
          "categories": [
            "cs.NI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13358v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12724v1",
          "title": "TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions",
          "summary": "This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.",
          "authors": [
            "Wei Zhu",
            "Irfan Tito Kurniawan",
            "Ye Zhao",
            "Mistuhiro Hayashibe"
          ],
          "published": "2026-02-13T08:54:05Z",
          "updated": "2026-02-13T08:54:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12724v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12700v1",
          "title": "Constrained PSO Six-Parameter Fuzzy PID Tuning Method for Balanced Optimization of Depth Tracking Performance in Underwater Vehicles",
          "summary": "Depth control of underwater vehicles in engineering applications must simultaneously satisfy requirements for rapid tracking, low overshoot, and actuator constraints. Traditional fuzzy PID tuning often relies on empirical methods, making it difficult to achieve a stable and reproducible equilibrium solution between performance enhancement and control cost. This paper proposes a constrained particle swarm optimization (PSO) method for tuning six-parameter fuzzy PID controllers. By adjusting the benchmark PID parameters alongside the fuzzy controller's input quantization factor and output proportional gain, it achieves synergistic optimization of the overall tuning strength and dynamic response characteristics of the fuzzy PID system. To ensure engineering feasibility of the optimization results, a time-weighted absolute error integral, adjustment time, relative overshoot control energy, and saturation occupancy rate are introduced. Control energy constraints are applied to construct a constraint-driven comprehensive evaluation system, suppressing pseudo-improvements achieved solely by increasing control inputs. Simulation results demonstrate that, while maintaining consistent control energy and saturation levels, the proposed method significantly enhances deep tracking performance: the time-weighted absolute error integral decreases from 0.2631 to 0.1473, the settling time shortens from 2.301 s to 1.613 s, and the relative overshoot reduces from 0.1494 to 0.01839. Control energy varied from 7980 to 7935, satisfying the energy constraint, while saturation occupancy decreased from 0.004 to 0.003. These results validate the effectiveness and engineering significance of the proposed constrained six-parameter joint tuning strategy for depth control in underwater vehicle navigation scenarios.",
          "authors": [
            "Yanxi Ding",
            "Tingyue Jia"
          ],
          "published": "2026-02-13T08:11:18Z",
          "updated": "2026-02-13T08:11:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12700v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12691v1",
          "title": "ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training",
          "summary": "We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.",
          "authors": [
            "Rushuai Yang",
            "Hecheng Wang",
            "Chiming Liu",
            "Xiaohan Yan",
            "Yunlong Wang",
            "Xuan Du",
            "Shuoyu Yue",
            "Yongcheng Liu",
            "Chuheng Zhang",
            "Lizhe Qi",
            "Yi Chen",
            "Wei Shan",
            "Maoqing Yao"
          ],
          "published": "2026-02-13T07:46:37Z",
          "updated": "2026-02-13T07:46:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12691v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12691v2",
          "title": "ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training",
          "summary": "We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.",
          "authors": [
            "Rushuai Yang",
            "Hecheng Wang",
            "Chiming Liu",
            "Xiaohan Yan",
            "Yunlong Wang",
            "Xuan Du",
            "Shuoyu Yue",
            "Yongcheng Liu",
            "Chuheng Zhang",
            "Lizhe Qi",
            "Yi Chen",
            "Wei Shan",
            "Maoqing Yao"
          ],
          "published": "2026-02-13T07:46:37Z",
          "updated": "2026-02-23T08:56:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12691v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12686v1",
          "title": "SignScene: Visual Sign Grounding for Mapless Navigation",
          "summary": "Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.",
          "authors": [
            "Nicky Zimmerman",
            "Joel Loo",
            "Benjamin Koh",
            "Zishuo Wang",
            "David Hsu"
          ],
          "published": "2026-02-13T07:42:48Z",
          "updated": "2026-02-13T07:42:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12686v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12684v1",
          "title": "Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution",
          "summary": "In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io",
          "authors": [
            "Rui Cai",
            "Jun Guo",
            "Xinze He",
            "Piaopiao Jin",
            "Jie Li",
            "Bingxuan Lin",
            "Futeng Liu",
            "Wei Liu",
            "Fei Ma",
            "Kun Ma",
            "Feng Qiu",
            "Heng Qu",
            "Yifei Su",
            "Qiao Sun",
            "Dong Wang",
            "Donghao Wang",
            "Yunhong Wang",
            "Rujie Wu",
            "Diyun Xiang",
            "Yu Yang",
            "Hangjun Ye",
            "Yuan Zhang",
            "Quanyun Zhou"
          ],
          "published": "2026-02-13T07:30:43Z",
          "updated": "2026-02-13T07:30:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12684v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12656v1",
          "title": "PMG: Parameterized Motion Generator for Human-like Locomotion Control",
          "summary": "Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.",
          "authors": [
            "Chenxi Han",
            "Yuheng Min",
            "Zihao Huang",
            "Ao Hong",
            "Hang Liu",
            "Yi Cheng",
            "Houde Liu"
          ],
          "published": "2026-02-13T06:38:04Z",
          "updated": "2026-02-13T06:38:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12656v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12656v2",
          "title": "PMG: Parameterized Motion Generator for Human-like Locomotion Control",
          "summary": "Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with high-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control. Website: https://pmg-icra26.github.io/",
          "authors": [
            "Chenxi Han",
            "Yuheng Min",
            "Zihao Huang",
            "Ao Hong",
            "Hang Liu",
            "Yi Cheng",
            "Houde Liu"
          ],
          "published": "2026-02-13T06:38:04Z",
          "updated": "2026-02-24T14:34:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12656v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12636v1",
          "title": "Dual-Granularity Contrastive Reward via Generated Episodic Guidance for Efficient Embodied RL",
          "summary": "Designing suitable rewards poses a significant challenge in reinforcement learning (RL), especially for embodied manipulation. Trajectory success rewards are suitable for human judges or model fitting, but the sparsity severely limits RL sample efficiency. While recent methods have effectively improved RL via dense rewards, they rely heavily on high-quality human-annotated data or abundant expert supervision. To tackle these issues, this paper proposes Dual-granularity contrastive reward via generated Episodic Guidance (DEG), a novel framework to seek sample-efficient dense rewards without requiring human annotations or extensive supervision. Leveraging the prior knowledge of large video generation models, DEG only needs a small number of expert videos for domain adaptation to generate dedicated task guidance for each RL episode. Then, the proposed dual-granularity reward that balances coarse-grained exploration and fine-grained matching, will guide the agent to efficiently approximate the generated guidance video sequentially in the contrastive self-supervised latent space, and finally complete the target task. Extensive experiments on 18 diverse tasks across both simulation and real-world settings show that DEG can not only serve as an efficient exploration stimulus to help the agent quickly discover sparse success rewards, but also guide effective RL and stable policy convergence independently.",
          "authors": [
            "Xin Liu",
            "Yixuan Li",
            "Yuhui Chen",
            "Yuxing Qin",
            "Haoran Li",
            "Dongbin Zhao"
          ],
          "published": "2026-02-13T05:41:55Z",
          "updated": "2026-02-13T05:41:55Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12636v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12633v1",
          "title": "Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning",
          "summary": "Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.",
          "authors": [
            "Tianyi Xiang",
            "Jiahang Cao",
            "Sikai Guo",
            "Guoyang Zhao",
            "Andrew F. Luo",
            "Jun Ma"
          ],
          "published": "2026-02-13T05:24:58Z",
          "updated": "2026-02-13T05:24:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12633v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12628v2",
          "title": "Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models",
          "summary": "Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \\underline{\\textit{RL}}-based sim-real \\underline{\\textit{Co}}-training \\modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.",
          "authors": [
            "Liangzhi Shi",
            "Shuaihang Chen",
            "Feng Gao",
            "Yinuo Chen",
            "Kang Chen",
            "Tonghe Zhang",
            "Hongzhi Zang",
            "Weinan Zhang",
            "Chao Yu",
            "Yu Wang"
          ],
          "published": "2026-02-13T05:15:50Z",
          "updated": "2026-02-16T05:44:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12628v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12616v1",
          "title": "When Environments Shift: Safe Planning with Generative Priors and Robust Conformal Prediction",
          "summary": "Autonomous systems operate in environments that may change over time. An example is the control of a self-driving vehicle among pedestrians and human-controlled vehicles whose behavior may change based on factors such as traffic density, road visibility, and social norms. Therefore, the environment encountered during deployment rarely mirrors the environment and data encountered during training -- a phenomenon known as distribution shift -- which can undermine the safety of autonomous systems. Conformal prediction (CP) has recently been used along with data from the training environment to provide prediction regions that capture the behavior of the environment with a desired probability. When embedded within a model predictive controller (MPC), one can provide probabilistic safety guarantees, but only when the deployment and training environments coincide. Once a distribution shift occurs, these guarantees collapse. We propose a planning framework that is robust under distribution shifts by: (i) assuming that the underlying data distribution of the environment is parameterized by a nuisance parameter, i.e., an observable, interpretable quantity such as traffic density, (ii) training a conditional diffusion model that captures distribution shifts as a function of the nuisance parameter, (iii) observing the nuisance parameter online and generating cheap, synthetic data from the diffusion model for the observed nuisance parameter, and (iv) designing an MPC that embeds CP regions constructed from such synthetic data. Importantly, we account for discrepancies between the underlying data distribution and the diffusion model by using robust CP. Thus, the plans computed using robust CP enjoy probabilistic safety guarantees, in contrast with plans obtained from a single, static set of training data. We empirically demonstrate safety under diverse distribution shifts in the ORCA simulator.",
          "authors": [
            "Kaizer Rahaman",
            "Jyotirmoy V. Deshmukh",
            "Ashish R. Hota",
            "Lars Lindemann"
          ],
          "published": "2026-02-13T04:48:03Z",
          "updated": "2026-02-13T04:48:03Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12616v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12597v1",
          "title": "PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People",
          "summary": "This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.",
          "authors": [
            "Mahdi Haghighat Joo",
            "Maryam Karimi Jafari",
            "Alireza Taheri"
          ],
          "published": "2026-02-13T04:17:55Z",
          "updated": "2026-02-13T04:17:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12597v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12584v1",
          "title": "Hemispherical Angular Power Mapping of Installed mmWave Radar Modules Under Realistic Deployment Constraints",
          "summary": "Characterizing the angular radiation behavior of installed millimeter-wave (mmWave) radar modules is increasingly important in practical sensing platforms, where packaging, mounting hardware, and nearby structures can significantly alter the effective emission profile. However, once a device is embedded in its host environment, conventional chamber- and turntable-based antenna measurements are often impractical. This paper presents a hemispherical angular received-power mapping methodology for in-situ EM validation of installed mmWave modules under realistic deployment constraints. The approach samples the accessible half-space around a stationary device-under-test by placing a calibrated receiving probe at prescribed (phi, theta, r) locations using geometry-consistent positioning and quasi-static acquisition. Amplitude-only received-power is recorded using standard RF instrumentation to generate hemispherical angular power maps that capture installation-dependent radiation characteristics. Proof-of-concept measurements on a 60-GHz radar module demonstrate repeatable hemi-spherical mapping with angular trends in good agreement with full-wave simulation, supporting practical on-site characterization of embedded mmWave transmitters.",
          "authors": [
            "Maaz Qureshi",
            "Mohammad Omid Bagheri",
            "William Melek",
            "George Shaker"
          ],
          "published": "2026-02-13T03:54:34Z",
          "updated": "2026-02-13T03:54:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12584v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15901v1",
          "title": "Coverage Path Planning for Autonomous Sailboats in Inhomogeneous and Time-Varying Oceans: A Spatiotemporal Optimization Approach",
          "summary": "Autonomous sailboats are well suited for long-duration ocean observation due to their wind-driven endurance. However, their performance is highly anisotropic and strongly influenced by inhomogeneous and time-varying wind and current fields, limiting the effectiveness of existing coverage methods such as boustrophedon sweeping. Planning under these environmental and maneuvering constraints remains underexplored. This paper presents a spatiotemporal coverage path planning framework tailored to autonomous sailboats, combining (1) topology-based morphological constraints in the spatial domain to promote compact and continuous coverage, and (2) forecast-aware look-ahead planning in the temporal domain to anticipate environmental evolution and enable foresighted decision-making. Simulations conducted under stochastic inhomogeneous and time-varying ocean environments, including scenarios with partial directional accessibility, demonstrate that the proposed method generates efficient and feasible coverage paths where traditional strategies often fail. To the best of our knowledge, this study provides the first dedicated solution to the coverage path planning problem for autonomous sailboats operating in inhomogeneous and time-varying ocean environments, establishing a foundation for future cooperative multi-sailboat coverage.",
          "authors": [
            "Yang An",
            "Zhikang Ge",
            "Taiyu Zhang",
            "Jean-Baptiste R. G. Souppez",
            "Gaofei Xu",
            "Zhengru Ren"
          ],
          "published": "2026-02-13T03:37:24Z",
          "updated": "2026-02-13T03:37:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15901v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15900v1",
          "title": "Adaptive Illumination Control for Robot Perception",
          "summary": "Robot perception under low light or high dynamic range is usually improved downstream - via more robust feature extraction, image enhancement, or closed-loop exposure control. However, all of these approaches are limited by the image captured these conditions. An alternate approach is to utilize a programmable onboard light that adds to ambient illumination and improves captured images. However, it is not straightforward to predict its impact on image formation. Illumination interacts nonlinearly with depth, surface reflectance, and scene geometry. It can both reveal structure and induce failure modes such as specular highlights and saturation. We introduce Lightning, a closed-loop illumination-control framework for visual SLAM that combines relighting, offline optimization, and imitation learning. This is performed in three stages. First, we train a Co-Located Illumination Decomposition (CLID) relighting model that decomposes a robot observation into an ambient component and a light-contribution field. CLID enables physically consistent synthesis of the same scene under alternative light intensities and thereby creates dense multi-intensity training data without requiring us to repeatedly re-run trajectories. Second, using these synthesized candidates, we formulate an offline Optimal Intensity Schedule (OIS) problem that selects illumination levels over a sequence trading off SLAM-relevant image utility against power consumption and temporal smoothness. Third, we distill this ideal solution into a real-time controller through behavior cloning, producing an Illumination Control Policy (ILC) that generalizes beyond the initial training distribution and runs online on a mobile robot to command discrete light-intensity levels. Across our evaluation, Lightning substantially improves SLAM trajectory robustness while reducing unnecessary illumination power.",
          "authors": [
            "Yash Turkar",
            "Shekoufeh Sadeghi",
            "Karthik Dantu"
          ],
          "published": "2026-02-13T03:34:01Z",
          "updated": "2026-02-13T03:34:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15900v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12549v1",
          "title": "Eva-Tracker: ESDF-update-free, Visibility-aware Planning with Target Reacquisition for Robust Aerial Tracking",
          "summary": "The Euclidean Signed Distance Field (ESDF) is widely used in visibility evaluation to prevent occlusions and collisions during tracking. However, frequent ESDF updates introduce considerable computational overhead. To address this issue, we propose Eva-Tracker, a visibility-aware trajectory planning framework for aerial tracking that eliminates ESDF updates and incorporates a recovery-capable path generation method for target reacquisition. First, we design a target trajectory prediction method and a visibility-aware initial path generation algorithm that maintain an appropriate observation distance, avoid occlusions, and enable rapid replanning to reacquire the target when it is lost. Then, we propose the Field of View ESDF (FoV-ESDF), a precomputed ESDF tailored to the tracker's field of view, enabling rapid visibility evaluation without requiring updates. Finally, we optimize the trajectory using differentiable FoV-ESDF-based objectives to ensure continuous visibility throughout the tracking process. Extensive simulations and real-world experiments demonstrate that our approach delivers more robust tracking results with lower computational effort than existing state-of-the-art methods. The source code is available at https://github.com/Yue-0/Eva-Tracker.",
          "authors": [
            "Yue Lin",
            "Yang Liu",
            "Dong Wang",
            "Huchuan Lu"
          ],
          "published": "2026-02-13T02:56:34Z",
          "updated": "2026-02-13T02:56:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12549v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12540v1",
          "title": "Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting",
          "summary": "Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \\textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \\textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \\textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.",
          "authors": [
            "Haoran Zhu",
            "Anna Choromanska"
          ],
          "published": "2026-02-13T02:42:21Z",
          "updated": "2026-02-13T02:42:21Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12540v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12532v1",
          "title": "CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning",
          "summary": "Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.",
          "authors": [
            "Yike Zhang",
            "Yaonan Wang",
            "Xinxin Sun",
            "Kaizhen Huang",
            "Zhiyuan Xu",
            "Junjie Ji",
            "Zhengping Che",
            "Jian Tang",
            "Jingtao Sun"
          ],
          "published": "2026-02-13T02:28:21Z",
          "updated": "2026-02-13T02:28:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12532v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12508v1",
          "title": "Monocular Reconstruction of Neural Tactile Fields",
          "summary": "Robots operating in the real world must plan through environments that deform, yield, and reconfigure under contact, requiring interaction-aware 3D representations that extend beyond static geometric occupancy. To address this, we introduce neural tactile fields, a novel 3D representation that maps spatial locations to the expected tactile response upon contact. Our model predicts these neural tactile fields from a single monocular RGB image -- the first method to do so. When integrated with off-the-shelf path planners, neural tactile fields enable robots to generate paths that avoid high-resistance objects while deliberately routing through low-resistance regions (e.g. foliage), rather than treating all occupied space as equally impassable. Empirically, our learning framework improves volumetric 3D reconstruction by $85.8\\%$ and surface reconstruction by $26.7\\%$ compared to state-of-the-art monocular 3D reconstruction methods (LRM and Direct3D).",
          "authors": [
            "Pavan Mantripragada",
            "Siddhanth Deshmukh",
            "Eadom Dessalene",
            "Manas Desai",
            "Yiannis Aloimonos"
          ],
          "published": "2026-02-13T01:25:19Z",
          "updated": "2026-02-13T01:25:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12508v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12492v1",
          "title": "Composable Model-Free RL for Navigation with Input-Affine Systems",
          "summary": "As autonomous robots move into complex, dynamic real-world environments, they must learn to navigate safely in real time, yet anticipating all possible behaviors is infeasible. We propose a composable, model-free reinforcement learning method that learns a value function and an optimal policy for each individual environment element (e.g., goal or obstacle) and composes them online to achieve goal reaching and collision avoidance. Assuming unknown nonlinear dynamics that evolve in continuous time and are input-affine, we derive a continuous-time Hamilton-Jacobi-Bellman (HJB) equation for the value function and show that the corresponding advantage function is quadratic in the action and optimal policy. Based on this structure, we introduce a model-free actor-critic algorithm that learns policies and value functions for static or moving obstacles using gradient descent. We then compose multiple reach/avoid models via a quadratically constrained quadratic program (QCQP), yielding formal obstacle-avoidance guarantees in terms of value-function level sets, providing a model-free alternative to CLF/CBF-based controllers. Simulations demonstrate improved performance over a PPO baseline applied to a discrete-time approximation.",
          "authors": [
            "Xinhuan Sang",
            "Abdelrahman Abdelgawad",
            "Roberto Tron"
          ],
          "published": "2026-02-13T00:19:35Z",
          "updated": "2026-02-13T00:19:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12492v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12487v1",
          "title": "Gradient-Enhanced Partitioned Gaussian Processes for Real-Time Quadrotor Dynamics Modeling",
          "summary": "We present a quadrotor dynamics Gaussian Process (GP) with gradient information that achieves real-time inference via state-space partitioning and approximation, and that includes aerodynamic effects using data from mid-fidelity potential flow simulations. While traditional GP-based approaches provide reliable Bayesian predictions with uncertainty quantification, they are computationally expensive and thus unsuitable for real-time simulations. To address this challenge, we integrate gradient information to improve accuracy and introduce a novel partitioning and approximation strategy to reduce online computational cost. In particular, for the latter, we associate a local GP with each non-overlapping region; by splitting the training data into local near and far subsets, and by using Schur complements, we show that a large part of the matrix inversions required for inference can be performed offline, enabling real-time inference at frequencies above 30 Hz on standard desktop hardware. To generate a training dataset that captures aerodynamic effects, such as rotor-rotor interactions and apparent wind direction, we use the CHARM code, which is a mid-fidelity aerodynamic solver. It is applied to the SUI Endurance quadrotor to predict force and torque, along with noise at three specified locations. The derivative information is obtained via finite differences. Experimental results demonstrate that the proposed partitioned GP with gradient conditioning achieves higher accuracy than standard partitioned GPs without gradient information, while greatly reducing computational time. This framework provides an efficient foundation for real-time aerodynamic prediction and control algorithms in complex and unsteady environments.",
          "authors": [
            "Xinhuan Sang",
            "Adam Rozman",
            "Sheryl Grace",
            "Roberto Tron"
          ],
          "published": "2026-02-13T00:00:51Z",
          "updated": "2026-02-13T00:00:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12487v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12421v1",
          "title": "An Autonomous, End-to-End, Convex-Based Framework for Close-Range Rendezvous Trajectory Design and Guidance with Hardware Testbed Validation",
          "summary": "Autonomous satellite servicing missions must execute close-range rendezvous under stringent safety and operational constraints while remaining computationally tractable for onboard use and robust to uncertainty in sensing, actuation, and dynamics. This paper presents CORTEX (Convex Optimization for Rendezvous Trajectory Execution), an autonomous, perception-enabled, real-time trajectory design and guidance framework for close-range rendezvous. CORTEX integrates a deep-learning perception pipeline with convex-optimisation-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to recover from large deviations caused by sensor faults and engine failures. CORTEX is validated in high-fidelity software simulation and hardware-in-the-loop experiments. The software pipeline (Basilisk) models high-fidelity relative dynamics, realistic thruster execution, perception, and attitude control. Hardware testing uses (i) an optical navigation testbed to assess perception-to-estimation performance and (ii) a planar air-bearing testbed to evaluate the end-to-end guidance loop under representative actuation and subsystem effects. A Monte-Carlo campaign in simulation includes initial-state uncertainty, thrust-magnitude errors, and missed-thrust events; under the strongest case investigated, CORTEX achieves terminal docking errors of $36.85 \\pm 44.46$ mm in relative position and $1.25 \\pm 2.26$ mm/s in relative velocity. On the planar air-bearing testbed, 18 cases are executed (10 nominal; 8 off-nominal requiring recomputation and/or abort due to simulated engine failure and sensor malfunctions), yielding terminal errors of $8.09 \\pm 5.29$ mm in position and $2.23 \\pm 1.72$ mm/s in velocity.",
          "authors": [
            "Minduli C. Wijayatunga",
            "Julian Guinane",
            "Nathan D. Wallace",
            "Xiaofeng Wu"
          ],
          "published": "2026-02-12T21:23:21Z",
          "updated": "2026-02-12T21:23:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12421v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13347v1",
          "title": "Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots",
          "summary": "Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.",
          "authors": [
            "Lijun Zhang",
            "Nikhil Chacko",
            "Petter Nilsson",
            "Ruinian Xu",
            "Shantanu Thakar",
            "Bai Lou",
            "Harpreet Sawhney",
            "Zhebin Zhang",
            "Mudit Agrawal",
            "Bhavana Chandrashekhar",
            "Aaron Parness"
          ],
          "published": "2026-02-12T21:22:31Z",
          "updated": "2026-02-12T21:22:31Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13347v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12416v1",
          "title": "Control Barrier Functions with Audio Risk Awareness for Robot Safe Navigation on Construction Sites",
          "summary": "Construction automation increasingly requires autonomous mobile robots, yet robust autonomy remains challenging on construction sites. These environments are dynamic and often visually occluded, which complicates perception and navigation. In this context, valuable information from audio sources remains underutilized in most autonomy stacks. This work presents a control barrier function (CBF)-based safety filter that provides safety guarantees for obstacle avoidance while adapting safety margins during navigation using an audio-derived risk cue. The proposed framework augments the CBF with a lightweight, real-time jackhammer detector based on signal envelope and periodicity. Its output serves as an exogenous risk that is directly enforced in the controller by modulating the barrier function. The approach is evaluated in simulation with two CBF formulations (circular and goal-aligned elliptical) with a unicycle robot navigating a cluttered construction environment. Results show that the CBF safety filter eliminates safety violations across all trials while reaching the target in 40.2% (circular) vs. 76.5% (elliptical), as the elliptical formulation better avoids deadlock. This integration of audio perception into a CBF-based controller demonstrates a pathway toward richer multimodal safety reasoning in autonomous robots for safety-critical and dynamic environments.",
          "authors": [
            "Johannes Mootz",
            "Reza Akhavian"
          ],
          "published": "2026-02-12T21:15:09Z",
          "updated": "2026-02-12T21:15:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12416v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12407v1",
          "title": "MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery",
          "summary": "Background: Robot-assisted minimally invasive surgery (RMIS) research increasingly relies on multimodal data, yet access to proprietary robot telemetry remains a major barrier. We introduce MiDAS, an open-source, platform-agnostic system enabling time-synchronized, non-invasive multimodal data acquisition across surgical robotic platforms. Methods: MiDAS integrates electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video capturing without requiring proprietary robot interfaces. We validated MiDAS on the open-source Raven-II and the clinical da Vinci Xi by collecting multimodal datasets of peg transfer and hernia repair suturing tasks performed by surgical residents. Correlation analysis and downstream gesture recognition experiments were conducted. Results: External hand and foot sensing closely approximated internal robot kinematics and non-invasive motion signals achieved gesture recognition performance comparable to proprietary telemetry. Conclusion: MiDAS enables reproducible multimodal RMIS data collection and is released with annotated datasets, including the first multimodal dataset capturing hernia repair suturing on high-fidelity simulation models.",
          "authors": [
            "Keshara Weerasinghe",
            "Seyed Hamid Reza Roodabeh",
            "Andrew Hawkins",
            "Zhaomeng Zhang",
            "Zachary Schrader",
            "Homa Alemzadeh"
          ],
          "published": "2026-02-12T20:56:15Z",
          "updated": "2026-02-12T20:56:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12407v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12405v1",
          "title": "Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning",
          "summary": "Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor",
          "authors": [
            "Carl Qi",
            "Xiaojie Wang",
            "Silong Yong",
            "Stephen Sheng",
            "Huitan Mao",
            "Sriram Srinivasan",
            "Manikantan Nambi",
            "Amy Zhang",
            "Yesh Dattatreya"
          ],
          "published": "2026-02-12T20:55:36Z",
          "updated": "2026-02-12T20:55:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12405v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12385v1",
          "title": "Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling",
          "summary": "High-performance autonomous mobile robots endure significant mechanical stress during in-the-wild operations, e.g., driving at high speeds or over rugged terrain. Although these platforms are engineered to withstand such conditions, mechanical degradation is inevitable. Structural damage manifests as consistent and notable changes in kinodynamic behavior compared to a healthy vehicle. Given the heterogeneous nature of structural failures, quantifying various damages to inform kinodynamics is challenging. We posit that natural language can describe and thus capture this variety of damages. Therefore, we propose Zero-shot Language Informed Kinodynamics (ZLIK), which employs self-supervised learning to ground semantic information of damage descriptions in kinodynamic behaviors to learn a forward kinodynamics model in a data-driven manner. Using the high-fidelity soft-body physics simulator BeamNG.tech, we collect data from a variety of structurally compromised vehicles. Our learned model achieves zero-shot adaptation to different damages with up to 81% reduction in kinodynamics error and generalizes across the sim-to-real and full-to-1/10$^{\\text{th}}$ scale gaps.",
          "authors": [
            "Anuj Pokhrel",
            "Aniket Datar",
            "Mohammad Nazeri",
            "Francesco Cancelliere",
            "Xuesu Xiao"
          ],
          "published": "2026-02-12T20:28:45Z",
          "updated": "2026-02-12T20:28:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12385v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12360v1",
          "title": "Predicting Dynamic Map States from Limited Field-of-View Sensor Data",
          "summary": "When autonomous systems are deployed in real-world scenarios, sensors are often subject to limited field-of-view (FOV) constraints, either naturally through system design, or through unexpected occlusions or sensor failures. In conditions where a large FOV is unavailable, it is important to be able to infer information about the environment and predict the state of nearby surroundings based on available data to maintain safe and accurate operation. In this work, we explore the effectiveness of deep learning for dynamic map state prediction based on limited FOV time series data. We show that by representing dynamic sensor data in a simple single-image format that captures both spatial and temporal information, we can effectively use a wide variety of existing image-to-image learning models to predict map states with high accuracy in a diverse set of sensing scenarios.",
          "authors": [
            "Knut Peterson",
            "David Han"
          ],
          "published": "2026-02-12T19:36:49Z",
          "updated": "2026-02-12T19:36:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12360v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12351v1",
          "title": "LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation",
          "summary": "This paper develops LongNav-R1, an end-to-end multi-turn reinforcement learning (RL) framework designed to optimize Visual-Language-Action (VLA) models for long-horizon navigation. Unlike existing single-turn paradigm, LongNav-R1 reformulates the navigation decision process as a continuous multi-turn conversation between the VLA policy and the embodied environment. This multi-turn RL framework offers two distinct advantages: i) it enables the agent to reason about the causal effects of historical interactions and sequential future outcomes; and ii) it allows the model to learn directly from online interactions, fostering diverse trajectory generation and avoiding the behavioral rigidity often imposed by human demonstrations. Furthermore, we introduce Horizon-Adaptive Policy Optimization. This mechanism explicitly accounts for varying horizon lengths during advantage estimation, facilitating accurate temporal credit assignment over extended sequences. Consequently, the agent develops diverse navigation behaviors and resists collapse during long-horizon tasks. Experiments on object navigation benchmarks validate the framework's efficacy: With 4,000 rollout trajectories, LongNav-R1 boosts the Qwen3-VL-2B success rate from 64.3% to 73.0%. These results demonstrate superior sample efficiency and significantly outperform state-of-the-art methods. The model's generalizability and robustness are further validated by its zero-shot performance in long-horizon real-world navigation settings. All source code will be open-sourced upon publication.",
          "authors": [
            "Yue Hu",
            "Avery Xi",
            "Qixin Xiao",
            "Seth Isaacson",
            "Henry X. Liu",
            "Ram Vasudevan",
            "Maani Ghaffari"
          ],
          "published": "2026-02-12T19:22:52Z",
          "updated": "2026-02-12T19:22:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12351v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12346v1",
          "title": "Schur-MI: Fast Mutual Information for Robotic Information Gathering",
          "summary": "Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning. This letter presents Schur-MI, a Gaussian process (GP) MI formulation that (i) leverages the iterative structure of RIG to precompute and reuse expensive intermediate quantities across planning steps, and (ii) uses a Schur-complement factorization to avoid large determinant computations. Together, these methods reduce the per-evaluation cost of MI from $\\mathcal{O}(|\\mathcal{V}|^3)$ to $\\mathcal{O}(|\\mathcal{A}|^3)$, where $\\mathcal{V}$ and $\\mathcal{A}$ denote the candidate and selected sensing locations, respectively. Experiments on real-world bathymetry datasets show that Schur-MI achieves up to a $12.7\\times$ speedup over the standard MI formulation. Field trials with an autonomous surface vehicle (ASV) performing adaptive IPP further validate its practicality. By making MI computation tractable for online planning, Schur-MI helps bridge the gap between information-theoretic objectives and real-time robotic exploration.",
          "authors": [
            "Kalvik Jakkala",
            "Jason O'Kane",
            "Srinivas Akella"
          ],
          "published": "2026-02-12T19:08:26Z",
          "updated": "2026-02-12T19:08:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12346v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12281v2",
          "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
          "summary": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.\" We first characterize the test-time scaling laws for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce CoVer-VLA, a hierarchical test-time verification pipeline using the trained verifier. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses the verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer-VLA achieves 14% gains in task progress and 9% in success rate.",
          "authors": [
            "Jacky Kwok",
            "Xilun Zhang",
            "Mengdi Xu",
            "Yuejiang Liu",
            "Azalia Mirhoseini",
            "Chelsea Finn",
            "Marco Pavone"
          ],
          "published": "2026-02-12T18:59:59Z",
          "updated": "2026-02-18T03:42:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12281v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12322v1",
          "title": "ForeAct: Steering Your VLA with Efficient Visual Foresight Planning",
          "summary": "Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).",
          "authors": [
            "Zhuoyang Zhang",
            "Shang Yang",
            "Qinghao Hu",
            "Luke J. Huang",
            "James Hou",
            "Yufei Sun",
            "Yao Lu",
            "Song Han"
          ],
          "published": "2026-02-12T18:56:27Z",
          "updated": "2026-02-12T18:56:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12322v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12246v1",
          "title": "6G Empowering Future Robotics: A Vision for Next-Generation Autonomous Systems",
          "summary": "The convergence of robotics and next-generation communication is a critical driver of technological advancement. As the world transitions from 5G to 6G, the foundational capabilities of wireless networks are evolving to support increasingly complex and autonomous robotic systems. This paper examines the transformative impact of 6G on enhancing key robotics functionalities. It provides a systematic mapping of IMT-2030 key performance indicators to robotic functional blocks including sensing, perception, cognition, actuation and self-learning. Building upon this mapping, we propose a high-level architectural framework integrating robotic, intelligent, and network service planes, underscoring the need for a holistic approach. As an example use case, we present a real-time, dynamic safety framework enabled by IMT-2030 capabilities for safe and efficient human-robot collaboration in shared spaces.",
          "authors": [
            "Mona Ghassemian",
            "Andrés Meseguer Valenzuela",
            "Ana Garcia Armada",
            "Dejan Vukobratovic",
            "Periklis Chatzimisios",
            "Kaspar Althoefer",
            "Ranga Rao Venkatesha Prasad"
          ],
          "published": "2026-02-12T18:31:24Z",
          "updated": "2026-02-12T18:31:24Z",
          "primary_category": "cs.NI",
          "categories": [
            "cs.NI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12246v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12244v1",
          "title": "Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks",
          "summary": "Open world language conditioned task planning is crucial for robots operating in large-scale household environments. While many recent works attempt to address this problem using Large Language Models (LLMs) via prompting or training, a key challenge remains scalability. Performance often degrades rapidly with increasing environment size, plan length, instruction ambiguity, and constraint complexity. In this work, we propose Any House Any Task (AHAT), a household task planner optimized for long-horizon planning in large environments given ambiguous human instructions. At its core, AHAT utilizes an LLM trained to map task instructions and textual scene graphs into grounded subgoals defined in the Planning Domain Definition Language (PDDL). These subgoals are subsequently solved to generate feasible and optimal long-horizon plans through explicit symbolic reasoning. To enhance the model's ability to decompose complex and ambiguous intentions, we introduce TGPO, a novel reinforcement learning algorithm that integrates external correction of intermediate reasoning traces into Group Relative Policy Optimization (GRPO). Experiments demonstrate that AHAT achieves significant performance gains over state-of-the-art prompting, planning, and learning methods, particularly in human-style household tasks characterized by brief instructions but requiring complex execution plans.",
          "authors": [
            "Zhihong Liu",
            "Yang Li",
            "Rengming Huang",
            "Cewu Lu",
            "Panpan Cai"
          ],
          "published": "2026-02-12T18:28:28Z",
          "updated": "2026-02-12T18:28:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12244v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12215v1",
          "title": "LDA-1B: Scaling Latent Dynamics Action Model via Universal Embodied Data Ingestion",
          "summary": "Recent robot foundation models largely rely on large-scale behavior cloning, which imitates expert actions but discards transferable dynamics knowledge embedded in heterogeneous embodied data. While the Unified World Model (UWM) formulation has the potential to leverage such diverse data, existing instantiations struggle to scale to foundation-level due to coarse data usage and fragmented datasets. We introduce LDA-1B, a robot foundation model that scales through universal embodied data ingestion by jointly learning dynamics, policy, and visual forecasting, assigning distinct roles to data of varying quality. To support this regime at scale, we assemble and standardize EI-30k, an embodied interaction dataset comprising over 30k hours of human and robot trajectories in a unified format. Scalable dynamics learning over such heterogeneous data is enabled by prediction in a structured DINO latent space, which avoids redundant pixel-space appearance modeling. Complementing this representation, LDA-1B employs a multi-modal diffusion transformer to handle asynchronous vision and action streams, enabling stable training at the 1B-parameter scale. Experiments in simulation and the real world show LDA-1B outperforms prior methods (e.g., $π_{0.5}$) by up to 21\\%, 48\\%, and 23\\% on contact-rich, dexterous, and long-horizon tasks, respectively. Notably, LDA-1B enables data-efficient fine-tuning, gaining 10\\% by leveraging 30\\% low-quality trajectories typically harmful and discarded.",
          "authors": [
            "Jiangran Lyu",
            "Kai Liu",
            "Xuheng Zhang",
            "Haoran Liao",
            "Yusen Feng",
            "Wenxuan Zhu",
            "Tingrui Shen",
            "Jiayi Chen",
            "Jiazhao Zhang",
            "Yifei Dong",
            "Wenbo Cui",
            "Senmao Qi",
            "Shuo Wang",
            "Yixin Zheng",
            "Mi Yan",
            "Xuesong Shi",
            "Haoran Li",
            "Dongbin Zhao",
            "Ming-Yu Liu",
            "Zhizheng Zhang",
            "Li Yi",
            "Yizhou Wang",
            "He Wang"
          ],
          "published": "2026-02-12T17:53:51Z",
          "updated": "2026-02-12T17:53:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12215v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12199v1",
          "title": "Sub--Riemannian boundary value problems for Optimal Geometric Locomotion",
          "summary": "We propose a geometric model for optimal shape-change-induced motions of slender locomotors, e.g., snakes slithering on sand. In these scenarios, the motion of a body in world coordinates is completely determined by the sequence of shapes it assumes. Specifically, we formulate Lagrangian least-dissipation principles as boundary value problems whose solutions are given by sub-Riemannian geodesics. Notably, our geometric model accounts not only for the energy dissipated by the body's displacement through the environment, but also for the energy dissipated by the animal's metabolism or a robot's actuators to induce shape changes such as bending and stretching, thus capturing overall locomotion efficiency. Our continuous model, together with a consistent time and space discretization, enables numerical computation of sub-Riemannian geodesics for three different types of boundary conditions, i.e., fixing initial and target body, restricting to cyclic motion, or solely prescribing body displacement and orientation. The resulting optimal deformation gaits qualitatively match observed motion trajectories of organisms such as snakes and spermatozoa, as well as known optimality results for low-dimensional systems such as Purcell's swimmers. Moreover, being geometrically less rigid than previous frameworks, our model enables new insights into locomotion mechanisms of, e.g., generalized Purcell's swimmers. The code is publicly available.",
          "authors": [
            "Oliver Gross",
            "Florine Hartwig",
            "Martin Rumpf",
            "Peter Schröder"
          ],
          "published": "2026-02-12T17:32:20Z",
          "updated": "2026-02-12T17:32:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.NA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12199v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12314v1",
          "title": "LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning",
          "summary": "We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM",
          "authors": [
            "Junwoon Lee",
            "Yulun Tian"
          ],
          "published": "2026-02-12T17:25:00Z",
          "updated": "2026-02-12T17:25:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12314v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12159v1",
          "title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting",
          "summary": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/",
          "authors": [
            "Wancai Zheng",
            "Hao Chen",
            "Xianlong Lu",
            "Linlin Ou",
            "Xinyi Yu"
          ],
          "published": "2026-02-12T16:41:26Z",
          "updated": "2026-02-12T16:41:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12159v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12096v1",
          "title": "Multi Graph Search for High-Dimensional Robot Motion Planning",
          "summary": "Efficient motion planning for high-dimensional robotic systems, such as manipulators and mobile manipulators, is critical for real-time operation and reliable deployment. Although advances in planning algorithms have enhanced scalability to high-dimensional state spaces, these improvements often come at the cost of generating unpredictable, inconsistent motions or requiring excessive computational resources and memory. In this work, we introduce Multi-Graph Search (MGS), a search-based motion planning algorithm that generalizes classical unidirectional and bidirectional search to a multi-graph setting. MGS maintains and incrementally expands multiple implicit graphs over the state space, focusing exploration on high-potential regions while allowing initially disconnected subgraphs to be merged through feasible transitions as the search progresses. We prove that MGS is complete and bounded-suboptimal, and empirically demonstrate its effectiveness on a range of manipulation and mobile manipulation tasks. Demonstrations, benchmarks and code are available at https://multi-graph-search.github.io/.",
          "authors": [
            "Itamar Mishani",
            "Maxim Likhachev"
          ],
          "published": "2026-02-12T15:50:15Z",
          "updated": "2026-02-12T15:50:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12096v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12095v2",
          "title": "Pack it in: Packing into Partially Filled Containers Through Contact",
          "summary": "The automation of warehouse operations is crucial for improving productivity and reducing human exposure to hazardous environments. One operation frequently performed in warehouses is bin-packing where items need to be placed into containers, either for delivery to a customer, or for temporary storage in the warehouse. Whilst prior bin-packing works have largely been focused on packing items into empty containers and have adopted collision-free strategies, it is often the case that containers will already be partially filled with items, often in suboptimal arrangements due to transportation about a warehouse. This paper presents a contact-aware packing approach that exploits purposeful interactions with previously placed objects to create free space and enable successful placement of new items. This is achieved by using a contact-based multi-object trajectory optimizer within a model predictive controller, integrated with a physics-aware perception system that estimates object poses even during inevitable occlusions, and a method that suggests physically-feasible locations to place the object inside the container.",
          "authors": [
            "David Russell",
            "Zisong Xu",
            "Maximo A. Roa",
            "Mehmet Dogar"
          ],
          "published": "2026-02-12T15:47:04Z",
          "updated": "2026-02-15T20:34:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12095v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12074v1",
          "title": "RF-Modulated Adaptive Communication Improves Multi-Agent Robotic Exploration",
          "summary": "Reliable coordination and efficient communication are critical challenges for multi-agent robotic exploration of environments where communication is limited. This work introduces Adaptive-RF Transmission (ART), a novel communication-aware planning algorithm that dynamically modulates transmission location based on signal strength and data payload size, enabling heterogeneous robot teams to share information efficiently without unnecessary backtracking. We further explore an extension to this approach called ART-SST, which enforces signal strength thresholds for high-fidelity data delivery. Through over 480 simulations across three cave-inspired environments, ART consistently outperforms existing strategies, including full rendezvous and minimum-signal heuristic approaches, achieving up to a 58% reduction in distance traveled and up to 52% faster exploration times compared to baseline methods. These results demonstrate that adaptive, payload-aware communication significantly improves coverage efficiency and mission speed in complex, communication-constrained environments, offering a promising foundation for future planetary exploration and search-and-rescue missions.",
          "authors": [
            "Lorin Achey",
            "Breanne Crockett",
            "Christoffer Heckman",
            "Bradley Hayes"
          ],
          "published": "2026-02-12T15:33:17Z",
          "updated": "2026-02-12T15:33:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12074v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12065v1",
          "title": "Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning",
          "summary": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning.",
          "authors": [
            "Xiang Liu",
            "Sen Cui",
            "Guocai Yao",
            "Zhong Cao",
            "Jingheng Ma",
            "Min Zhang",
            "Changshui Zhang"
          ],
          "published": "2026-02-12T15:23:45Z",
          "updated": "2026-02-12T15:23:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12065v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12063v2",
          "title": "VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model",
          "summary": "The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w",
          "authors": [
            "Yanjiang Guo",
            "Tony Lee",
            "Lucy Xiaoyang Shi",
            "Jianyu Chen",
            "Percy Liang",
            "Chelsea Finn"
          ],
          "published": "2026-02-12T15:21:47Z",
          "updated": "2026-02-15T00:04:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12063v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12062v1",
          "title": "HoloBrain-0 Technical Report",
          "summary": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation.",
          "authors": [
            "Xuewu Lin",
            "Tianwei Lin",
            "Yun Du",
            "Hongyu Xie",
            "Yiwei Jin",
            "Jiawei Li",
            "Shijie Wu",
            "Qingze Wang",
            "Mengdi Li",
            "Mengao Zhao",
            "Ziang Li",
            "Chaodong Huang",
            "Hongzhe Bi",
            "Lichao Huang",
            "Zhizhong Su"
          ],
          "published": "2026-02-12T15:21:04Z",
          "updated": "2026-02-12T15:21:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12062v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12055v1",
          "title": "Multi UAVs Preflight Planning in a Shared and Dynamic Airspace",
          "summary": "Preflight planning for large-scale Unmanned Aerial Vehicle (UAV) fleets in dynamic, shared airspace presents significant challenges, including temporal No-Fly Zones (NFZs), heterogeneous vehicle profiles, and strict delivery deadlines. While Multi-Agent Path Finding (MAPF) provides a formal framework, existing methods often lack the scalability and flexibility required for real-world Unmanned Traffic Management (UTM). We propose DTAPP-IICR: a Delivery-Time Aware Prioritized Planning method with Incremental and Iterative Conflict Resolution. Our framework first generates an initial solution by prioritizing missions based on urgency. Secondly, it computes roundtrip trajectories using SFIPP-ST, a novel 4D single-agent planner (Safe Flight Interval Path Planning with Soft and Temporal Constraints). SFIPP-ST handles heterogeneous UAVs, strictly enforces temporal NFZs, and models inter-agent conflicts as soft constraints. Subsequently, an iterative Large Neighborhood Search, guided by a geometric conflict graph, efficiently resolves any residual conflicts. A completeness-preserving directional pruning technique further accelerates the 3D search. On benchmarks with temporal NFZs, DTAPP-IICR achieves near-100% success with fleets of up to 1,000 UAVs and gains up to 50% runtime reduction from pruning, outperforming batch Enhanced Conflict-Based Search in the UTM context. Scaling successfully in realistic city-scale operations where other priority-based methods fail even at moderate deployments, DTAPP-IICR is positioned as a practical and scalable solution for preflight planning in dense, dynamic urban airspace.",
          "authors": [
            "Amath Sow",
            "Mauricio Rodriguez Cesen",
            "Fabiola Martins Campos de Oliveira",
            "Mariusz Wzorek",
            "Daniel de Leng",
            "Mattias Tiger",
            "Fredrik Heintz",
            "Christian Esteve Rothenberg"
          ],
          "published": "2026-02-12T15:18:46Z",
          "updated": "2026-02-12T15:18:46Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.MA",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12055v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12047v1",
          "title": "Safety Beyond the Training Data: Robust Out-of-Distribution MPC via Conformalized System Level Synthesis",
          "summary": "We present a novel framework for robust out-of-distribution planning and control using conformal prediction (CP) and system level synthesis (SLS), addressing the challenge of ensuring safety and robustness when using learned dynamics models beyond the training data distribution. We first derive high-confidence model error bounds using weighted CP with a learned, state-control-dependent covariance model. These bounds are integrated into an SLS-based robust nonlinear model predictive control (MPC) formulation, which performs constraint tightening over the prediction horizon via volume-optimized forward reachable sets. We provide theoretical guarantees on coverage and robustness under distributional drift, and analyze the impact of data density and trajectory tube size on prediction coverage. Empirically, we demonstrate our method on nonlinear systems of increasing complexity, including a 4D car and a {12D} quadcopter, improving safety and robustness compared to fixed-bound and non-robust baselines, especially outside of the data distribution.",
          "authors": [
            "Anutam Srinivasan",
            "Antoine Leeman",
            "Glen Chou"
          ],
          "published": "2026-02-12T15:11:44Z",
          "updated": "2026-02-12T15:11:44Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG",
            "eess.SY",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12047v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12032v1",
          "title": "When would Vision-Proprioception Policies Fail in Robotic Manipulation?",
          "summary": "Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation.",
          "authors": [
            "Jingxian Lu",
            "Wenke Xia",
            "Yuxuan Wu",
            "Zhiwu Lu",
            "Di Hu"
          ],
          "published": "2026-02-12T15:00:48Z",
          "updated": "2026-02-12T15:00:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12032v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12024v1",
          "title": "Adaptive-Horizon Conflict-Based Search for Closed-Loop Multi-Agent Path Finding",
          "summary": "MAPF is a core coordination problem for large robot fleets in automated warehouses and logistics. Existing approaches are typically either open-loop planners, which generate fixed trajectories and struggle to handle disturbances, or closed-loop heuristics without reliable performance guarantees, limiting their use in safety-critical deployments. This paper presents ACCBS, a closed-loop algorithm built on a finite-horizon variant of CBS with a horizon-changing mechanism inspired by iterative deepening in MPC. ACCBS dynamically adjusts the planning horizon based on the available computational budget, and reuses a single constraint tree to enable seamless transitions between horizons. As a result, it produces high-quality feasible solutions quickly while being asymptotically optimal as the budget increases, exhibiting anytime behavior. Extensive case studies demonstrate that ACCBS combines flexibility to disturbances with strong performance guarantees, effectively bridging the gap between theoretical optimality and practical robustness for large-scale robot deployment.",
          "authors": [
            "Jiarui Li",
            "Federico Pecora",
            "Runyu Zhang",
            "Gioele Zardini"
          ],
          "published": "2026-02-12T14:55:16Z",
          "updated": "2026-02-12T14:55:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12024v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.12012v1",
          "title": "Decentralized Multi-Robot Obstacle Detection and Tracking in a Maritime Scenario",
          "summary": "Autonomous aerial-surface robot teams are promising for maritime monitoring. Robust deployment requires reliable perception over reflective water and scalable coordination under limited communication. We present a decentralized multi-robot framework for detecting and tracking floating containers using multiple UAVs cooperating with an autonomous surface vessel. Each UAV performs YOLOv8 and stereo-disparity-based visual detection, then tracks targets with per-object EKFs using uncertainty-aware data association. Compact track summaries are exchanged and fused conservatively via covariance intersection, ensuring consistency under unknown correlations. An information-driven assignment module allocates targets and selects UAV hover viewpoints by trading expected uncertainty reduction against travel effort and safety separation. Simulation results in a maritime scenario demonstrate improved coverage, localization accuracy, and tracking consistency while maintaining modest communication requirements.",
          "authors": [
            "Muhammad Farhan Ahmed",
            "Vincent Frémont"
          ],
          "published": "2026-02-12T14:42:17Z",
          "updated": "2026-02-12T14:42:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.12012v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11978v1",
          "title": "Accelerating Robotic Reinforcement Learning with Agent Guidance",
          "summary": "Reinforcement Learning (RL) offers a powerful paradigm for autonomous robots to master generalist manipulation skills through trial-and-error. However, its real-world application is stifled by severe sample inefficiency. Recent Human-in-the-Loop (HIL) methods accelerate training by using human corrections, yet this approach faces a scalability barrier. Reliance on human supervisors imposes a 1:1 supervision ratio that limits fleet expansion, suffers from operator fatigue over extended sessions, and introduces high variance due to inconsistent human proficiency. We present Agent-guided Policy Search (AGPS), a framework that automates the training pipeline by replacing human supervisors with a multimodal agent. Our key insight is that the agent can be viewed as a semantic world model, injecting intrinsic value priors to structure physical exploration. By using executable tools, the agent provides precise guidance via corrective waypoints and spatial constraints for exploration pruning. We validate our approach on two tasks, ranging from precision insertion to deformable object manipulation. Results demonstrate that AGPS outperforms HIL methods in sample efficiency. This automates the supervision pipeline, unlocking the path to labor-free and scalable robot learning. Project website: https://agps-rl.github.io/agps.",
          "authors": [
            "Haojun Chen",
            "Zili Zou",
            "Chengdong Ma",
            "Yaoxiang Pu",
            "Haotong Zhang",
            "Yuanpei Chen",
            "Yaodong Yang"
          ],
          "published": "2026-02-12T14:09:32Z",
          "updated": "2026-02-12T14:09:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11978v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11934v1",
          "title": "Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control",
          "summary": "We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a \"blind spot\" for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act.",
          "authors": [
            "Yu Deng",
            "Yufeng Jin",
            "Xiaogang Jia",
            "Jiahong Xue",
            "Gerhard Neumann",
            "Georgia Chalvatzaki"
          ],
          "published": "2026-02-12T13:30:24Z",
          "updated": "2026-02-12T13:30:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11934v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11929v1",
          "title": "General Humanoid Whole-Body Control via Pretraining and Fast Adaptation",
          "summary": "Learning a general whole-body controller for humanoid robots remains challenging due to the diversity of motion distributions, the difficulty of fast adaptation, and the need for robust balance in high-dynamic scenarios. Existing approaches often require task-specific training or suffer from performance degradation when adapting to new motions. In this paper, we present FAST, a general humanoid whole-body control framework that enables Fast Adaptation and Stable Motion Tracking. FAST introduces Parseval-Guided Residual Policy Adaptation, which learns a lightweight delta action policy under orthogonality and KL constraints, enabling efficient adaptation to out-of-distribution motions while mitigating catastrophic forgetting. To further improve physical robustness, we propose Center-of-Mass-Aware Control, which incorporates CoM-related observations and objectives to enhance balance when tracking challenging reference motions. Extensive experiments in simulation and real-world deployment demonstrate that FAST consistently outperforms state-of-the-art baselines in robustness, adaptation efficiency, and generalization.",
          "authors": [
            "Zepeng Wang",
            "Jiangxing Wang",
            "Shiqing Yao",
            "Yu Zhang",
            "Ziluo Ding",
            "Ming Yang",
            "Yuxuan Wang",
            "Haobin Jiang",
            "Chao Ma",
            "Xiaochuan Shi",
            "Zongqing Lu"
          ],
          "published": "2026-02-12T13:26:22Z",
          "updated": "2026-02-12T13:26:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11929v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11890v1",
          "title": "Data-Driven Trajectory Imputation for Vessel Mobility Analysis",
          "summary": "Modeling vessel activity at sea is critical for a wide range of applications, including route planning, transportation logistics, maritime safety, and environmental monitoring. Over the past two decades, the Automatic Identification System (AIS) has enabled real-time monitoring of hundreds of thousands of vessels, generating huge amounts of data daily. One major challenge in using AIS data is the presence of large gaps in vessel trajectories, often caused by coverage limitations or intentional transmission interruptions. These gaps can significantly degrade data quality, resulting in inaccurate or incomplete analysis. State-of-the-art imputation approaches have mainly been devised to tackle gaps in vehicle trajectories, even when the underlying road network is not considered. But the motion patterns of sailing vessels differ substantially, e.g., smooth turns, maneuvering near ports, or navigating in adverse weather conditions. In this application paper, we propose HABIT, a lightweight, configurable H3 Aggregation-Based Imputation framework for vessel Trajectories. This data-driven framework provides a valuable means to impute missing trajectory segments by extracting, analyzing, and indexing motion patterns from historical AIS data. Our empirical study over AIS data across various timeframes, densities, and vessel types reveals that HABIT produces maritime trajectory imputations performing comparably to baseline methods in terms of accuracy, while performing better in terms of latency while accounting for vessel characteristics and their motion patterns.",
          "authors": [
            "Giannis Spiliopoulos",
            "Alexandros Troupiotis-Kapeliaris",
            "Kostas Patroumpas",
            "Nikolaos Liapis",
            "Dimitrios Skoutas",
            "Dimitris Zissis",
            "Nikos Bikakis"
          ],
          "published": "2026-02-12T12:40:27Z",
          "updated": "2026-02-12T12:40:27Z",
          "primary_category": "cs.DB",
          "categories": [
            "cs.DB",
            "cs.CG",
            "cs.RO",
            "eess.IV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11890v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11885v1",
          "title": "Learning to Manipulate Anything: Revealing Data Scaling Laws in Bounding-Box Guided Policies",
          "summary": "Diffusion-based policies show limited generalization in semantic manipulation, posing a key obstacle to the deployment of real-world robots. This limitation arises because relying solely on text instructions is inadequate to direct the policy's attention toward the target object in complex and dynamic environments. To solve this problem, we propose leveraging bounding-box instruction to directly specify target object, and further investigate whether data scaling laws exist in semantic manipulation tasks. Specifically, we design a handheld segmentation device with an automated annotation pipeline, Label-UMI, which enables the efficient collection of demonstration data with semantic labels. We further propose a semantic-motion-decoupled framework that integrates object detection and bounding-box guided diffusion policy to improve generalization and adaptability in semantic manipulation. Throughout extensive real-world experiments on large-scale datasets, we validate the effectiveness of the approach, and reveal a power-law relationship between generalization performance and the number of bounding-box objects. Finally, we summarize an effective data collection strategy for semantic manipulation, which can achieve 85\\% success rates across four tasks on both seen and unseen objects. All datasets and code will be released to the community.",
          "authors": [
            "Yihao Wu",
            "Jinming Ma",
            "Junbo Tan",
            "Yanzhao Yu",
            "Shoujie Li",
            "Mingliang Zhou",
            "Diyun Xiang",
            "Xueqian Wang"
          ],
          "published": "2026-02-12T12:34:56Z",
          "updated": "2026-02-12T12:34:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11885v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11882v1",
          "title": "Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning",
          "summary": "Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at https://github.com/suraj-ranganath/DINO-MBQuant.",
          "authors": [
            "Suraj Ranganath",
            "Anish Patnaik",
            "Vaishak Menon"
          ],
          "published": "2026-02-12T12:32:51Z",
          "updated": "2026-02-12T12:32:51Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11882v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11875v1",
          "title": "DiffPlace: Street View Generation via Place-Controllable Diffusion Model Enhancing Place Recognition",
          "summary": "Generative models have advanced significantly in realistic image synthesis, with diffusion models excelling in quality and stability. Recent multi-view diffusion models improve 3D-aware street view generation, but they struggle to produce place-aware and background-consistent urban scenes from text, BEV maps, and object bounding boxes. This limits their effectiveness in generating realistic samples for place recognition tasks. To address these challenges, we propose DiffPlace, a novel framework that introduces a place-ID controller to enable place-controllable multi-view image generation. The place-ID controller employs linear projection, perceiver transformer, and contrastive learning to map place-ID embeddings into a fixed CLIP space, allowing the model to synthesize images with consistent background buildings while flexibly modifying foreground objects and weather conditions. Extensive experiments, including quantitative comparisons and augmented training evaluations, demonstrate that DiffPlace outperforms existing methods in both generation quality and training support for visual place recognition. Our results highlight the potential of generative models in enhancing scene-level and place-aware synthesis, providing a valuable approach for improving place recognition in autonomous driving",
          "authors": [
            "Ji Li",
            "Zhiwei Li",
            "Shihao Li",
            "Zhenjiang Yu",
            "Boyang Wang",
            "Haiou Liu"
          ],
          "published": "2026-02-12T12:26:09Z",
          "updated": "2026-02-12T12:26:09Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11875v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11862v1",
          "title": "LAMP: Implicit Language Map for Robot Navigation",
          "summary": "Recent advances in vision-language models have made zero-shot navigation feasible, enabling robots to follow natural language instructions without requiring labeling. However, existing methods that explicitly store language vectors in grid or node-based maps struggle to scale to large environments due to excessive memory requirements and limited resolution for fine-grained planning. We introduce LAMP (Language Map), a novel neural language field-based navigation framework that learns a continuous, language-driven map and directly leverages it for fine-grained path generation. Unlike prior approaches, our method encodes language features as an implicit neural field rather than storing them explicitly at every location. By combining this implicit representation with a sparse graph, LAMP supports efficient coarse path planning and then performs gradient-based optimization in the learned field to refine poses near the goal. This coarse-to-fine pipeline, language-driven, gradient-guided optimization is the first application of an implicit language map for precise path generation. This refinement is particularly effective at selecting goal regions not directly observed by leveraging semantic similarities in the learned feature space. To further enhance robustness, we adopt a Bayesian framework that models embedding uncertainty via the von Mises-Fisher distribution, thereby improving generalization to unobserved regions. To scale to large environments, LAMP employs a graph sampling strategy that prioritizes spatial coverage and embedding confidence, retaining only the most informative nodes and substantially reducing computational overhead. Our experimental results, both in NVIDIA Isaac Sim and on a real multi-floor building, demonstrate that LAMP outperforms existing explicit methods in both memory efficiency and fine-grained goal-reaching accuracy.",
          "authors": [
            "Sibaek Lee",
            "Hyeonwoo Yu",
            "Giseop Kim",
            "Sunwook Choi"
          ],
          "published": "2026-02-12T12:09:03Z",
          "updated": "2026-02-12T12:09:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11862v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11832v1",
          "title": "JEPA-VLA: Video Predictive Embedding is Needed for VLA Models",
          "summary": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.",
          "authors": [
            "Shangchen Miao",
            "Ningya Feng",
            "Jialong Wu",
            "Ye Lin",
            "Xu He",
            "Dong Li",
            "Mingsheng Long"
          ],
          "published": "2026-02-12T11:20:43Z",
          "updated": "2026-02-12T11:20:43Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11832v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11758v1",
          "title": "HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model",
          "summary": "Humanoid robots show promise for complex whole-body tasks in unstructured environments. Although Human-Object Interaction (HOI) has advanced, most methods focus on fully actuated objects rigidly coupled to the robot, ignoring underactuated objects with independent dynamics and non-holonomic constraints. These introduce control challenges from coupling forces and occlusions. We present HAIC, a unified framework for robust interaction across diverse object dynamics without external state estimation. Our key contribution is a dynamics predictor that estimates high-order object states (velocity, acceleration) solely from proprioceptive history. These predictions are projected onto static geometric priors to form a spatially grounded dynamic occupancy map, enabling the policy to infer collision boundaries and contact affordances in blind spots. We use asymmetric fine-tuning, where a world model continuously adapts to the student policy's exploration, ensuring robust state estimation under distribution shifts. Experiments on a humanoid robot show HAIC achieves high success rates in agile tasks (skateboarding, cart pushing/pulling under various loads) by proactively compensating for inertial perturbations, and also masters multi-object long-horizon tasks like carrying a box across varied terrain by predicting the dynamics of multiple objects.",
          "authors": [
            "Dongting Li",
            "Xingyu Chen",
            "Qianyang Wu",
            "Bo Chen",
            "Sikai Wu",
            "Hanyu Wu",
            "Guoyao Zhang",
            "Liang Li",
            "Mingliang Zhou",
            "Diyun Xiang",
            "Jianzhu Ma",
            "Qiang Zhang",
            "Renjing Xu"
          ],
          "published": "2026-02-12T09:34:35Z",
          "updated": "2026-02-12T09:34:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11758v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11740v1",
          "title": "Counterfactual Conditional Likelihood Rewards for Multiagent Exploration",
          "summary": "Efficient exploration is critical for multiagent systems to discover coordinated strategies, particularly in open-ended domains such as search and rescue or planetary surveying. However, when exploration is encouraged only at the individual agent level, it often leads to redundancy, as agents act without awareness of how their teammates are exploring. In this work, we introduce Counterfactual Conditional Likelihood (CCL) rewards, which score each agent's exploration by isolating its unique contribution to team exploration. Unlike prior methods that reward agents solely for the novelty of their individual observations, CCL emphasizes observations that are informative with respect to the joint exploration of the team. Experiments in continuous multiagent domains show that CCL rewards accelerate learning for domains with sparse team rewards, where most joint actions yield zero rewards, and are particularly effective in tasks that require tight coordination among agents.",
          "authors": [
            "Ayhan Alp Aydeniz",
            "Robert Loftin",
            "Kagan Tumer"
          ],
          "published": "2026-02-12T09:08:49Z",
          "updated": "2026-02-12T09:08:49Z",
          "primary_category": "cs.MA",
          "categories": [
            "cs.MA",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11740v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11735v1",
          "title": "AC-MASAC: An Attentive Curriculum Learning Framework for Heterogeneous UAV Swarm Coordination",
          "summary": "Cooperative path planning for heterogeneous UAV swarms poses significant challenges for Multi-Agent Reinforcement Learning (MARL), particularly in handling asymmetric inter-agent dependencies and addressing the risks of sparse rewards and catastrophic forgetting during training. To address these issues, this paper proposes an attentive curriculum learning framework (AC-MASAC). The framework introduces a role-aware heterogeneous attention mechanism to explicitly model asymmetric dependencies. Moreover, a structured curriculum strategy is designed, integrating hierarchical knowledge transfer and stage-proportional experience replay to address the issues of sparse rewards and catastrophic forgetting. The proposed framework is validated on a custom multi-agent simulation platform, and the results show that our method has significant advantages over other advanced methods in terms of Success Rate, Formation Keeping Rate, and Success-weighted Mission Time. The code is available at \\textcolor{red}{https://github.com/Wanhao-Liu/AC-MASAC}.",
          "authors": [
            "Wanhao Liu",
            "Junhong Dai",
            "Yixuan Zhang",
            "Shengyun Yin",
            "Panshuo Li"
          ],
          "published": "2026-02-12T09:03:34Z",
          "updated": "2026-02-12T09:03:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11735v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11714v1",
          "title": "GSO-SLAM: Bidirectionally Coupled Gaussian Splatting and Direct Visual Odometry",
          "summary": "We propose GSO-SLAM, a real-time monocular dense SLAM system that leverages Gaussian scene representation. Unlike existing methods that couple tracking and mapping with a unified scene, incurring computational costs, or loosely integrate them with well-structured tracking frameworks, introducing redundancies, our method bidirectionally couples Visual Odometry (VO) and Gaussian Splatting (GS). Specifically, our approach formulates joint optimization within an Expectation-Maximization (EM) framework, enabling the simultaneous refinement of VO-derived semi-dense depth estimates and the GS representation without additional computational overhead. Moreover, we present Gaussian Splat Initialization, which utilizes image information, keyframe poses, and pixel associations from VO to produce close approximations to the final Gaussian scene, thereby eliminating the need for heuristic methods. Through extensive experiments, we validate the effectiveness of our method, showing that it not only operates in real time but also achieves state-of-the-art geometric/photometric fidelity of the reconstructed scene and tracking accuracy.",
          "authors": [
            "Jiung Yeon",
            "Seongbo Ha",
            "Hyeonwoo Yu"
          ],
          "published": "2026-02-12T08:44:32Z",
          "updated": "2026-02-12T08:44:32Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11714v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11706v1",
          "title": "LLM-Driven 3D Scene Generation of Agricultural Simulation Environments",
          "summary": "Procedural generation techniques in 3D rendering engines have revolutionized the creation of complex environments, reducing reliance on manual design. Recent approaches using Large Language Models (LLMs) for 3D scene generation show promise but often lack domain-specific reasoning, verification mechanisms, and modular design. These limitations lead to reduced control and poor scalability. This paper investigates the use of LLMs to generate agricultural synthetic simulation environments from natural language prompts, specifically to address the limitations of lacking domain-specific reasoning, verification mechanisms, and modular design. A modular multi-LLM pipeline was developed, integrating 3D asset retrieval, domain knowledge injection, and code generation for the Unreal rendering engine using its API. This results in a 3D environment with realistic planting layouts and environmental context, all based on the input prompt and the domain knowledge. To enhance accuracy and scalability, the system employs a hybrid strategy combining LLM optimization techniques such as few-shot prompting, Retrieval-Augmented Generation (RAG), finetuning, and validation. Unlike monolithic models, the modular architecture enables structured data handling, intermediate verification, and flexible expansion. The system was evaluated using structured prompts and semantic accuracy metrics. A user study assessed realism and familiarity against real-world images, while an expert comparison demonstrated significant time savings over manual scene design. The results confirm the effectiveness of multi-LLM pipelines in automating domain-specific 3D scene generation with improved reliability and precision. Future work will explore expanding the asset hierarchy, incorporating real-time generation, and adapting the pipeline to other simulation domains beyond agriculture.",
          "authors": [
            "Arafa Yoncalik",
            "Wouter Jansen",
            "Nico Huebel",
            "Mohammad Hasan Rahmani",
            "Jan Steckel"
          ],
          "published": "2026-02-12T08:33:01Z",
          "updated": "2026-02-12T08:33:01Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11706v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11660v1",
          "title": "Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes",
          "summary": "Reliable 3D instance segmentation is fundamental to language-grounded robotic manipulation. Its critical application lies in cluttered environments, where occlusions, limited viewpoints, and noisy masks degrade perception. To address these challenges, we present Clutt3R-Seg, a zero-shot pipeline for robust 3D instance segmentation for language-grounded grasping in cluttered scenes. Our key idea is to introduce a hierarchical instance tree of semantic cues. Unlike prior approaches that attempt to refine noisy masks, our method leverages them as informative cues: through cross-view grouping and conditional substitution, the tree suppresses over- and under-segmentation, yielding view-consistent masks and robust 3D instances. Each instance is enriched with open-vocabulary semantic embeddings, enabling accurate target selection from natural language instructions. To handle scene changes during multi-stage tasks, we further introduce a consistency-aware update that preserves instance correspondences from only a single post-interaction image, allowing efficient adaptation without rescanning. Clutt3R-Seg is evaluated on both synthetic and real-world datasets, and validated on a real robot. Across all settings, it consistently outperforms state-of-the-art baselines in cluttered and sparse-view scenarios. Even on the most challenging heavy-clutter sequences, Clutt3R-Seg achieves an AP@25 of 61.66, over 2.2x higher than baselines, and with only four input views it surpasses MaskClustering with eight views by more than 2x. The code is available at: https://github.com/jeonghonoh/clutt3r-seg.",
          "authors": [
            "Jeongho Noh",
            "Tai Hyoung Rhee",
            "Eunho Lee",
            "Jeongyun Kim",
            "Sunwoo Lee",
            "Ayoung Kim"
          ],
          "published": "2026-02-12T07:25:52Z",
          "updated": "2026-02-12T07:25:52Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11660v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11656v1",
          "title": "SToRM: Supervised Token Reduction for Multi-modal LLMs toward efficient end-to-end autonomous driving",
          "summary": "In autonomous driving, end-to-end (E2E) driving systems that predict control commands directly from sensor data have achieved significant advancements. For safe driving in unexpected scenarios, these systems may additionally rely on human interventions such as natural language instructions. Using a multi-modal large language model (MLLM) facilitates human-vehicle interaction and can improve performance in such scenarios. However, this approach requires substantial computational resources due to its reliance on an LLM and numerous visual tokens from sensor inputs, which are limited in autonomous vehicles. Many MLLM studies have explored reducing visual tokens, but often suffer end-task performance degradation compared to using all tokens. To enable efficient E2E driving while maintaining performance comparable to using all tokens, this paper proposes the first Supervised Token Reduction framework for multi-modal LLMs (SToRM). The proposed framework consists of three key elements. First, a lightweight importance predictor with short-term sliding windows estimates token importance scores. Second, a supervised training approach uses an auxiliary path to obtain pseudo-supervision signals from an all-token LLM pass. Third, an anchor-context merging module partitions tokens into anchors and context tokens, and merges context tokens into relevant anchors to reduce redundancy while minimizing information loss. Experiments on the LangAuto benchmark show that SToRM outperforms state-of-the-art E2E driving MLLMs under the same reduced-token budget, maintaining all-token performance while reducing computational cost by up to 30x.",
          "authors": [
            "Seo Hyun Kim",
            "Jin Bok Park",
            "Do Yeon Koo",
            "Ho Gun Park",
            "Il Yong Chun"
          ],
          "published": "2026-02-12T07:21:24Z",
          "updated": "2026-02-12T07:21:24Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11656v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11648v1",
          "title": "Human-Like Gaze Behavior in Social Robots: A Deep Learning Approach Integrating Human and Non-Human Stimuli",
          "summary": "Nonverbal behaviors, particularly gaze direction, play a crucial role in enhancing effective communication in social interactions. As social robots increasingly participate in these interactions, they must adapt their gaze based on human activities and remain receptive to all cues, whether human-generated or not, to ensure seamless and effective communication. This study aims to increase the similarity between robot and human gaze behavior across various social situations, including both human and non-human stimuli (e.g., conversations, pointing, door openings, and object drops). A key innovation in this study, is the investigation of gaze responses to non-human stimuli, a critical yet underexplored area in prior research. These scenarios, were simulated in the Unity software as a 3D animation and a 360-degree real-world video. Data on gaze directions from 41 participants were collected via virtual reality (VR) glasses. Preprocessed data, trained two neural networks-LSTM and Transformer-to build predictive models based on individuals' gaze patterns. In the animated scenario, the LSTM and Transformer models achieved prediction accuracies of 67.6% and 70.4%, respectively; In the real-world scenario, the LSTM and Transformer models achieved accuracies of 72% and 71.6%, respectively. Despite the gaze pattern differences among individuals, our models outperform existing approaches in accuracy while uniquely considering non-human stimuli, offering a significant advantage over previous literature. Furthermore, deployed on the NAO robot, the system was evaluated by 275 participants via a comprehensive questionnaire, with results demonstrating high satisfaction during interactions. This work advances social robotics by enabling robots to dynamically mimic human gaze behavior in complex social contexts.",
          "authors": [
            "Faezeh Vahedi",
            "Morteza Memari",
            "Ramtin Tabatabaei",
            "Alireza Taheri"
          ],
          "published": "2026-02-12T07:01:17Z",
          "updated": "2026-02-12T07:01:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11648v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11643v1",
          "title": "ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning",
          "summary": "Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches mostly focus on the alignment of visual and tactile features and the integration mechanism tends to be direct concatenation. Consequently, they struggle to effectively cope with occluded scenarios due to neglecting the inherent complementary nature of both modalities and the alignment may not be exploited enough, limiting the potential of their real-world deployment. In this paper, we present ViTaS, a simple yet effective framework that incorporates both visual and tactile information to guide the behavior of an agent. We introduce Soft Fusion Contrastive Learning, an advanced version of conventional contrastive learning method and a CVAE module to utilize the alignment and complementarity within visuo-tactile representations. We demonstrate the effectiveness of our method in 12 simulated and 3 real-world environments, and our experiments show that ViTaS significantly outperforms existing baselines. Project page: https://skyrainwind.github.io/ViTaS/index.html.",
          "authors": [
            "Yufeng Tian",
            "Shuiqi Cheng",
            "Tianming Wei",
            "Tianxing Zhou",
            "Yuanhang Zhang",
            "Zixian Liu",
            "Qianwei Han",
            "Zhecheng Yuan",
            "Huazhe Xu"
          ],
          "published": "2026-02-12T06:56:29Z",
          "updated": "2026-02-12T06:56:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11643v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11598v1",
          "title": "ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation",
          "summary": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation. To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.",
          "authors": [
            "Zedong Chu",
            "Shichao Xie",
            "Xiaolong Wu",
            "Yanfen Shen",
            "Minghua Luo",
            "Zhengbo Wang",
            "Fei Liu",
            "Xiaoxu Leng",
            "Junjun Hu",
            "Mingyang Yin",
            "Jia Lu",
            "Yingnan Guo",
            "Kai Yang",
            "Jiawei Han",
            "Xu Chen",
            "Yanqing Zhu",
            "Yuxiang Zhao",
            "Xin Liu",
            "Yirong Yang",
            "Ye He",
            "Jiahang Wang",
            "Yang Cai",
            "Tianlin Zhang",
            "Li Gao",
            "Liu Liu",
            "Mingchao Sun",
            "Fan Jiang",
            "Chiyu Wang",
            "Zhicheng Liu",
            "Hongyu Pan",
            "Honglin Han",
            "Zhining Gu",
            "Kuan Yang",
            "Jianfang Zhang",
            "Di Jing",
            "Zihao Guan",
            "Wei Guo",
            "Guoqing Liu",
            "Di Yang",
            "Xiangpo Yang",
            "Menglin Yang",
            "Hongguang Xing",
            "Weiguo Li",
            "Mu Xu"
          ],
          "published": "2026-02-12T05:30:20Z",
          "updated": "2026-02-12T05:30:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11598v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11575v2",
          "title": "ReaDy-Go: Real-to-Sim Dynamic 3D Gaussian Splatting Simulation for Environment-Specific Visual Navigation with Moving Obstacles",
          "summary": "Visual navigation models often struggle in real-world dynamic environments due to limited robustness to the sim-to-real gap and the difficulty of training policies tailored to target deployment environments (e.g., households, restaurants, and factories). Although real-to-sim navigation simulation using 3D Gaussian Splatting (GS) can mitigate these challenges, prior GS-based works have considered only static scenes or non-photorealistic human obstacles built from simulator assets, despite the importance of safe navigation in dynamic environments. To address these issues, we propose ReaDy-Go, a novel real-to-sim simulation pipeline that synthesizes photorealistic dynamic scenarios in target environments by augmenting a reconstructed static GS scene with dynamic human GS obstacles, and trains navigation policies using the generated datasets. The pipeline provides three key contributions: (1) a dynamic GS simulator that integrates static scene GS with a human animation module, enabling the insertion of animatable human GS avatars and the synthesis of plausible human motions from 2D trajectories, (2) a navigation dataset generation framework that leverages the simulator along with a robot expert planner designed for dynamic GS representations and a human planner, and (3) robust navigation policies to both the sim-to-real gap and moving obstacles. The proposed simulator generates thousands of photorealistic navigation scenarios with animatable human GS avatars from arbitrary viewpoints. ReaDy-Go outperforms baselines across target environments in both simulation and real-world experiments, demonstrating improved navigation performance even after sim-to-real transfer and in the presence of moving obstacles. Moreover, zero-shot sim-to-real deployment in an unseen environment indicates its generalization potential. Project page: https://syeon-yoo.github.io/ready-go-site/.",
          "authors": [
            "Seungyeon Yoo",
            "Youngseok Jang",
            "Dabin Kim",
            "Youngsoo Han",
            "Seungwoo Jung",
            "H. Jin Kim"
          ],
          "published": "2026-02-12T04:48:18Z",
          "updated": "2026-02-14T15:05:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11575v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11554v2",
          "title": "HyperDet: 3D Object Detection with Hyper 4D Radar Point Clouds",
          "summary": "4D mmWave radar provides weather-robust, velocity-aware measurements and is more cost-effective than LiDAR. However, radar-only 3D detection still trails LiDAR-based systems because radar point clouds are sparse, irregular, and often corrupted by multipath noise, yielding weak and unstable geometry. We present HyperDet, a detector-agnostic radar-only 3D detection framework that constructs a task-aware hyper 4D radar point cloud for standard LiDAR-oriented detectors. HyperDet aggregates returns from multiple surround-view 4D radars over consecutive frames to improve coverage and density, then applies geometry-aware cross-sensor consensus validation with a lightweight self-consistency check outside overlap regions to suppress inconsistent returns. It further integrates a foreground-focused diffusion module with training-time mixed radar-LiDAR supervision to densify object structures while lifting radar attributes (e.g., Doppler, RCS); the model is distilled into a consistency model for single-step inference. On MAN TruckScenes, HyperDet consistently improves over raw radar inputs with VoxelNeXt and CenterPoint, partially narrowing the radar-LiDAR gap. These results show that input-level refinement enables radar to better leverage LiDAR-oriented detectors without architectural modifications.",
          "authors": [
            "Yichun Xiao",
            "Runwei Guan",
            "Fangqiang Ding"
          ],
          "published": "2026-02-12T04:21:58Z",
          "updated": "2026-02-13T21:13:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11554v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15899v2",
          "title": "SceneVGGT: VGGT-based online 3D semantic SLAM for indoor scene understanding and navigation",
          "summary": "We present SceneVGGT, a spatio-temporal 3D scene understanding framework that combines SLAM with semantic mapping for autonomous and assistive navigation. Built on VGGT, our method scales to long video streams via a sliding-window pipeline. We align local submaps using camera-pose transformations, enabling memory- and speed-efficient mapping while preserving geometric consistency. Semantics are lifted from 2D instance masks to 3D objects using the VGGT tracking head, maintaining temporally coherent identities for change detection. As a proof of concept, object locations are projected onto an estimated floor plane for assistive navigation. The pipeline's GPU memory usage remains under 17 GB, irrespectively of the length of the input sequence and achieves competitive point-cloud performance on the ScanNet++ benchmark. Overall, SceneVGGT ensures robust semantic identification and is fast enough to support interactive assistive navigation with audio feedback.",
          "authors": [
            "Anna Gelencsér-Horváth",
            "Gergely Dinya",
            "Dorka Boglárka Erős",
            "Péter Halász",
            "Islam Muhammad Muqsit",
            "Kristóf Karacs"
          ],
          "published": "2026-02-12T03:11:23Z",
          "updated": "2026-02-19T10:32:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.IV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15899v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11468v2",
          "title": "Effective Task Planning with Missing Objects using Learning-Informed Object Search",
          "summary": "Task planning for mobile robots often assumes full environment knowledge and so popular approaches, like planning via the PDDL, cannot plan when the locations of task-critical objects are unknown. Recent learning-driven object search approaches are effective, but operate as standalone tools and so are not straightforwardly incorporated into full task planners, which must additionally determine both what objects are necessary and when in the plan they should be sought out. To address this limitation, we develop a planning framework centered around novel model-based LIOS actions: each a policy that aims to find and retrieve a single object. High-level planning treats LIOS actions as deterministic and so -- informed by model-based calculations of the expected cost of each -- generates plans that interleave search and execution for effective, sound, and complete learning-informed task planning despite uncertainty. Our work effectively reasons about uncertainty while maintaining compatibility with existing full-knowledge solvers. In simulated ProcTHOR homes and in the real world, our approach outperforms non-learned and learned baselines on tasks including retrieval and meal prep.",
          "authors": [
            "Raihan Islam Arnob",
            "Max Merlin",
            "Abhishek Paudel",
            "Benned Hedegaard",
            "George Konidaris",
            "Gregory J. Stein"
          ],
          "published": "2026-02-12T00:56:35Z",
          "updated": "2026-02-13T13:22:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11468v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11464v1",
          "title": "EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos",
          "summary": "Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable. To address this, we propose the EasyMimic framework, a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras. Our method first extracts 3D hand trajectories from the videos. An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks. Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes. Project website: https://zt375356.github.io/EasyMimic-Project/.",
          "authors": [
            "Tao Zhang",
            "Song Xia",
            "Ye Wang",
            "Qin Jin"
          ],
          "published": "2026-02-12T00:41:01Z",
          "updated": "2026-02-12T00:41:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11464v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11399v1",
          "title": "Can We Really Learn One Representation to Optimize All Rewards?",
          "summary": "As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.",
          "authors": [
            "Chongyi Zheng",
            "Royina Karegoudra Jayanth",
            "Benjamin Eysenbach"
          ],
          "published": "2026-02-11T22:06:25Z",
          "updated": "2026-02-11T22:06:25Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11399v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11393v1",
          "title": "Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video",
          "summary": "We present an approach to robot learning from egocentric human videos by modeling human preferences in a reward function and optimizing robot behavior to maximize this reward. Prior work on reward learning from human videos attempts to measure the long-term value of a visual state as the temporal distance between it and the terminal state in a demonstration video. These approaches make assumptions that limit performance when learning from video. They must also transfer the learned value function across the embodiment and environment gap. Our method models human preferences by learning to predict the motion of tracked points between subsequent images and defines a reward function as the agreement between predicted and observed object motion in a robot's behavior at each step. We then use a modified Soft Actor Critic (SAC) algorithm initialized with 10 on-robot demonstrations to estimate a value function from this reward and optimize a policy that maximizes this value function, all on the robot. Our approach is capable of learning on a real robot, and we show that policies learned with our reward model match or outperform prior work across multiple tasks in both simulation and on the real robot.",
          "authors": [
            "Mrinal Verghese",
            "Christopher G. Atkeson"
          ],
          "published": "2026-02-11T21:56:47Z",
          "updated": "2026-02-11T21:56:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11393v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11337v2",
          "title": "MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation",
          "summary": "Deploying robots at scale demands robustness to the long tail of everyday situations. The countless variations in scene layout, object geometry, and task specifications that characterize real environments are vast and underrepresented in existing robot benchmarks. Measuring this level of generalization requires infrastructure at a scale and diversity that physical evaluation alone cannot provide. We introduce MolmoSpaces, a fully open ecosystem to support large-scale benchmarking of robot policies. MolmoSpaces consists of over 230k diverse indoor environments, ranging from handcrafted household scenes to procedurally generated multiroom houses, populated with 130k richly annotated object assets, including 48k manipulable objects with 42M stable grasps. Crucially, these environments are simulator-agnostic, supporting popular options such as MuJoCo, Isaac, and ManiSkill. The ecosystem supports the full spectrum of embodied tasks: static and mobile manipulation, navigation, and multiroom long-horizon tasks requiring coordinated perception, planning, and interaction across entire indoor environments. We also design MolmoSpaces-Bench, a benchmark suite of 8 tasks in which robots interact with our diverse scenes and richly annotated objects. Our experiments show MolmoSpaces-Bench exhibits strong sim-to-real correlation (R = 0.96, \\r{ho} = 0.98), confirm newer and stronger zero-shot policies outperform earlier versions in our benchmarks, and identify key sensitivities to prompt phrasing, initial joint positions, and camera occlusion. Through MolmoSpaces and its open-source assets and tooling, we provide a foundation for scalable data generation, policy training, and benchmark creation for robot learning research.",
          "authors": [
            "Yejin Kim",
            "Wilbert Pumacay",
            "Omar Rayyan",
            "Max Argus",
            "Winson Han",
            "Eli VanderBilt",
            "Jordi Salvador",
            "Abhay Deshpande",
            "Rose Hendrix",
            "Snehal Jauhri",
            "Shuo Liu",
            "Nur Muhammad Mahi Shafiullah",
            "Maya Guru",
            "Ainaz Eftekhar",
            "Karen Farley",
            "Donovan Clay",
            "Jiafei Duan",
            "Arjun Guru",
            "Piper Wolters",
            "Alvaro Herrasti",
            "Ying-Chun Lee",
            "Georgia Chalvatzaki",
            "Yuchen Cui",
            "Ali Farhadi",
            "Dieter Fox",
            "Ranjay Krishna"
          ],
          "published": "2026-02-11T20:16:31Z",
          "updated": "2026-02-19T00:59:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11337v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11321v1",
          "title": "ExtremControl: Low-Latency Humanoid Teleoperation with Direct Extremity Control",
          "summary": "Building a low-latency humanoid teleoperation system is essential for collecting diverse reactive and dynamic demonstrations. However, existing approaches rely on heavily pre-processed human-to-humanoid motion retargeting and position-only PD control, resulting in substantial latency that severely limits responsiveness and prevents tasks requiring rapid feedback and fast reactions. To address this problem, we propose ExtremControl, a low latency whole-body control framework that: (1) operates directly on SE(3) poses of selected rigid links, primarily humanoid extremities, to avoid full-body retargeting; (2) utilizes a Cartesian-space mapping to directly convert human motion to humanoid link targets; and (3) incorporates velocity feedforward control at low level to support highly responsive behavior under rapidly changing control interfaces. We further provide a unified theoretical formulation of ExtremControl and systematically validate its effectiveness through experiments in both simulation and real-world environments. Building on ExtremControl, we implement a low-latency humanoid teleoperation system that supports both optical motion capture and VR-based motion tracking, achieving end-to-end latency as low as 50ms and enabling highly responsive behaviors such as ping-pong ball balancing, juggling, and real-time return, thereby substantially surpassing the 200ms latency limit observed in prior work.",
          "authors": [
            "Ziyan Xiong",
            "Lixing Fang",
            "Junyun Huang",
            "Kashu Yamazaki",
            "Hao Zhang",
            "Chuang Gan"
          ],
          "published": "2026-02-11T19:49:12Z",
          "updated": "2026-02-11T19:49:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11321v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11291v1",
          "title": "H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model",
          "summary": "World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.",
          "authors": [
            "Wenyuan Chen",
            "Jinbang Huang",
            "Oscar Pang",
            "Zhiyuan Li",
            "Xiao Hu",
            "Lingfeng Zhang",
            "Zhanguang Zhang",
            "Mark Coates",
            "Tongtong Cao",
            "Xingyue Quan",
            "Yingxue Zhang"
          ],
          "published": "2026-02-11T19:08:36Z",
          "updated": "2026-02-11T19:08:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11291v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11150v1",
          "title": "YOR: Your Own Mobile Manipulator for Generalizable Robotics",
          "summary": "Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR's capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/",
          "authors": [
            "Manan H Anjaria",
            "Mehmet Enes Erciyes",
            "Vedant Ghatnekar",
            "Neha Navarkar",
            "Haritheja Etukuru",
            "Xiaole Jiang",
            "Kanad Patel",
            "Dhawal Kabra",
            "Nicholas Wojno",
            "Radhika Ajay Prayage",
            "Soumith Chintala",
            "Lerrel Pinto",
            "Nur Muhammad Mahi Shafiullah",
            "Zichen Jeff Cui"
          ],
          "published": "2026-02-11T18:59:00Z",
          "updated": "2026-02-11T18:59:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11150v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11143v1",
          "title": "APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots",
          "summary": "Humanoid locomotion has advanced rapidly with deep reinforcement learning (DRL), enabling robust feet-based traversal over uneven terrain. Yet platforms beyond leg length remain largely out of reach because current RL training paradigms often converge to jumping-like solutions that are high-impact, torque-limited, and unsafe for real-world deployment. To address this gap, we propose APEX, a system for perceptive, climbing-based high-platform traversal that composes terrain-conditioned behaviors: climb-up and climb-down at vertical edges, walking or crawling on the platform, and stand-up and lie-down for posture reconfiguration. Central to our approach is a generalized ratchet progress reward for learning contact-rich, goal-reaching maneuvers. It tracks the best-so-far task progress and penalizes non-improving steps, providing dense yet velocity-free supervision that enables efficient exploration under strong safety regularization. Based on this formulation, we train LiDAR-based full-body maneuver policies and reduce the sim-to-real perception gap through a dual strategy: modeling mapping artifacts during training and applying filtering and inpainting to elevation maps during deployment. Finally, we distill all six skills into a single policy that autonomously selects behaviors and transitions based on local geometry and commands. Experiments on a 29-DoF Unitree G1 humanoid demonstrate zero-shot sim-to-real traversal of 0.8 meter platforms (approximately 114% of leg length), with robust adaptation to platform height and initial pose, as well as smooth and stable multi-skill transitions.",
          "authors": [
            "Yikai Wang",
            "Tingxuan Leng",
            "Changyi Lin",
            "Shiqi Liu",
            "Shir Simon",
            "Bingqing Chen",
            "Jonathan Francis",
            "Ding Zhao"
          ],
          "published": "2026-02-11T18:55:11Z",
          "updated": "2026-02-11T18:55:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11143v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11142v1",
          "title": "Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows",
          "summary": "Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive normalizing flow policies at both the high- and low-levels of the hierarchy is introduced. This design enables tractable log-likelihood computation, efficient sampling, and the ability to model rich multimodal behaviors. New theoretical guarantees are derived, including explicit KL-divergence bounds for Real-valued non-volume preserving (RealNVP) policies and PAC-style sample efficiency results, showing that NF-HIQL preserves stability while improving generalization. Empirically, NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling, and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior goal-conditioned and hierarchical baselines, demonstrating superior robustness under limited data and highlighting the potential of flow-based architectures for scalable, data-efficient hierarchical reinforcement learning.",
          "authors": [
            "Shaswat Garg",
            "Matin Moezzi",
            "Brandon Da Silva"
          ],
          "published": "2026-02-11T18:54:48Z",
          "updated": "2026-02-11T18:54:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11142v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11125v1",
          "title": "Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots",
          "summary": "We study the \\textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \\textit{Look-Compute-Move} (LCM) model with \\textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \\textbf{line-segment setting}, the \\textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \\textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \\textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.",
          "authors": [
            "Animesh Maiti",
            "Abhinav Chakraborty",
            "Bibhuti Das",
            "Subhash Bhagat",
            "Krishnendu Mukhopadhyaya"
          ],
          "published": "2026-02-11T18:38:24Z",
          "updated": "2026-02-11T18:38:24Z",
          "primary_category": "cs.DC",
          "categories": [
            "cs.DC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11125v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11116v1",
          "title": "Multi-UAV Trajectory Optimization for Bearing-Only Localization in GPS Denied Environments",
          "summary": "Accurate localization of maritime targets by unmanned aerial vehicles (UAVs) remains challenging in GPS-denied environments. UAVs equipped with gimballed electro-optical sensors are typically used to localize targets, however, reliance on these sensors increases mechanical complexity, cost, and susceptibility to single-point failures, limiting scalability and robustness in multi-UAV operations. This work presents a new trajectory optimization framework that enables cooperative target localization using UAVs with fixed, non-gimballed cameras operating in coordination with a surface vessel. This estimation-aware optimization generates dynamically feasible trajectories that explicitly account for mission constraints, platform dynamics, and out-of-frame events. Estimation-aware trajectories outperform heuristic paths by reducing localization error by more than a factor of two, motivating their use in cooperative operations. Results further demonstrate that coordinated UAVs with fixed, non-gimballed cameras achieve localization accuracy that meets or exceeds that of single gimballed systems, while substantially lowering system complexity and cost, enabling scalability, and enhancing mission resilience.",
          "authors": [
            "Alfonso Sciacchitano",
            "Liraz Mudrik",
            "Sean Kragelund",
            "Isaac Kaminer"
          ],
          "published": "2026-02-11T18:29:05Z",
          "updated": "2026-02-11T18:29:05Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11116v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11113v1",
          "title": "A receding-horizon multi-contact motion planner for legged robots in challenging environments",
          "summary": "We present a novel receding-horizon multi-contact motion planner for legged robots in challenging scenarios, able to plan motions such as chimney climbing, navigating very narrow passages or crossing large gaps. Our approach adds new capabilities to the state of the art, including the ability to reactively re-plan in response to new information, and planning contact locations and whole-body trajectories simultaneously, simplifying the implementation and removing the need for post-processing or complex multi-stage approaches. Our method is more resistant to local minima problems than other potential field based approaches, and our quadratic-program-based posture generator returns nodes more quickly than those of existing algorithms. Rigorous statistical analysis shows that, with short planning horizons (e.g., one step ahead), our planner is faster than the state-of-the-art across all scenarios tested (between 45% and 98% faster on average, depending on the scenario), while planning less efficient motions (requiring 5% fewer to 700% more stance changes on average). In all but one scenario (Chimney Walking), longer planning horizons (e.g., four steps ahead) extended the average planning times (between 73% faster and 400% slower than the state-of-the-art) but resulted in higher quality motion plans (between 8% more and 47% fewer stance changes than the state-of-the-art).",
          "authors": [
            "Daniel S. J. Derwent",
            "Simon Watson",
            "Bruno V. Adorno"
          ],
          "published": "2026-02-11T18:25:29Z",
          "updated": "2026-02-11T18:25:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11113v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11082v1",
          "title": "Digging for Data: Experiments in Rock Pile Characterization Using Only Proprioceptive Sensing in Excavation",
          "summary": "Characterization of fragmented rock piles is a fundamental task in the mining and quarrying industries, where rock is fragmented by blasting, transported using wheel loaders, and then sent for further processing. This field report studies a novel method for estimating the relative particle size of fragmented rock piles from only proprioceptive data collected while digging with a wheel loader. Rather than employ exteroceptive sensors (e.g., cameras or LiDAR sensors) to estimate rock particle sizes, the studied method infers rock fragmentation from an excavator's inertial response during excavation. This paper expands on research that postulated the use of wavelet analysis to construct a unique feature that is proportional to the level of rock fragmentation. We demonstrate through extensive field experiments that the ratio of wavelet features, constructed from data obtained by excavating in different rock piles with different size distributions, approximates the ratio of the mean particle size of the two rock piles. Full-scale excavation experiments were performed with a battery electric, 18-tonne capacity, load-haul-dump (LHD) machine in representative conditions in an operating quarry. The relative particle size estimates generated with the proposed sensing methodology are compared with those obtained from both a vision-based fragmentation analysis tool and from sieving of sampled materials.",
          "authors": [
            "Unal Artan",
            "Martin Magnusson",
            "Joshua A. Marshall"
          ],
          "published": "2026-02-11T17:47:34Z",
          "updated": "2026-02-11T17:47:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SP"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11082v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11075v1",
          "title": "RISE: Self-Improving Robot Policy with Compositional World Model",
          "summary": "Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.",
          "authors": [
            "Jiazhi Yang",
            "Kunyang Lin",
            "Jinwei Li",
            "Wencong Zhang",
            "Tianwei Lin",
            "Longyan Wu",
            "Zhizhong Su",
            "Hao Zhao",
            "Ya-Qin Zhang",
            "Li Chen",
            "Ping Luo",
            "Xiangyu Yue",
            "Hongyang Li"
          ],
          "published": "2026-02-11T17:43:36Z",
          "updated": "2026-02-11T17:43:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11075v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11049v1",
          "title": "SQ-CBF: Signed Distance Functions for Numerically Stable Superquadric-Based Safety Filtering",
          "summary": "Ensuring safe robot operation in cluttered and dynamic environments remains a fundamental challenge. While control barrier functions provide an effective framework for real-time safety filtering, their performance critically depends on the underlying geometric representation, which is often simplified, leading to either overly conservative behavior or insufficient collision coverage. Superquadrics offer an expressive way to model complex shapes using a few primitives and are increasingly used for robot safety. To integrate this representation into collision avoidance, most existing approaches directly use their implicit functions as barrier candidates. However, we identify a critical but overlooked issue in this practice: the gradients of the implicit SQ function can become severely ill-conditioned, potentially rendering the optimization infeasible and undermining reliable real-time safety filtering. To address this issue, we formulate an SQ-based safety filtering framework that uses signed distance functions as barrier candidates. Since analytical SDFs are unavailable for general SQs, we compute distances using the efficient Gilbert-Johnson-Keerthi algorithm and obtain gradients via randomized smoothing. Extensive simulation and real-world experiments demonstrate consistent collision-free manipulation in cluttered and unstructured scenes, showing robustness to challenging geometries, sensing noise, and dynamic disturbances, while improving task efficiency in teleoperation tasks. These results highlight a pathway toward safety filters that remain precise and reliable under the geometric complexity of real-world environments.",
          "authors": [
            "Haocheng Zhao",
            "Lukas Brunke",
            "Oliver Lagerquist",
            "Siqi Zhou",
            "Angela P. Schoellig"
          ],
          "published": "2026-02-11T17:19:43Z",
          "updated": "2026-02-11T17:19:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11049v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11021v1",
          "title": "ContactGaussian-WM: Learning Physics-Grounded World Model from Videos",
          "summary": "Developing world models that understand complex physical interactions is essential for advancing robotic planning and simulation.However, existing methods often struggle to accurately model the environment under conditions of data scarcity and complex contact-rich dynamic motion.To address these challenges, we propose ContactGaussian-WM, a differentiable physics-grounded rigid-body world model capable of learning intricate physical laws directly from sparse and contact-rich video sequences.Our framework consists of two core components: (1) a unified Gaussian representation for both visual appearance and collision geometry, and (2) an end-to-end differentiable learning framework that differentiates through a closed-form physics engine to infer physical properties from sparse visual observations.Extensive simulations and real-world evaluations demonstrate that ContactGaussian-WM outperforms state-of-the-art methods in learning complex scenarios, exhibiting robust generalization capabilities.Furthermore, we showcase the practical utility of our framework in downstream applications, including data synthesis and real-time MPC.",
          "authors": [
            "Meizhong Wang",
            "Wanxin Jin",
            "Kun Cao",
            "Lihua Xie",
            "Yiguang Hong"
          ],
          "published": "2026-02-11T16:48:13Z",
          "updated": "2026-02-11T16:48:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11021v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11236v1",
          "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning",
          "summary": "Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.",
          "authors": [
            "Yandan Yang",
            "Shuang Zeng",
            "Tong Lin",
            "Xinyuan Chang",
            "Dekang Qi",
            "Junjin Xiao",
            "Haoyun Liu",
            "Ronghan Chen",
            "Yuzhi Chen",
            "Dongjie Huo",
            "Feng Xiong",
            "Xing Wei",
            "Zhiheng Ma",
            "Mu Xu"
          ],
          "published": "2026-02-11T16:47:01Z",
          "updated": "2026-02-11T16:47:01Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.CL",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11236v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11004v1",
          "title": "Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception",
          "summary": "Autonomous vehicles (AVs) rely on sensors and deep neural networks (DNNs) to perceive their surrounding environment and make maneuver decisions in real time. However, achieving real-time DNN inference in the AV's perception pipeline is challenging due to the large gap between the computation requirement and the AV's limited resources. Most, if not all, of existing studies focus on optimizing the DNN inference time to achieve faster perception by compressing the DNN model with pruning and quantization. In contrast, we present a Predictable Perception system with DNNs (PP-DNN) that reduce the amount of image data to be processed while maintaining the same level of accuracy for multi-tenant DNNs by dynamically selecting critical frames and regions of interest (ROIs). PP-DNN is based on our key insight that critical frames and ROIs for AVs vary with the AV's surrounding environment. However, it is challenging to identify and use critical frames and ROIs in multi-tenant DNNs for predictable inference. Given image-frame streams, PP-DNN leverages an ROI generator to identify critical frames and ROIs based on the similarities of consecutive frames and traffic scenarios. PP-DNN then leverages a FLOPs predictor to predict multiply-accumulate operations (MACs) from the dynamic critical frames and ROIs. The ROI scheduler coordinates the processing of critical frames and ROIs with multiple DNN models. Finally, we design a detection predictor for the perception of non-critical frames. We have implemented PP-DNN in an ROS-based AV pipeline and evaluated it with the BDD100K and the nuScenes dataset. PP-DNN is observed to significantly enhance perception predictability, increasing the number of fusion frames by up to 7.3x, reducing the fusion delay by >2.6x and fusion-delay variations by >2.3x, improving detection completeness by 75.4% and the cost-effectiveness by up to 98% over the baseline.",
          "authors": [
            "Liangkai Liu",
            "Kang G. Shin",
            "Jinkyu Lee",
            "Chengmo Yang",
            "Weisong Shi"
          ],
          "published": "2026-02-11T16:25:10Z",
          "updated": "2026-02-11T16:25:10Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11004v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10997v1",
          "title": "Multi-Task Reinforcement Learning of Drone Aerobatics by Exploiting Geometric Symmetries",
          "summary": "Flight control for autonomous micro aerial vehicles (MAVs) is evolving from steady flight near equilibrium points toward more aggressive aerobatic maneuvers, such as flips, rolls, and Power Loop. Although reinforcement learning (RL) has shown great potential in these tasks, conventional RL methods often suffer from low data efficiency and limited generalization. This challenge becomes more pronounced in multi-task scenarios where a single policy is required to master multiple maneuvers. In this paper, we propose a novel end-to-end multi-task reinforcement learning framework, called GEAR (Geometric Equivariant Aerobatics Reinforcement), which fully exploits the inherent SO(2) rotational symmetry in MAV dynamics and explicitly incorporates this property into the policy network architecture. By integrating an equivariant actor network, FiLM-based task modulation, and a multi-head critic, GEAR achieves both efficiency and flexibility in learning diverse aerobatic maneuvers, enabling a data-efficient, robust, and unified framework for aerobatic control. GEAR attains a 98.85\\% success rate across various aerobatic tasks, significantly outperforming baseline methods. In real-world experiments, GEAR demonstrates stable execution of multiple maneuvers and the capability to combine basic motion primitives to complete complex aerobatics.",
          "authors": [
            "Zhanyu Guo",
            "Zikang Yin",
            "Guobin Zhu",
            "Shiliang Guo",
            "Shiyu Zhao"
          ],
          "published": "2026-02-11T16:21:48Z",
          "updated": "2026-02-11T16:21:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10997v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10983v2",
          "title": "Scaling World Model for Hierarchical Manipulation Policies",
          "summary": "Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \\our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \\our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \\href{https://vista-wm.github.io/}{https://vista-wm.github.io}",
          "authors": [
            "Qian Long",
            "Yueze Wang",
            "Jiaxi Song",
            "Junbo Zhang",
            "Peiyan Li",
            "Wenxuan Wang",
            "Yuqi Wang",
            "Haoyang Li",
            "Shaoxuan Xie",
            "Guocai Yao",
            "Hanbo Zhang",
            "Xinlong Wang",
            "Zhongyuan Wang",
            "Xuguang Lan",
            "Huaping Liu",
            "Xinghang Li"
          ],
          "published": "2026-02-11T16:12:33Z",
          "updated": "2026-02-12T10:16:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10983v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10980v1",
          "title": "RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation",
          "summary": "VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.",
          "authors": [
            "Yuhao Chen",
            "Zhihao Zhan",
            "Xiaoxin Lin",
            "Zijian Song",
            "Hao Liu",
            "Qinhan Lyu",
            "Yubo Zu",
            "Xiao Chen",
            "Zhiyuan Liu",
            "Tao Pu",
            "Tianshui Chen",
            "Keze Wang",
            "Liang Lin",
            "Guangrun Wang"
          ],
          "published": "2026-02-11T16:08:30Z",
          "updated": "2026-02-11T16:08:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10980v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10963v1",
          "title": "Lie Group Variational Integrator for the Geometrically Exact Rod with Circular Cross-Section Incorporating Cross-Sectional Deformation",
          "summary": "In this paper, we derive the continuous space-time equations of motion of a three-dimensional geometrically exact rod, or the Cosserat rod, incorporating planar cross-sectional deformation. We then adopt the Lie group variational integrator technique to obtain a discrete model of the rod incorporating both rotational motion and cross-sectional deformation as well. The resulting discrete model possesses several desirable features: it ensures volume conservation of the discrete elements by considering cross-sectional deformation through a local dilatation factor, it demonstrates the beneficial properties associated with the variational integrator technique, such as the preservation of the rotational configuration, and energy conservation with a bounded error. An exhaustive set of numerical results under various initial conditions of the rod demonstrates the efficacy of the model in replicating the physics of the system.",
          "authors": [
            "Srishti Siddharth",
            "Vivek Natarajan",
            "Ravi N. Banavar"
          ],
          "published": "2026-02-11T15:54:59Z",
          "updated": "2026-02-11T15:54:59Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO",
            "math.NA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10963v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10961v3",
          "title": "Stability Analysis of Geometric Control for a Canonical Class of Underactuated Aerial Vehicles with Spurious Forces",
          "summary": "Standard geometric control relies on force-moment decoupling, an assumption that breaks down in many aerial platforms due to spurious forces naturally induced by control moments. While strategies for such coupled systems have been validated experimentally, a rigorous theoretical certification of their stability is currently missing. This work fills this gap by providing the first formal stability analysis for a generic class of floating rigid bodies subject to spurious forces. We introduce a canonical model and construct a Lyapunov-based proof establishing local exponential stability of the hovering equilibrium. Crucially, the analysis explicitly addresses the structural challenges - specifically the induced non-minimum-phase behavior - that prevent the application of standard cascade arguments.",
          "authors": [
            "Simone Orelli",
            "Mirko Mizzoni",
            "Antonio Franchi"
          ],
          "published": "2026-02-11T15:51:21Z",
          "updated": "2026-02-19T09:46:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10961v3",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10946v1",
          "title": "Developing Neural Network-Based Gaze Control Systems for Social Robots",
          "summary": "During multi-party interactions, gaze direction is a key indicator of interest and intent, making it essential for social robots to direct their attention appropriately. Understanding the social context is crucial for robots to engage effectively, predict human intentions, and navigate interactions smoothly. This study aims to develop an empirical motion-time pattern for human gaze behavior in various social situations (e.g., entering, leaving, waving, talking, and pointing) using deep neural networks based on participants' data. We created two video clips-one for a computer screen and another for a virtual reality headset-depicting different social scenarios. Data were collected from 30 participants: 15 using an eye-tracker and 15 using an Oculus Quest 1 headset. Deep learning models, specifically Long Short-Term Memory (LSTM) and Transformers, were used to analyze and predict gaze patterns. Our models achieved 60% accuracy in predicting gaze direction in a 2D animation and 65% accuracy in a 3D animation. Then, the best model was implemented onto the Nao robot; and 36 new participants evaluated its performance. The feedback indicated overall satisfaction, with those experienced in robotics rating the models more favorably.",
          "authors": [
            "Ramtin Tabatabaei",
            "Alireza Taheri"
          ],
          "published": "2026-02-11T15:30:39Z",
          "updated": "2026-02-11T15:30:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10946v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10943v1",
          "title": "Towards Learning a Generalizable 3D Scene Representation from 2D Observations",
          "summary": "We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.",
          "authors": [
            "Martin Gromniak",
            "Jan-Gerrit Habekost",
            "Sebastian Kamp",
            "Sven Magg",
            "Stefan Wermter"
          ],
          "published": "2026-02-11T15:22:41Z",
          "updated": "2026-02-11T15:22:41Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10943v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10942v1",
          "title": "Design, Development, and Use of Maya Robot as an Assistant for the Therapy/Education of Children with Cancer: a Pilot Study",
          "summary": "This study centers around the design and implementation of the Maya Robot, a portable elephant-shaped social robot, intended to engage with children undergoing cancer treatment. Initial efforts were devoted to enhancing the robot's facial expression recognition accuracy, achieving a 98% accuracy through deep neural networks. Two subsequent preliminary exploratory experiments were designed to advance the study's objectives. The first experiment aimed to compare pain levels experienced by children during the injection process, with and without the presence of the Maya robot. Twenty-five children, aged 4 to 9, undergoing cancer treatment participated in this counterbalanced study. The paired T-test results revealed a significant reduction in perceived pain when the robot was actively present in the injection room. The second experiment sought to assess perspectives of hospitalized children and their mothers during engagement with Maya through a game. Forty participants, including 20 children aged 4 to 9 and their mothers, were involved. Post Human-Maya Interactions, UTAUT questionnaire results indicated that children experienced significantly less anxiety than their parents during the interaction and game play. Notably, children exhibited higher trust levels in both the robot and the games, presenting a statistically significant difference in trust levels compared to their parents (P-value < 0.05). This preliminary exploratory study highlights the positive impact of utilizing Maya as an assistant for therapy/education in a clinical setting, particularly benefiting children undergoing cancer treatment. The findings underscore the potential of social robots in pediatric healthcare contexts, emphasizing improved pain management and emotional well-being among young patients.",
          "authors": [
            "Alireza Taheri",
            "Minoo Alemi",
            "Elham Ranjkar",
            "Raman Rafatnejad",
            "Ali F. Meghdari"
          ],
          "published": "2026-02-11T15:20:41Z",
          "updated": "2026-02-11T15:20:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10942v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10910v1",
          "title": "Safe mobility support system using crowd mapping and avoidance route planning using VLM",
          "summary": "Autonomous mobile robots offer promising solutions for labor shortages and increased operational efficiency. However, navigating safely and effectively in dynamic environments, particularly crowded areas, remains challenging. This paper proposes a novel framework that integrates Vision-Language Models (VLM) and Gaussian Process Regression (GPR) to generate dynamic crowd-density maps (``Abstraction Maps'') for autonomous robot navigation. Our approach utilizes VLM's capability to recognize abstract environmental concepts, such as crowd densities, and represents them probabilistically via GPR. Experimental results from real-world trials on a university campus demonstrated that robots successfully generated routes avoiding both static obstacles and dynamic crowds, enhancing navigation safety and adaptability.",
          "authors": [
            "Sena Saito",
            "Kenta Tabata",
            "Renato Miyagusuku",
            "Koichi Ozaki"
          ],
          "published": "2026-02-11T14:47:51Z",
          "updated": "2026-02-11T14:47:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10910v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10904v1",
          "title": "Biomimetic Mantaray robot toward the underwater autonomous -- Experimental verification of swimming and diving by flapping motion -",
          "summary": "This study presents the development and experimental verification of a biomimetic manta ray robot for underwater autonomous exploration. Inspired by manta rays, the robot uses flapping motion for propulsion to minimize seabed disturbance and enhance efficiency compared to traditional screw propulsion. The robot features pectoral fins driven by servo motors and a streamlined control box to reduce fluid resistance. The control system, powered by a Raspberry Pi 3B, includes an IMU and pressure sensor for real-time monitoring and control. Experiments in a pool assessed the robot's swimming and diving capabilities. Results show stable swimming and diving motions with PD control. The robot is suitable for applications in environments like aquariums and fish nurseries, requiring minimal disturbance and efficient maneuverability. Our findings demonstrate the potential of bio-inspired robotic designs to improve ecological monitoring and underwater exploration.",
          "authors": [
            "Kenta Tabata",
            "Ryosuke Oku",
            "Jun Ito",
            "Renato Miyagusuku",
            "Koichi Ozaki"
          ],
          "published": "2026-02-11T14:29:08Z",
          "updated": "2026-02-11T14:29:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10904v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10793v1",
          "title": "Semi-Supervised Cross-Domain Imitation Learning",
          "summary": "Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.",
          "authors": [
            "Li-Min Chu",
            "Kai-Siang Ma",
            "Ming-Hong Chen",
            "Ping-Chun Hsieh"
          ],
          "published": "2026-02-11T12:38:08Z",
          "updated": "2026-02-11T12:38:08Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10793v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10771v1",
          "title": "From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?",
          "summary": "Cyclists often encounter safety-critical situations in urban traffic, highlighting the need for assistive systems that support safe and informed decision-making. Recently, vision-language models (VLMs) have demonstrated strong performance on autonomous driving benchmarks, suggesting their potential for general traffic understanding and navigation-related reasoning. However, existing evaluations are predominantly vehicle-centric and fail to assess perception and reasoning from a cyclist-centric viewpoint. To address this gap, we introduce CyclingVQA, a diagnostic benchmark designed to probe perception, spatio-temporal understanding, and traffic-rule-to-lane reasoning from a cyclist's perspective. Evaluating 31+ recent VLMs spanning general-purpose, spatially enhanced, and autonomous-driving-specialized models, we find that current models demonstrate encouraging capabilities, while also revealing clear areas for improvement in cyclist-centric perception and reasoning, particularly in interpreting cyclist-specific traffic cues and associating signs with the correct navigational lanes. Notably, several driving-specialized models underperform strong generalist VLMs, indicating limited transfer from vehicle-centric training to cyclist-assistive scenarios. Finally, through systematic error analysis, we identify recurring failure modes to guide the development of more effective cyclist-assistive intelligent systems.",
          "authors": [
            "Krishna Kanth Nakka",
            "Vedasri Nakka"
          ],
          "published": "2026-02-11T12:01:37Z",
          "updated": "2026-02-11T12:01:37Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10771v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10719v1",
          "title": "From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving",
          "summary": "Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer's confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.",
          "authors": [
            "Sining Ang",
            "Yuguang Yang",
            "Chenxu Dang",
            "Canyu Chen",
            "Cheng Chi",
            "Haiyan Liu",
            "Xuanyao Mao",
            "Jason Bao",
            "Xuliang",
            "Bingchuan Sun",
            "Yan Wang"
          ],
          "published": "2026-02-11T10:25:05Z",
          "updated": "2026-02-11T10:25:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10719v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10717v1",
          "title": "Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation",
          "summary": "Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.",
          "authors": [
            "Songen Gu",
            "Yunuo Cai",
            "Tianyu Wang",
            "Simo Wu",
            "Yanwei Fu"
          ],
          "published": "2026-02-11T10:23:52Z",
          "updated": "2026-02-11T10:23:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10717v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10704v1",
          "title": "(MGS)$^2$-Net: Unifying Micro-Geometric Scale and Macro-Geometric Structure for Cross-View Geo-Localization",
          "summary": "Cross-view geo-localization (CVGL) is pivotal for GNSS-denied UAV navigation but remains brittle under the drastic geometric misalignment between oblique aerial views and orthographic satellite references. Existing methods predominantly operate within a 2D manifold, neglecting the underlying 3D geometry where view-dependent vertical facades (macro-structure) and scale variations (micro-scale) severely corrupt feature alignment. To bridge this gap, we propose (MGS)$^2$, a geometry-grounded framework. The core of our innovation is the Macro-Geometric Structure Filtering (MGSF) module. Unlike pixel-wise matching sensitive to noise, MGSF leverages dilated geometric gradients to physically filter out high-frequency facade artifacts while enhancing the view-invariant horizontal plane, directly addressing the domain shift. To guarantee robust input for this structural filtering, we explicitly incorporate a Micro-Geometric Scale Adaptation (MGSA) module. MGSA utilizes depth priors to dynamically rectify scale discrepancies via multi-branch feature fusion. Furthermore, a Geometric-Appearance Contrastive Distillation (GACD) loss is designed to strictly discriminate against oblique occlusions. Extensive experiments demonstrate that (MGS)$^2$ achieves state-of-the-art performance, recording a Recall@1 of 97.5\\% on University-1652 and 97.02\\% on SUES-200. Furthermore, the framework exhibits superior cross-dataset generalization against geometric ambiguity. The code is available at: \\href{https://github.com/GabrielLi1473/MGS-Net}{https://github.com/GabrielLi1473/MGS-Net}.",
          "authors": [
            "Minglei Li",
            "Mengfan He",
            "Chao Chen",
            "Ziyang Meng"
          ],
          "published": "2026-02-11T10:03:31Z",
          "updated": "2026-02-11T10:03:31Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10704v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10703v1",
          "title": "Omnidirectional Dual-Arm Aerial Manipulator with Proprioceptive Contact Localization for Landing on Slanted Roofs",
          "summary": "Operating drones in urban environments often means they need to land on rooftops, which can have different geometries and surface irregularities. Accurately detecting roof inclination using conventional sensing methods, such as vision-based or acoustic techniques, can be unreliable, as measurement quality is strongly influenced by external factors including weather conditions and surface materials. To overcome these challenges, we propose a novel unmanned aerial manipulator morphology featuring a dual-arm aerial manipulator with an omnidirectional 3D workspace and extended reach. Building on this design, we develop a proprioceptive contact detection and contact localization strategy based on a momentum-based torque observer. This enables the UAM to infer the inclination of slanted surfaces blindly - through physical interaction - prior to touchdown. We validate the approach in flight experiments, demonstrating robust landings on surfaces with inclinations of up to 30.5 degrees and achieving an average surface inclination estimation error of 2.87 degrees over 9 experiments at different incline angles.",
          "authors": [
            "Martijn B. J. Brummelhuis",
            "Nathan F. Lepora",
            "Salua Hamaza"
          ],
          "published": "2026-02-11T10:03:17Z",
          "updated": "2026-02-11T10:03:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10703v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10702v1",
          "title": "A Unified Experimental Architecture for Informative Path Planning: from Simulation to Deployment with GuadalPlanner",
          "summary": "The evaluation of informative path planning algorithms for autonomous vehicles is often hindered by fragmented execution pipelines and limited transferability between simulation and real-world deployment. This paper introduces a unified architecture that decouples high-level decision-making from vehicle-specific control, enabling algorithms to be evaluated consistently across different abstraction levels without modification. The proposed architecture is realized through GuadalPlanner, which defines standardized interfaces between planning, sensing, and vehicle execution. It is an open and extensible research tool that supports discrete graph-based environments and interchangeable planning strategies, and is built upon widely adopted robotics technologies, including ROS2, MAVLink, and MQTT. Its design allows the same algorithmic logic to be deployed in fully simulated environments, software-in-the-loop configurations, and physical autonomous vehicles using an identical execution pipeline. The approach is validated through a set of experiments, including real-world deployment on an autonomous surface vehicle performing water quality monitoring with real-time sensor feedback.",
          "authors": [
            "Alejandro Mendoza Barrionuevo",
            "Dame Seck Diop",
            "Alejandro Casado Pérez",
            "Daniel Gutiérrez Reina",
            "Sergio L. Toral Marín",
            "Samuel Yanes Luis"
          ],
          "published": "2026-02-11T10:02:31Z",
          "updated": "2026-02-11T10:02:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG",
            "cs.SE"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10702v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10688v1",
          "title": "3D-Printed Anisotropic Soft Magnetic Coating for Directional Rolling of a Magnetically Actuated Capsule Robot",
          "summary": "Capsule robots are promising tools for minimally invasive diagnostics and therapy, with applications from gastrointestinal endoscopy to targeted drug delivery and biopsy sampling. Conventional magnetic capsule robots embed bulky permanent magnets at both ends, reducing the usable cavity by about 10-20 mm and limiting integration of functional modules. We propose a compact, 3D-printed soft capsule robot with a magnetic coating that replaces internal magnets, enabling locomotion via a thin, functional shell while preserving the entire interior cavity as a continuous volume for medical payloads. The compliant silicone-magnetic composite also improves swallowability, even with a slightly larger capsule size. Magnetostatic simulations and experiments confirm that programmed NSSN/SNNS pole distributions provide strong anisotropy and reliable torque generation, enabling stable bidirectional rolling, omnidirectional steering, climbing on 7.5 degree inclines, and traversal of 5 mm protrusions. Rolling motion is sustained when the magnetic field at the capsule reaches at least 0.3 mT, corresponding to an effective actuation depth of 30 mm in our setup. Future work will optimize material composition, coating thickness, and magnetic layouts to enhance force output and durability, while next-generation robotic-arm-based field generators with closed-loop feedback will address nonlinearities and expand maneuverability. Together, these advances aim to transition coating-based capsule robots toward reliable clinical deployment and broaden their applications in minimally invasive diagnostics and therapy.",
          "authors": [
            "Jin Zhou",
            "Chongxun Wang",
            "Zikang Shen",
            "Fangzhou Xia"
          ],
          "published": "2026-02-11T09:43:03Z",
          "updated": "2026-02-11T09:43:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10688v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10686v1",
          "title": "Free-Flying Crew Cooperative Robots on the ISS: A Joint Review of Astrobee, CIMON, and Int-Ball Operations",
          "summary": "Intra-vehicular free-flying robots are anticipated to support various work in human spaceflight while working side-by-side with astronauts. Such example of robots includes NASA's Astrobee, DLR's CIMON, and JAXA's Int-Ball, which are deployed on the International Space Station. This paper presents the first joint analyses of these robot's shared experiences, co-authored by their development and operation team members. Despite the different origins and design philosophies, the development and operations of these platforms encountered various convergences. Hence, this paper presents a detailed overview of these robots, presenting their objectives, design, and onboard operations. Hence, joint lessons learned across the lifecycle are presented, from design to on-orbit operations. These lessons learned are anticipated to serve for future development and research as design recommendations.",
          "authors": [
            "Seiko Piotr Yamaguchi",
            "Andres Mora Vargas",
            "Till Eisenberg",
            "Christian Rogon",
            "Tatsuya Yamamoto",
            "Shona Inoue",
            "Christoph Kössl",
            "Brian Coltin",
            "Trey Smith",
            "Jose V. Benavides"
          ],
          "published": "2026-02-11T09:40:02Z",
          "updated": "2026-02-11T09:40:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10686v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10655v2",
          "title": "Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software",
          "summary": "Autonomous Underwater Robots (AURs) operate in challenging underwater environments, including low visibility and harsh water conditions. Such conditions present challenges for software engineers developing perception modules for the AUR software. To successfully carry out these tasks, deep learning has been incorporated into the AUR software to support its operations. However, the unique challenges of underwater environments pose difficulties for deep learning models, which often rely on labeled data that is scarce and noisy. This may undermine the trustworthiness of AUR software that relies on perception modules. Vision-Language Models (VLMs) offer promising solutions for AUR software as they generalize to unseen objects and remain robust in noisy conditions by inferring information from contextual cues. Despite this potential, their performance and uncertainty in underwater environments remain understudied from a software engineering perspective. Motivated by the needs of an industrial partner in assurance and risk management for maritime systems to assess the potential use of VLMs in this context, we present an empirical evaluation of VLM-based perception modules within the AUR software. We assess their ability to detect underwater trash by computing performance, uncertainty, and their relationship, to enable software engineers to select appropriate VLMs for their AUR software.",
          "authors": [
            "Muhammad Yousaf",
            "Aitor Arrieta",
            "Shaukat Ali",
            "Paolo Arcaini",
            "Shuai Wang"
          ],
          "published": "2026-02-11T08:59:44Z",
          "updated": "2026-02-13T08:33:37Z",
          "primary_category": "cs.SE",
          "categories": [
            "cs.SE",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10655v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11214v1",
          "title": "DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration",
          "summary": "Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.",
          "authors": [
            "Manuel Hetzel",
            "Kerim Turacan",
            "Hannes Reichert",
            "Konrad Doll",
            "Bernhard Sick"
          ],
          "published": "2026-02-11T08:59:33Z",
          "updated": "2026-02-11T08:59:33Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11214v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15893v1",
          "title": "Statistical-Geometric Degeneracy in UAV Search: A Physics-Aware Asymmetric Filtering Approach",
          "summary": "Post-disaster survivor localization using Unmanned Aerial Vehicles (UAVs) faces a fundamental physical challenge: the prevalence of Non-Line-of-Sight (NLOS) propagation in collapsed structures. Unlike standard Gaussian noise, signal reflection from debris introduces strictly non-negative ranging biases. Existing robust estimators, typically designed with symmetric loss functions (e.g., Huber or Tukey), implicitly rely on the assumption of error symmetry. Consequently, they experience a theoretical mismatch in this regime, leading to a phenomenon we formally identify as Statistical-Geometric Degeneracy (SGD)-a state where the estimator stagnates due to the coupling of persistent asymmetric bias and limited observation geometry. While emerging data-driven approaches offer alternatives, they often struggle with the scarcity of training data and the sim-to-real gap inherent in unstructured disaster zones. In this work, we propose a physically-grounded solution, the AsymmetricHuberEKF, which explicitly incorporates the non-negative physical prior of NLOS biases via a derived asymmetric loss function. Theoretically, we show that standard symmetric filters correspond to a degenerate case of our framework where the physical constraint is relaxed. Furthermore, we demonstrate that resolving SGD requires not just a robust filter, but specific bilateral information, which we achieve through a co-designed active sensing strategy. Validated in a 2D nadir-view scanning scenario, our approach significantly accelerates convergence compared to symmetric baselines, offering a resilient building block for search operations where data is scarce and geometry is constrained.",
          "authors": [
            "Zhiyuan Ren",
            "Yudong Fang",
            "Tao Zhang",
            "Wenchi Cheng",
            "Ben Lan"
          ],
          "published": "2026-02-11T08:33:56Z",
          "updated": "2026-02-11T08:33:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.IT",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15893v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10610v1",
          "title": "Pitch Angle Control of a Magnetically Actuated Capsule Robot with Nonlinear FEA-based MPC and EKF Multisensory Fusion",
          "summary": "Magnetically actuated capsule robots promise minimally invasive diagnosis and therapy in the gastrointestinal (GI) tract, but existing systems largely neglect control of capsule pitch, a degree of freedom critical for contact-rich interaction with inclined gastric walls. This paper presents a nonlinear, model-based framework for magnetic pitch control of an ingestible capsule robot actuated by a four-coil electromagnetic array. Angle-dependent magnetic forces and torques acting on embedded permanent magnets are characterized using three-dimensional finite-element simulations and embedded as lookup tables in a control-oriented rigid-body pitching model with rolling contact and actuator dynamics. A constrained model predictive controller (MPC) is designed to regulate pitch while respecting hardware-imposed current and slew-rate limits. Experiments on a compliant stomach-inspired surface demonstrate robust pitch reorientation from both horizontal and upright configurations, achieving about three to five times faster settling and reduced oscillatory motion than on-off control. Furthermore, an extended Kalman filter (EKF) fusing inertial sensing with intermittent visual measurements enables stable closed-loop control when the camera update rate is reduced from 30 Hz to 1 Hz, emulating clinically realistic imaging constraints. These results establish finite-element-informed MPC with sensor fusion as a scalable strategy for pitch regulation, controlled docking, and future multi-degree-of-freedom capsule locomotion.",
          "authors": [
            "Chongxun Wang",
            "Zikang Shen",
            "Apoorav Rathore",
            "Akanimoh Udombeh",
            "Harrison Teng",
            "Fangzhou Xia"
          ],
          "published": "2026-02-11T07:59:52Z",
          "updated": "2026-02-11T07:59:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10610v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10594v1",
          "title": "Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning",
          "summary": "Imitation Learning (IL) enables robots to learn complex skills from demonstrations without explicit task modeling, but it typically requires large amounts of demonstrations, creating significant collection costs. Prior work has investigated using flow as an intermediate representation to enable the use of human videos as a substitute, thereby reducing the amount of required robot demonstrations. However, most prior work has focused on the flow, either on the object or on specific points of the robot/hand, which cannot describe the motion of interaction. Meanwhile, relying on flow to achieve generalization to scenarios observed only in human videos remains limited, as flow alone cannot capture precise motion details. Furthermore, conditioning on scene observation to produce precise actions may cause the flow-conditioned policy to overfit to training tasks and weaken the generalization indicated by the flow. To address these gaps, we propose SFCrP, which includes a Scene Flow prediction model for Cross-embodiment learning (SFCr) and a Flow and Cropped point cloud conditioned Policy (FCrP). SFCr learns from both robot and human videos and predicts any point trajectories. FCrP follows the general flow motion and adjusts the action based on observations for precision tasks. Our method outperforms SOTA baselines across various real-world task settings, while also exhibiting strong spatial and instance generalization to scenarios seen only in human videos.",
          "authors": [
            "Runze Tang",
            "Penny Sweetser"
          ],
          "published": "2026-02-11T07:32:27Z",
          "updated": "2026-02-11T07:32:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10594v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13329v1",
          "title": "HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving",
          "summary": "Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation. Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.",
          "authors": [
            "Yiru Wang",
            "Zichong Gu",
            "Yu Gao",
            "Anqing Jiang",
            "Zhigang Sun",
            "Shuo Wang",
            "Yuwen Heng",
            "Hao Sun"
          ],
          "published": "2026-02-11T07:08:33Z",
          "updated": "2026-02-11T07:08:33Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13329v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10561v1",
          "title": "Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots",
          "summary": "This paper presents a closed-loop automation framework for heterogeneous modular robots, covering the full pipeline from morphological construction to adaptive control. In this framework, a mobile manipulator handles heterogeneous functional modules including structural, joint, and wheeled modules to dynamically assemble diverse robot configurations and provide them with immediate locomotion capability. To address the state-space explosion in large-scale heterogeneous reconfiguration, we propose a hierarchical planner: the high-level planner uses a bidirectional heuristic search with type-penalty terms to generate module-handling sequences, while the low level planner employs A* search to compute optimal execution trajectories. This design effectively decouples discrete configuration planning from continuous motion execution. For adaptive motion generation of unknown assembled configurations, we introduce a GPU accelerated Annealing-Variance Model Predictive Path Integral (MPPI) controller. By incorporating a multi stage variance annealing strategy to balance global exploration and local convergence, the controller enables configuration-agnostic, real-time motion control. Large scale simulations show that the type-penalty term is critical for planning robustness in heterogeneous scenarios. Moreover, the greedy heuristic produces plans with lower physical execution costs than the Hungarian heuristic. The proposed annealing-variance MPPI significantly outperforms standard MPPI in both velocity tracking accuracy and control frequency, achieving real time control at 50 Hz. The framework validates the full-cycle process, including module assembly, robot merging and splitting, and dynamic motion generation.",
          "authors": [
            "Chongxi Meng",
            "Da Zhao",
            "Yifei Zhao",
            "Minghao Zeng",
            "Yanmin Zhou",
            "Zhipeng Wang",
            "Bin He"
          ],
          "published": "2026-02-11T06:18:04Z",
          "updated": "2026-02-11T06:18:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10561v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10556v2",
          "title": "LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer",
          "summary": "A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.",
          "authors": [
            "Lihan Zha",
            "Asher J. Hancock",
            "Mingtong Zhang",
            "Tenny Yin",
            "Yixuan Huang",
            "Dhruv Shah",
            "Allen Z. Ren",
            "Anirudha Majumdar"
          ],
          "published": "2026-02-11T06:09:11Z",
          "updated": "2026-02-15T15:32:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10556v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10555v1",
          "title": "An Ontology-driven Dynamic Knowledge Base for Uninhabited Ground Vehicles",
          "summary": "In this paper, the concept of Dynamic Contextual Mission Data (DCMD) is introduced to develop an ontology-driven dynamic knowledge base for Uninhabited Ground Vehicles (UGVs) at the tactical edge. The dynamic knowledge base with DCMD is added to the UGVs to: support enhanced situation awareness; improve autonomous decision making; and facilitate agility within complex and dynamic environments. As UGVs are heavily reliant on the a priori information added pre-mission, unexpected occurrences during a mission can cause identification ambiguities and require increased levels of user input. Updating this a priori information with contextual information can help UGVs realise their full potential. To address this, the dynamic knowledge base was designed using an ontology-driven representation, supported by near real-time information acquisition and analysis, to provide in-mission on-platform DCMD updates. This was implemented on a team of four UGVs that executed a laboratory based surveillance mission. The results showed that the ontology-driven dynamic representation of the UGV operational environment was machine actionable, producing contextual information to support a successful and timely mission, and contributed directly to the situation awareness.",
          "authors": [
            "Hsan Sandar Win",
            "Andrew Walters",
            "Cheng-Chew Lim",
            "Daniel Webber",
            "Seth Leslie",
            "Tan Doan"
          ],
          "published": "2026-02-11T06:06:18Z",
          "updated": "2026-02-11T06:06:18Z",
          "primary_category": "cs.MA",
          "categories": [
            "cs.MA",
            "cs.DB",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10555v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10547v1",
          "title": "ReSPEC: A Framework for Online Multispectral Sensor Reconfiguration in Dynamic Environments",
          "summary": "Multi-sensor fusion is central to robust robotic perception, yet most existing systems operate under static sensor configurations, collecting all modalities at fixed rates and fidelity regardless of their situational utility. This rigidity wastes bandwidth, computation, and energy, and prevents systems from prioritizing sensors under challenging conditions such as poor lighting or occlusion. Recent advances in reinforcement learning (RL) and modality-aware fusion suggest the potential for adaptive perception, but prior efforts have largely focused on re-weighting features at inference time, ignoring the physical cost of sensor data collection. We introduce a framework that unifies sensing, learning, and actuation into a closed reconfiguration loop. A task-specific detection backbone extracts multispectral features (e.g. RGB, IR, mmWave, depth) and produces quantitative contribution scores for each modality. These scores are passed to an RL agent, which dynamically adjusts sensor configurations, including sampling frequency, resolution, sensing range, and etc., in real time. Less informative sensors are down-sampled or deactivated, while critical sensors are sampled at higher fidelity as environmental conditions evolve. We implement and evaluate this framework on a mobile rover, showing that adaptive control reduces GPU load by 29.3\\% with only a 5.3\\% accuracy drop compared to a heuristic baseline. These results highlight the potential of resource-aware adaptive sensing for embedded robotic platforms.",
          "authors": [
            "Yanchen Liu",
            "Yuang Fan",
            "Minghui Zhao",
            "Xiaofan Jiang"
          ],
          "published": "2026-02-11T05:39:24Z",
          "updated": "2026-02-11T05:39:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10547v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10514v1",
          "title": "Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning",
          "summary": "While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.",
          "authors": [
            "Shihao Dong",
            "Yeke Chen",
            "Zeren Luo",
            "Jiahui Zhang",
            "Bowen Xu",
            "Jinghan Lin",
            "Yimin Han",
            "Ji Ma",
            "Zhiyou Yu",
            "Yudong Zhao",
            "Peng Lu"
          ],
          "published": "2026-02-11T04:28:04Z",
          "updated": "2026-02-11T04:28:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10514v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10503v1",
          "title": "Towards Long-Lived Robots: Continual Learning VLA Models via Reinforcement Fine-Tuning",
          "summary": "Pretrained on large-scale and diverse datasets, VLA models demonstrate strong generalization and adaptability as general-purpose robotic policies. However, Supervised Fine-Tuning (SFT), which serves as the primary mechanism for adapting VLAs to downstream domains, requires substantial amounts of task-specific data and is prone to catastrophic forgetting. To address these limitations, we propose LifeLong-RFT, a simple yet effective Reinforcement Fine-Tuning (RFT) strategy for VLA models independent of online environmental feedback and pre-trained reward models. By integrating chunking-level on-policy reinforcement learning with the proposed Multi-Dimensional Process Reward (MDPR) mechanism, LifeLong-RFT quantifies the heterogeneous contributions of intermediate action chunks across three dimensions to facilitate policy optimization. Specifically, (1) the Quantized Action Consistency Reward (QACR) ensures accurate action prediction within the discrete action space; (2) the Continuous Trajectory Alignment Reward (CTAR) aligns decoded continuous action chunks with reference trajectories to ensure precise control; (3) the Format Compliance Reward (FCR) guarantees the structural validity of outputs. Comprehensive experiments across SimplerEnv, LIBERO, and real-world tasks demonstrate that LifeLong-RFT exhibits strong performance in multi-task learning. Furthermore, for continual learning on the LIBERO benchmark, our method achieves a 22% gain in average success rate over SFT, while effectively adapting to new tasks using only 20% of the training data. Overall, our method provides a promising post-training paradigm for VLAs.",
          "authors": [
            "Yuan Liu",
            "Haoran Li",
            "Shuai Tian",
            "Yuxing Qin",
            "Yuhui Chen",
            "Yupeng Zheng",
            "Yongzhen Huang",
            "Dongbin Zhao"
          ],
          "published": "2026-02-11T04:05:03Z",
          "updated": "2026-02-11T04:05:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10503v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10492v1",
          "title": "End-to-End LiDAR optimization for 3D point cloud registration",
          "summary": "LiDAR sensors are a key modality for 3D perception, yet they are typically designed independently of downstream tasks such as point cloud registration. Conventional registration operates on pre-acquired datasets with fixed LiDAR configurations, leading to suboptimal data collection and significant computational overhead for sampling, noise filtering, and parameter tuning. In this work, we propose an adaptive LiDAR sensing framework that dynamically adjusts sensor parameters, jointly optimizing LiDAR acquisition and registration hyperparameters. By integrating registration feedback into the sensing loop, our approach optimally balances point density, noise, and sparsity, improving registration accuracy and efficiency. Evaluations in the CARLA simulation demonstrate that our method outperforms fixed-parameter baselines while retaining generalization abilities, highlighting the potential of adaptive LiDAR for autonomous perception and robotic applications.",
          "authors": [
            "Siddhant Katyan",
            "Marc-André Gardner",
            "Jean-François Lalonde"
          ],
          "published": "2026-02-11T03:51:10Z",
          "updated": "2026-02-11T03:51:10Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10492v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10417v1",
          "title": "RadarEye: Robust Liquid Level Tracking Using mmWave Radar in Robotic Pouring",
          "summary": "Transparent liquid manipulation in robotic pouring remains challenging for perception systems: specular/refraction effects and lighting variability degrade visual cues, undermining reliable level estimation. To address this challenge, we introduce RadarEye, a real-time mmWave radar signal processing pipeline for robust liquid level estimation and tracking during the whole pouring process. RadarEye integrates (i) a high-resolution range-angle beamforming module for liquid level sensing and (ii) a physics-informed mid-pour tracker that suppresses multipath to maintain lock on the liquid surface despite stream-induced clutter and source container reflections. The pipeline delivers sub-millisecond latency. In real-robot water-pouring experiments, RadarEye achieves a 0.35 cm median absolute height error at 0.62 ms per update, substantially outperforming vision and ultrasound baselines.",
          "authors": [
            "Hongyu Deng",
            "He Chen"
          ],
          "published": "2026-02-11T01:57:13Z",
          "updated": "2026-02-11T01:57:13Z",
          "primary_category": "eess.SP",
          "categories": [
            "eess.SP",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10417v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10399v1",
          "title": "LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies",
          "summary": "Recent advances in legged locomotion learning are still dominated by the utilization of geometric representations of the environment, limiting the robot's capability to respond to higher-level semantics such as human instructions. To address this limitation, we propose a novel approach that integrates high-level commonsense reasoning from foundation models into the process of legged locomotion adaptation. Specifically, our method utilizes a pre-trained large language model to synthesize an instruction-grounded skill database tailored for legged robots. A pre-trained vision-language model is employed to extract high-level environmental semantics and ground them within the skill database, enabling real-time skill advisories for the robot. To facilitate versatile skill control, we train a style-conditioned policy capable of generating diverse and robust locomotion skills with high fidelity to specified styles. To the best of our knowledge, this is the first work to demonstrate real-time adaptation of legged locomotion using high-level reasoning from environmental semantics and instructions with instruction-following accuracy of up to 87% without the need for online query to on-the-cloud foundation models.",
          "authors": [
            "I Made Aswin Nahrendra",
            "Seunghyun Lee",
            "Dongkyu Lee",
            "Hyun Myung"
          ],
          "published": "2026-02-11T01:00:18Z",
          "updated": "2026-02-11T01:00:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10399v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10365v1",
          "title": "Solving Geodesic Equations with Composite Bernstein Polynomials for Trajectory Planning",
          "summary": "This work presents a trajectory planning method based on composite Bernstein polynomials for autonomous systems navigating complex environments. The method is implemented in a symbolic optimization framework that enables continuous paths and precise control over trajectory shape. Trajectories are planned over a cost surface that encodes obstacles as continuous fields rather than discrete boundaries. Regions near obstacles are assigned higher costs, naturally encouraging the trajectory to maintain a safe distance while still allowing efficient routing through constrained spaces. The use of composite Bernstein polynomials preserves continuity while enabling fine control over local curvature to satisfy geodesic constraints. The symbolic representation supports exact derivatives, improving optimization efficiency. The method applies to both two- and three-dimensional environments and is suitable for ground, aerial, underwater, and space systems. In spacecraft trajectory planning, for example, it enables the generation of continuous, dynamically feasible trajectories with high numerical efficiency, making it well suited for orbital maneuvers, rendezvous and proximity operations, cluttered gravitational environments, and planetary exploration missions with limited onboard computational resources. Demonstrations show that the approach efficiently generates smooth, collision-free paths in scenarios with multiple obstacles, maintaining clearance without extensive sampling or post-processing. The optimization incorporates three constraint types: (1) a Gaussian surface inequality enforcing minimum obstacle clearance; (2) geodesic equations guiding the path along locally efficient directions on the cost surface; and (3) boundary constraints enforcing fixed start and end conditions. The method can serve as a standalone planner or as an initializer for more complex motion planning problems.",
          "authors": [
            "Nick Gorman",
            "Gage MacLin",
            "Maxwell Hammond",
            "Venanzio Cichella"
          ],
          "published": "2026-02-10T23:33:15Z",
          "updated": "2026-02-10T23:33:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10365v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13324v1",
          "title": "Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge",
          "summary": "Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel \"Controlled Input\" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.",
          "authors": [
            "Jesse Barkley",
            "Abraham George",
            "Amir Barati Farimani"
          ],
          "published": "2026-02-10T23:00:19Z",
          "updated": "2026-02-10T23:00:19Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13324v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10305v1",
          "title": "Confounding Robust Continuous Control via Automatic Reward Shaping",
          "summary": "Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.",
          "authors": [
            "Mateo Juliani",
            "Mingxuan Li",
            "Elias Bareinboim"
          ],
          "published": "2026-02-10T21:23:12Z",
          "updated": "2026-02-10T21:23:12Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10305v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10289v1",
          "title": "A Human-in-the-Loop Confidence-Aware Failure Recovery Framework for Modular Robot Policies",
          "summary": "Robots operating in unstructured human environments inevitably encounter failures, especially in robot caregiving scenarios. While humans can often help robots recover, excessive or poorly targeted queries impose unnecessary cognitive and physical workload on the human partner. We present a human-in-the-loop failure-recovery framework for modular robotic policies, where a policy is composed of distinct modules such as perception, planning, and control, any of which may fail and often require different forms of human feedback. Our framework integrates calibrated estimates of module-level uncertainty with models of human intervention cost to decide which module to query and when to query the human. It separates these two decisions: a module selector identifies the module most likely responsible for failure, and a querying algorithm determines whether to solicit human input or act autonomously. We evaluate several module-selection strategies and querying algorithms in controlled synthetic experiments, revealing trade-offs between recovery efficiency, robustness to system and user variables, and user workload. Finally, we deploy the framework on a robot-assisted bite acquisition system and demonstrate, in studies involving individuals with both emulated and real mobility limitations, that it improves recovery success while reducing the workload imposed on users. Our results highlight how explicitly reasoning about both robot uncertainty and human effort can enable more efficient and user-centered failure recovery in collaborative robots. Supplementary materials and videos can be found at: http://emprise.cs.cornell.edu/modularhil",
          "authors": [
            "Rohan Banerjee",
            "Krishna Palempalli",
            "Bohan Yang",
            "Jiaying Fang",
            "Alif Abdullah",
            "Tom Silver",
            "Sarah Dean",
            "Tapomayukh Bhattacharjee"
          ],
          "published": "2026-02-10T21:03:46Z",
          "updated": "2026-02-10T21:03:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10289v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10285v2",
          "title": "Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning",
          "summary": "Autonomous driving requires reasoning about interactions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online trajectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible computational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/.",
          "authors": [
            "Ananya Trivedi",
            "Anjian Li",
            "Mohamed Elnoor",
            "Yusuf Umut Ciftci",
            "Avinash Singh",
            "Jovin D'sa",
            "Sangjae Bae",
            "David Isele",
            "Taskin Padir",
            "Faizan M. Tariq"
          ],
          "published": "2026-02-10T20:57:01Z",
          "updated": "2026-02-14T03:13:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10285v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10234v2",
          "title": "Transforming Policy-Car Swerving for Mitigating Stop-and-Go Traffic Waves: A Practice-Oriented Jam-Absorption Driving Strategy",
          "summary": "Stop-and-go waves, as a major form of freeway traffic congestion, cause severe and long-lasting adverse effects, including reduced traffic efficiency, increased driving risks, and higher vehicle emissions. Amongst the highway traffic management strategies, jam-absorption driving (JAD), in which a dedicated vehicle performs \"slow-in\" and \"fast-out\" maneuvers before being captured by a stop-and-go wave, has been proposed as a potential method for preventing the propagation of such waves. However, most existing JAD strategies remain impractical mainly due to the lack of discussion regarding implementation vehicles and operational conditions. Inspired by real-world observations of police-car swerving behavior, this paper first introduces a Single-Vehicle Two-Detector Jam-Absorption Driving (SVDD-JAD) problem, and then proposes a practical JAD strategy that transforms such behavior into a maneuver capable of suppressing the propagation of an isolated stop-and-go wave. Five key parameters that significantly affect the proposed strategy, namely, JAD speed, inflow traffic speed, wave width, wave speed, and in-wave speed, are identified and systematically analyzed. Using a SUMO-based simulation as an illustrative example, we further demonstrate how these parameters can be measured in practice with two stationary roadside traffic detectors. The results show that the proposed JAD strategy successfully suppresses the propagation of a stop-and-go wave, without triggering a secondary wave. This paper is expected to take a significant step toward making JAD practical, advancing it from a theoretical concept to a feasible and implementable strategy. To promote reproducibility in the transportation domain, we have also open-sourced all the code on our GitHub repository https://github.com/gotrafficgo.",
          "authors": [
            "Zhengbing He"
          ],
          "published": "2026-02-10T19:23:50Z",
          "updated": "2026-02-12T19:15:14Z",
          "primary_category": "physics.soc-ph",
          "categories": [
            "physics.soc-ph",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10234v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10116v1",
          "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
          "summary": "Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.",
          "authors": [
            "Hongchi Xia",
            "Xuan Li",
            "Zhaoshuo Li",
            "Qianli Ma",
            "Jiashu Xu",
            "Ming-Yu Liu",
            "Yin Cui",
            "Tsung-Yi Lin",
            "Wei-Chiu Ma",
            "Shenlong Wang",
            "Shuran Song",
            "Fangyin Wei"
          ],
          "published": "2026-02-10T18:59:55Z",
          "updated": "2026-02-10T18:59:55Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10116v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10116v2",
          "title": "SAGE: Scalable Agentic 3D Scene Generation for Embodied AI",
          "summary": "Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://research.nvidia.com/labs/dir/sage/.",
          "authors": [
            "Hongchi Xia",
            "Xuan Li",
            "Zhaoshuo Li",
            "Qianli Ma",
            "Jiashu Xu",
            "Ming-Yu Liu",
            "Yin Cui",
            "Tsung-Yi Lin",
            "Wei-Chiu Ma",
            "Shenlong Wang",
            "Shuran Song",
            "Fangyin Wei"
          ],
          "published": "2026-02-10T18:59:55Z",
          "updated": "2026-02-20T21:37:26Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10116v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10114v1",
          "title": "Decoupled MPPI-Based Multi-Arm Motion Planning",
          "summary": "Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.",
          "authors": [
            "Dan Evron",
            "Elias Goldsztejn",
            "Ronen I. Brafman"
          ],
          "published": "2026-02-10T18:59:51Z",
          "updated": "2026-02-10T18:59:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10114v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10111v1",
          "title": "Learning Agile Quadrotor Flight in the Real World",
          "summary": "Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.",
          "authors": [
            "Yunfan Ren",
            "Zhiyuan Zhu",
            "Jiaxu Xing",
            "Davide Scaramuzza"
          ],
          "published": "2026-02-10T18:59:28Z",
          "updated": "2026-02-10T18:59:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10111v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10109v1",
          "title": "ST4VLA: Spatially Guided Training for Vision-Language-Action Models",
          "summary": "Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/",
          "authors": [
            "Jinhui Ye",
            "Fangjing Wang",
            "Ning Gao",
            "Junqiu Yu",
            "Yangkun Zhu",
            "Bin Wang",
            "Jinyu Zhang",
            "Weiyang Jin",
            "Yanwei Fu",
            "Feng Zheng",
            "Yilun Chen",
            "Jiangmiao Pang"
          ],
          "published": "2026-02-10T18:59:17Z",
          "updated": "2026-02-10T18:59:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10109v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10106v1",
          "title": "EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration",
          "summary": "Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.",
          "authors": [
            "Modi Shi",
            "Shijia Peng",
            "Jin Chen",
            "Haoran Jiang",
            "Yinghui Li",
            "Di Huang",
            "Ping Luo",
            "Hongyang Li",
            "Li Chen"
          ],
          "published": "2026-02-10T18:59:03Z",
          "updated": "2026-02-10T18:59:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10106v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10105v1",
          "title": "DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos",
          "summary": "Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).",
          "authors": [
            "Juncheng Mu",
            "Sizhe Yang",
            "Yiming Bao",
            "Hojin Bae",
            "Tianming Wei",
            "Linning Xu",
            "Boyi Li",
            "Huazhe Xu",
            "Jiangmiao Pang"
          ],
          "published": "2026-02-10T18:59:02Z",
          "updated": "2026-02-10T18:59:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10105v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10101v1",
          "title": "Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction",
          "summary": "3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.",
          "authors": [
            "Sizhe Yang",
            "Linning Xu",
            "Hao Li",
            "Juncheng Mu",
            "Jia Zeng",
            "Dahua Lin",
            "Jiangmiao Pang"
          ],
          "published": "2026-02-10T18:58:15Z",
          "updated": "2026-02-10T18:58:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10101v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10098v2",
          "title": "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model",
          "summary": "Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.",
          "authors": [
            "Jingwen Sun",
            "Wenyao Zhang",
            "Zekun Qi",
            "Shaojie Ren",
            "Zezhi Liu",
            "Hanxin Zhu",
            "Guangzhong Sun",
            "Xin Jin",
            "Zhibo Chen"
          ],
          "published": "2026-02-10T18:58:01Z",
          "updated": "2026-02-14T03:00:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10098v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10093v1",
          "title": "UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking",
          "summary": "Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.",
          "authors": [
            "Baijun Chen",
            "Weijie Wan",
            "Tianxing Chen",
            "Xianda Guo",
            "Congsheng Xu",
            "Yuanyang Qi",
            "Haojie Zhang",
            "Longyan Wu",
            "Tianling Xu",
            "Zixuan Li",
            "Yizhe Wu",
            "Rui Li",
            "Xiaokang Yang",
            "Ping Luo",
            "Wei Sui",
            "Yao Mu"
          ],
          "published": "2026-02-10T18:57:00Z",
          "updated": "2026-02-10T18:57:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10093v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10069v1",
          "title": "Humanoid Factors: Design Principles for AI Humanoids in Human Worlds",
          "summary": "Human factors research has long focused on optimizing environments, tools, and systems to account for human performance. Yet, as humanoid robots begin to share our workplaces, homes, and public spaces, the design challenge expands. We must now consider not only factors for humans but also factors for humanoids, since both will coexist and interact within the same environments. Unlike conventional machines, humanoids introduce expectations of human-like behavior, communication, and social presence, which reshape usability, trust, and safety considerations. In this article, we introduce the concept of humanoid factors as a framework structured around four pillars - physical, cognitive, social, and ethical - that shape the development of humanoids to help them effectively coexist and collaborate with humans. This framework characterizes the overlap and divergence between human capabilities and those of general-purpose humanoids powered by AI foundation models. To demonstrate our framework's practical utility, we then apply the framework to evaluate a real-world humanoid control algorithm, illustrating how conventional task completion metrics in robotics overlook key human cognitive and interaction principles. We thus position humanoid factors as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence.",
          "authors": [
            "Xinyuan Liu",
            "Eren Sadikoglu",
            "Ransalu Senanayake",
            "Lixiao Huang"
          ],
          "published": "2026-02-10T18:34:43Z",
          "updated": "2026-02-10T18:34:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10069v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10035v1",
          "title": "A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation",
          "summary": "Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.",
          "authors": [
            "Marc-Philip Ecker",
            "Christoph Fröhlich",
            "Johannes Huemer",
            "David Gruber",
            "Bernhard Bischof",
            "Tobias Glück",
            "Wolfgang Kemmetmüller"
          ],
          "published": "2026-02-10T17:58:25Z",
          "updated": "2026-02-10T17:58:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10035v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10032v1",
          "title": "Perception with Guarantees: Certified Pose Estimation via Reachability Analysis",
          "summary": "Agents in cyber-physical systems are increasingly entrusted with safety-critical tasks. Ensuring safety of these agents often requires localizing the pose for subsequent actions. Pose estimates can, e.g., be obtained from various combinations of lidar sensors, cameras, and external services such as GPS. Crucially, in safety-critical domains, a rough estimate is insufficient to formally determine safety, i.e., guaranteeing safety even in the worst-case scenario, and external services might additionally not be trustworthy. We address this problem by presenting a certified pose estimation in 3D solely from a camera image and a well-known target geometry. This is realized by formally bounding the pose, which is computed by leveraging recent results from reachability analysis and formal neural network verification. Our experiments demonstrate that our approach efficiently and accurately localizes agents in both synthetic and real-world experiments.",
          "authors": [
            "Tobias Ladner",
            "Yasser Shoukry",
            "Matthias Althoff"
          ],
          "published": "2026-02-10T17:55:49Z",
          "updated": "2026-02-10T17:55:49Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10032v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10015v2",
          "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments",
          "summary": "Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.",
          "authors": [
            "Dharmendra Sharma",
            "Archit Sharma",
            "John Rebeiro",
            "Vaibhav Kesharwani",
            "Peeyush Thakur",
            "Narendra Kumar Dhar",
            "Laxmidhar Behera"
          ],
          "published": "2026-02-10T17:37:35Z",
          "updated": "2026-02-11T05:36:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10015v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10013v1",
          "title": "Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper",
          "summary": "Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .",
          "authors": [
            "Xuhui Kang",
            "Tongxuan Tian",
            "Sung-Wook Lee",
            "Binghao Huang",
            "Yunzhu Li",
            "Yen-Ling Kuo"
          ],
          "published": "2026-02-10T17:36:33Z",
          "updated": "2026-02-10T17:36:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10013v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.10007v1",
          "title": "A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging",
          "summary": "Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS",
          "authors": [
            "Bharathkumar Hegde",
            "Melanie Bouroche"
          ],
          "published": "2026-02-10T17:30:09Z",
          "updated": "2026-02-10T17:30:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.MA",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.10007v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09991v1",
          "title": "Acoustic Drone Package Delivery Detection",
          "summary": "In recent years, the illicit use of unmanned aerial vehicles (UAVs) for deliveries in restricted area such as prisons became a significant security challenge. While numerous studies have focused on UAV detection or localization, little attention has been given to delivery events identification. This study presents the first acoustic package delivery detection algorithm using a ground-based microphone array. The proposed method estimates both the drone's propeller speed and the delivery event using solely acoustic features. A deep neural network detects the presence of a drone and estimates the propeller's rotation speed or blade passing frequency (BPF) from a mel spectrogram. The algorithm analyzes the BPFs to identify probable delivery moments based on sudden changes before and after a specific time. Results demonstrate a mean absolute error of the blade passing frequency estimator of 16 Hz when the drone is less than 150 meters away from the microphone array. The drone presence detection estimator has a accuracy of 97%. The delivery detection algorithm correctly identifies 96% of events with a false positive rate of 8%. This study shows that deliveries can be identified using acoustic signals up to a range of 100 meters.",
          "authors": [
            "François Marcoux",
            "François Grondin"
          ],
          "published": "2026-02-10T17:16:20Z",
          "updated": "2026-02-10T17:16:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09991v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09973v1",
          "title": "RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation",
          "summary": "Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.",
          "authors": [
            "Hao Li",
            "Ziqin Wang",
            "Zi-han Ding",
            "Shuai Yang",
            "Yilun Chen",
            "Yang Tian",
            "Xiaolin Hu",
            "Tai Wang",
            "Dahua Lin",
            "Feng Zhao",
            "Si Liu",
            "Jiangmiao Pang"
          ],
          "published": "2026-02-10T17:01:54Z",
          "updated": "2026-02-10T17:01:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09973v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09972v1",
          "title": "Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning",
          "summary": "While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.",
          "authors": [
            "Zixuan Wang",
            "Huang Fang",
            "Shaoan Wang",
            "Yuanfei Luo",
            "Heng Dong",
            "Wei Li",
            "Yiming Gan"
          ],
          "published": "2026-02-10T17:00:16Z",
          "updated": "2026-02-10T17:00:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09972v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09940v1",
          "title": "Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation",
          "summary": "Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.",
          "authors": [
            "Archit Sharma",
            "Dharmendra Sharma",
            "John Rebeiro",
            "Peeyush Thakur",
            "Narendra Dhar",
            "Laxmidhar Behera"
          ],
          "published": "2026-02-10T16:25:39Z",
          "updated": "2026-02-10T16:25:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09940v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09893v1",
          "title": "TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data",
          "summary": "Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.",
          "authors": [
            "Zhengxue Cheng",
            "Yan Zhao",
            "Keyu Wang",
            "Hengdi Zhang",
            "Li Song"
          ],
          "published": "2026-02-10T15:30:44Z",
          "updated": "2026-02-10T15:30:44Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09893v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09888v1",
          "title": "TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback",
          "summary": "Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.",
          "authors": [
            "Zihao Li",
            "Yanan Zhou",
            "Ranpeng Qiu",
            "Hangyu Wu",
            "Guoqiang Ren",
            "Weiming Zhi"
          ],
          "published": "2026-02-10T15:26:42Z",
          "updated": "2026-02-10T15:26:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09888v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09849v2",
          "title": "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation",
          "summary": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.",
          "authors": [
            "Yucheng Hu",
            "Jianke Zhang",
            "Yuanfei Luo",
            "Yanjiang Guo",
            "Xiaoyu Chen",
            "Xinshu Sun",
            "Kun Feng",
            "Qingzhou Lu",
            "Sheng Chen",
            "Yangang Zhang",
            "Wei Li",
            "Jianyu Chen"
          ],
          "published": "2026-02-10T14:54:01Z",
          "updated": "2026-02-11T03:54:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09849v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09772v1",
          "title": "Design and Evaluation of an Assisted Programming Interface for Behavior Trees in Robotics",
          "summary": "The possibility to create reactive robot programs faster without the need for extensively trained programmers is becoming increasingly important. So far, it has not been explored how various techniques for creating Behavior Tree (BT) program representations could be combined with complete graphical user interfaces (GUIs) to allow a human user to validate and edit trees suggested by automated methods. In this paper, we introduce BEhavior TRee GUI (BETR-GUI) for creating BTs with the help of an AI assistant that combines methods using large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor. A user study with 60 participants shows that by combining different assistive methods, BETR-GUI enables users to perform better at solving the robot programming tasks. The results also show that humans using the full variant of BETR-GUI perform better than the AI assistant running on its own.",
          "authors": [
            "Jonathan Styrud",
            "Matteo Iovino",
            "Rebecca Stower",
            "Mart Kartašev",
            "Mikael Norrlöf",
            "Mårten Björkman",
            "Christian Smith"
          ],
          "published": "2026-02-10T13:34:00Z",
          "updated": "2026-02-10T13:34:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09772v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09767v1",
          "title": "Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning",
          "summary": "Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\\% expansion in state-space coverage compared to the baseline.",
          "authors": [
            "Ruopeng Cui",
            "Yifei Bi",
            "Haojie Luo",
            "Wei Li"
          ],
          "published": "2026-02-10T13:28:13Z",
          "updated": "2026-02-10T13:28:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09767v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09765v1",
          "title": "NavDreamer: Video Models as Zero-Shot 3D Navigators",
          "summary": "Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.",
          "authors": [
            "Xijie Huang",
            "Weiqi Gai",
            "Tianyue Wu",
            "Congyu Wang",
            "Zhiyang Liu",
            "Xin Zhou",
            "Yuze Wu",
            "Fei Gao"
          ],
          "published": "2026-02-10T13:24:12Z",
          "updated": "2026-02-10T13:24:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09765v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09722v1",
          "title": "Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization",
          "summary": "While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard \"scale data\" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla",
          "authors": [
            "Ye Wang",
            "Sipeng Zheng",
            "Hao Luo",
            "Wanpeng Zhang",
            "Haoqi Yuan",
            "Chaoyi Xu",
            "Haiweng Xu",
            "Yicheng Feng",
            "Mingyang Yu",
            "Zhiyu Kang",
            "Zongqing Lu",
            "Qin Jin"
          ],
          "published": "2026-02-10T12:25:43Z",
          "updated": "2026-02-10T12:25:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09722v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09714v1",
          "title": "Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments",
          "summary": "We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.",
          "authors": [
            "Alejandro Gonzalez-Garcia",
            "Sebastiaan Wyns",
            "Sonia De Santis",
            "Jan Swevers",
            "Wilm Decré"
          ],
          "published": "2026-02-10T12:18:14Z",
          "updated": "2026-02-10T12:18:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09714v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09661v1",
          "title": "RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination",
          "summary": "This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.",
          "authors": [
            "Ameer Alhashemi",
            "Layan Abdulhadi",
            "Karam Abuodeh",
            "Tala Baghdadi",
            "Suryanarayana Datla"
          ],
          "published": "2026-02-10T11:11:22Z",
          "updated": "2026-02-10T11:11:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09661v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09657v1",
          "title": "AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild",
          "summary": "Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.",
          "authors": [
            "Xiaolou Sun",
            "Wufei Si",
            "Wenhui Ni",
            "Yuntian Li",
            "Dongming Wu",
            "Fei Xie",
            "Runwei Guan",
            "He-Yang Xu",
            "Henghui Ding",
            "Yuan Wu",
            "Yutao Yue",
            "Yongming Huang",
            "Hui Xiong"
          ],
          "published": "2026-02-10T11:08:07Z",
          "updated": "2026-02-10T11:08:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09657v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09628v1",
          "title": "TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior",
          "summary": "Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.",
          "authors": [
            "Jie Li",
            "Bing Tang",
            "Feng Wu",
            "Rongyun Cao"
          ],
          "published": "2026-02-10T10:14:06Z",
          "updated": "2026-02-10T10:14:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09628v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09617v1",
          "title": "AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception",
          "summary": "Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.",
          "authors": [
            "Ruoxuan Feng",
            "Yuxuan Zhou",
            "Siyu Mei",
            "Dongzhan Zhou",
            "Pengwei Wang",
            "Shaowei Cui",
            "Bin Fang",
            "Guocai Yao",
            "Di Hu"
          ],
          "published": "2026-02-10T10:05:53Z",
          "updated": "2026-02-10T10:05:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09617v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09583v1",
          "title": "Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation",
          "summary": "Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.",
          "authors": [
            "Marco Moletta",
            "Michael C. Welle",
            "Danica Kragic"
          ],
          "published": "2026-02-10T09:35:22Z",
          "updated": "2026-02-10T09:35:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09583v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09580v1",
          "title": "Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows",
          "summary": "Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy's temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.",
          "authors": [
            "Chenyu Yang",
            "Denis Tarasov",
            "Davide Liconti",
            "Hehui Zheng",
            "Robert K. Katzschmann"
          ],
          "published": "2026-02-10T09:28:20Z",
          "updated": "2026-02-10T09:28:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09580v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09563v1",
          "title": "Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization",
          "summary": "Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.",
          "authors": [
            "Lucas Palazzolo",
            "Mickaël Binois",
            "Laëtitia Giraldi"
          ],
          "published": "2026-02-10T09:14:32Z",
          "updated": "2026-02-10T09:14:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09563v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09472v1",
          "title": "LLM-Grounded Dynamic Task Planning with Hierarchical Temporal Logic for Human-Aware Multi-Robot Collaboration",
          "summary": "While Large Language Models (LLM) enable non-experts to specify open-world multi-robot tasks, the generated plans often lack kinematic feasibility and are not efficient, especially in long-horizon scenarios. Formal methods like Linear Temporal Logic (LTL) offer correctness and optimal guarantees, but are typically confined to static, offline settings and struggle with computational scalability. To bridge this gap, we propose a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves the corresponding Simultaneous Task Allocation and Planning (STAP) problem. Unlike static approaches, our system resolves stochastic environmental changes, such as moving users or updated instructions via a receding horizon planning (RHP) loop with real-time perception, which dynamically refines plans through a hierarchical state space. Extensive real-world experiments demonstrate that our approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.",
          "authors": [
            "Shuyuan Hu",
            "Tao Lin",
            "Kai Ye",
            "Yang Yang",
            "Tianwei Zhang"
          ],
          "published": "2026-02-10T07:11:36Z",
          "updated": "2026-02-10T07:11:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09472v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09430v1",
          "title": "Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments",
          "summary": "Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.",
          "authors": [
            "Yiwen Pang",
            "Bo Zhou",
            "Changjin Li",
            "Xuanhao Wang",
            "Shengxiang Xu",
            "Deng-Bao Wang",
            "Min-Ling Zhang",
            "Shimin Di"
          ],
          "published": "2026-02-10T05:50:19Z",
          "updated": "2026-02-10T05:50:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09430v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09429v2",
          "title": "First-order friction models with bristle dynamics: lumped and distributed formulations",
          "summary": "Dynamic models, particularly rate-dependent models, have proven effective in capturing the key phenomenological features of frictional processes, whilst also possessing important mathematical properties that facilitate the design of control and estimation algorithms. However, many rate-dependent formulations are built on empirical considerations, whereas physical derivations may offer greater interpretability. In this context, starting from fundamental physical principles, this paper introduces a novel class of first-order dynamic friction models that approximate the dynamics of a bristle element by inverting the friction characteristic. Amongst the developed models, a specific formulation closely resembling the LuGre model is derived using a simple rheological equation for the bristle element. This model is rigorously analyzed in terms of stability and passivity -- important properties that support the synthesis of observers and controllers. Furthermore, a distributed version, formulated as a hyperbolic partial differential equation (PDE), is presented, which enables the modeling of frictional processes commonly encountered in rolling contact phenomena. The tribological behavior of the proposed description is evaluated through classical experiments and validated against the response predicted by the LuGre model, revealing both notable similarities and key differences.",
          "authors": [
            "Luigi Romano",
            "Ole Morten Aamo",
            "Jan Åslund",
            "Erik Frisk"
          ],
          "published": "2026-02-10T05:49:16Z",
          "updated": "2026-02-11T13:17:47Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09429v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09427v2",
          "title": "Lateral tracking control of all-wheel steering vehicles with intelligent tires",
          "summary": "The accurate characterization of tire dynamics is critical for advancing control strategies in autonomous road vehicles, as tire behavior significantly influences handling and stability through the generation of forces and moments at the tire-road interface. Smart tire technologies have emerged as a promising tool for sensing key variables such as road friction, tire pressure, and wear states, and for estimating kinematic and dynamic states like vehicle speed and tire forces. However, most existing estimation and control algorithms rely on empirical correlations or machine learning approaches, which require extensive calibration and can be sensitive to variations in operating conditions. In contrast, model-based techniques, which leverage infinite-dimensional representations of tire dynamics using partial differential equations (PDEs), offer a more robust approach. This paper proposes a novel model-based, output-feedback lateral tracking control strategy for all-wheel steering vehicles that integrates distributed tire dynamics with smart tire technologies. The primary contributions include the suppression of micro-shimmy phenomena at low speeds and path-following via force control, achieved through the estimation of tire slip angles, vehicle kinematics, and lateral tire forces. The proposed controller and observer are based on formulations using ODE-PDE systems, representing rigid body dynamics and distributed tire behavior. This work marks the first rigorous control strategy for vehicular systems equipped with distributed tire representations in conjunction with smart tire technologies.",
          "authors": [
            "Luigi Romano",
            "Ole Morten Aamo",
            "Jan Åslund",
            "Erik Frisk"
          ],
          "published": "2026-02-10T05:45:17Z",
          "updated": "2026-02-11T13:17:21Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09427v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09414v1",
          "title": "Finite-time Stable Pose Estimation on TSE(3) using Point Cloud and Velocity Sensors",
          "summary": "This work presents a finite-time stable pose estimator (FTS-PE) for rigid bodies undergoing rotational and translational motion in three dimensions, using measurements from onboard sensors that provide position vectors to inertially-fixed points and body velocities. The FTS-PE is a full-state observer for the pose (position and orientation) and velocities and is obtained through a Lyapunov analysis that shows its stability in finite time and its robustness to bounded measurement noise. Further, this observer is designed directly on the state space, the tangent bundle of the Lie group of rigid body motions, SE(3), without using local coordinates or (dual) quaternion representations. Therefore, it can estimate arbitrary rigid body motions without encountering singularities or the unwinding phenomenon and be readily applied to autonomous vehicles. A version of this observer that does not need translational velocity measurements and uses only point clouds and angular velocity measurements from rate gyros, is also obtained. It is discretized using the framework of geometric mechanics for numerical and experimental implementations. The numerical simulations compare the FTS-PE with a dual-quaternion extended Kalman filter and our previously developed variational pose estimator (VPE). The experimental results are obtained using point cloud images and rate gyro measurements obtained from a Zed 2i stereo depth camera sensor. These results validate the stability and robustness of the FTS-PE.",
          "authors": [
            "Nazanin S. Hashkavaei",
            "Abhijit Dongare",
            "Neon Srinivasu",
            "Amit K. Sanyal"
          ],
          "published": "2026-02-10T05:11:01Z",
          "updated": "2026-02-10T05:11:01Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09414v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09370v1",
          "title": "Phase-Aware Policy Learning for Skateboard Riding of Quadruped Robots via Feature-wise Linear Modulation",
          "summary": "Skateboards offer a compact and efficient means of transportation as a type of personal mobility device. However, controlling them with legged robots poses several challenges for policy learning due to perception-driven interactions and multi-modal control objectives across distinct skateboarding phases. To address these challenges, we introduce Phase-Aware Policy Learning (PAPL), a reinforcement-learning framework tailored for skateboarding with quadruped robots. PAPL leverages the cyclic nature of skateboarding by integrating phase-conditioned Feature-wise Linear Modulation layers into actor and critic networks, enabling a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases. Our evaluations in simulation validate command-tracking accuracy and conduct ablation studies quantifying each component's contribution. We also compare locomotion efficiency against leg and wheel-leg baselines and show real-world transferability.",
          "authors": [
            "Minsung Yoon",
            "Jeil Jeong",
            "Sung-Eui Yoon"
          ],
          "published": "2026-02-10T03:20:37Z",
          "updated": "2026-02-10T03:20:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09370v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09368v1",
          "title": "Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes",
          "summary": "Gradient-based methods can efficiently optimize controllers using physical priors and differentiable simulators, but contact-rich manipulation remains challenging due to discontinuous or vanishing gradients from hybrid contact dynamics. Smoothing the dynamics yields continuous gradients, but the resulting model mismatch can cause controller failures when executed on real systems. We address this trade-off by planning with smoothed dynamics while explicitly quantifying and compensating for the induced errors, providing formal guarantees of constraint satisfaction and goal reachability on the true hybrid dynamics. Our method smooths both contact dynamics and geometry via a novel differentiable simulator based on convex optimization, which enables us to characterize the discrepancy from the true dynamics as a set-valued deviation. This deviation constrains the optimization of time-varying affine feedback policies through analytical bounds on the system's reachable set, enabling robust constraint satisfaction guarantees for the true closed-loop hybrid dynamics, while relying solely on informative gradients from the smoothed dynamics. We evaluate our method on several contact-rich tasks, including planar pushing, object rotation, and in-hand dexterous manipulation, achieving guaranteed constraint satisfaction with lower safety violation and goal error than baselines. By bridging differentiable physics with set-valued robust control, our method is the first certifiable gradient-based policy synthesis method for contact-rich manipulation.",
          "authors": [
            "Wei-Chen Li",
            "Glen Chou"
          ],
          "published": "2026-02-10T03:19:42Z",
          "updated": "2026-02-10T03:19:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09368v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09367v1",
          "title": "CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments",
          "summary": "Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.",
          "authors": [
            "Jinghan Yang",
            "Jingyi Hou",
            "Xinbo Yu",
            "Wei He",
            "Yifan Wu"
          ],
          "published": "2026-02-10T03:18:41Z",
          "updated": "2026-02-10T03:18:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09367v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09287v1",
          "title": "Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction",
          "summary": "In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.",
          "authors": [
            "Minja Axelsson",
            "Henry Shevlin"
          ],
          "published": "2026-02-10T00:13:30Z",
          "updated": "2026-02-10T00:13:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09287v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09259v1",
          "title": "Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation",
          "summary": "In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.",
          "authors": [
            "Yizhou Li",
            "Shuyuan Yang",
            "Jiaji Su",
            "Zonghe Chua"
          ],
          "published": "2026-02-09T22:52:59Z",
          "updated": "2026-02-09T22:52:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09259v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09255v2",
          "title": "STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory",
          "summary": "Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable Task Conditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust long horizon reasoning, scalability, and practical utility. Project Website: https://trailab.github.io/STaR-website/",
          "authors": [
            "Mingfeng Yuan",
            "Hao Zhang",
            "Mahan Mohammadi",
            "Runhao Li",
            "Jinjun Shan",
            "Steven L. Waslander"
          ],
          "published": "2026-02-09T22:38:53Z",
          "updated": "2026-02-12T06:19:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09255v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09227v1",
          "title": "From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers",
          "summary": "In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.",
          "authors": [
            "Ananya Yammanuru",
            "Maria Lusardi",
            "Nancy M. Amato",
            "Katherine Driggs-Campbell"
          ],
          "published": "2026-02-09T21:53:07Z",
          "updated": "2026-02-09T21:53:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09227v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09204v1",
          "title": "Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications",
          "summary": "Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.",
          "authors": [
            "Ozan Kaya",
            "Emir Cem Gezer",
            "Roger Skjetne",
            "Ingrid Bouwer Utne"
          ],
          "published": "2026-02-09T21:15:33Z",
          "updated": "2026-02-09T21:15:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09204v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09203v2",
          "title": "Elements of Robot Morphology: Supporting Designers in Robot Form Exploration",
          "summary": "Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.",
          "authors": [
            "Amy Koike",
            "Serena Ge Guo",
            "Xinning He",
            "Callie Y. Kim",
            "Dakota Sullivan",
            "Bilge Mutlu"
          ],
          "published": "2026-02-09T21:13:20Z",
          "updated": "2026-02-18T17:41:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09203v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09153v1",
          "title": "SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes",
          "summary": "Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stages$\\unicode{x2013}$from architectural layout to furniture placement to small object population$\\unicode{x2013}$each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.",
          "authors": [
            "Nicholas Pfaff",
            "Thomas Cohn",
            "Sergey Zakharov",
            "Rick Cory",
            "Russ Tedrake"
          ],
          "published": "2026-02-09T19:56:04Z",
          "updated": "2026-02-09T19:56:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.GR"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09153v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09123v1",
          "title": "Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality",
          "summary": "Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.",
          "authors": [
            "Jackson Habala",
            "Gabriel B. Margolis",
            "Tianyu Wang",
            "Pratyush Bhatt",
            "Juntao He",
            "Naheel Naeem",
            "Zhaochen Xu",
            "Pulkit Agrawal",
            "Daniel I. Goldman",
            "Di Luo",
            "Baxi Chong"
          ],
          "published": "2026-02-09T19:13:14Z",
          "updated": "2026-02-09T19:13:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09123v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09023v1",
          "title": "TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation",
          "summary": "Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.",
          "authors": [
            "Qinwen Xu",
            "Jiaming Liu",
            "Rui Zhou",
            "Shaojun Shi",
            "Nuowei Han",
            "Zhuoyang Liu",
            "Chenyang Gu",
            "Shuo Gu",
            "Yang Yue",
            "Gao Huang",
            "Wenzhao Zheng",
            "Sirui Han",
            "Peng Jia",
            "Shanghang Zhang"
          ],
          "published": "2026-02-09T18:59:52Z",
          "updated": "2026-02-09T18:59:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09023v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09021v1",
          "title": "$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
          "summary": "High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.",
          "authors": [
            "Checheng Yu",
            "Chonghao Sima",
            "Gangcheng Jiang",
            "Hai Zhang",
            "Haoguang Mai",
            "Hongyang Li",
            "Huijie Wang",
            "Jin Chen",
            "Kaiyang Wu",
            "Li Chen",
            "Lirui Zhao",
            "Modi Shi",
            "Ping Luo",
            "Qingwen Bu",
            "Shijia Peng",
            "Tianyu Li",
            "Yibo Yuan"
          ],
          "published": "2026-02-09T18:59:45Z",
          "updated": "2026-02-09T18:59:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09021v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09018v1",
          "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving",
          "summary": "Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \\in \\{0,1,2,3\\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\\rightarrow$ urban and day $\\rightarrow$ night ($\\sim 31\\%$ each); actor swaps $\\sim 10\\%$, moderate rain $\\sim 7\\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\\% \\rightarrow 70.1\\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.",
          "authors": [
            "Amir Mallak",
            "Alaa Maalouf"
          ],
          "published": "2026-02-09T18:59:03Z",
          "updated": "2026-02-09T18:59:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09018v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09017v1",
          "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
          "summary": "The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/",
          "authors": [
            "Zichen Jeff Cui",
            "Omar Rayyan",
            "Haritheja Etukuru",
            "Bowen Tan",
            "Zavier Andrianarivo",
            "Zicheng Teng",
            "Yihang Zhou",
            "Krish Mehta",
            "Nicholas Wojno",
            "Kevin Yuanbo Wu",
            "Manan H Anjaria",
            "Ziyuan Wu",
            "Manrong Mao",
            "Guangxun Zhang",
            "Binit Shah",
            "Yejin Kim",
            "Soumith Chintala",
            "Lerrel Pinto",
            "Nur Muhammad Mahi Shafiullah"
          ],
          "published": "2026-02-09T18:58:50Z",
          "updated": "2026-02-09T18:58:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09017v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09013v2",
          "title": "Dexterous Manipulation Policies from RGB Human Videos via 3D Hand-Object Trajectory Reconstruction",
          "summary": "Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 3D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.",
          "authors": [
            "Hongyi Chen",
            "Tony Dong",
            "Tiancheng Wu",
            "Liquan Wang",
            "Yash Jangir",
            "Yaru Niu",
            "Yufei Ye",
            "Homanga Bharadhwaj",
            "Zackory Erickson",
            "Jeffrey Ichnowski"
          ],
          "published": "2026-02-09T18:56:02Z",
          "updated": "2026-02-11T23:32:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09013v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09002v1",
          "title": "From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection",
          "summary": "Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io",
          "authors": [
            "Zilin Fang",
            "Anxing Xiao",
            "David Hsu",
            "Gim Hee Lee"
          ],
          "published": "2026-02-09T18:46:12Z",
          "updated": "2026-02-09T18:46:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09002v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08999v1",
          "title": "CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion",
          "summary": "With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue",
          "authors": [
            "Mouad Abrini",
            "Mohamed Chetouani"
          ],
          "published": "2026-02-09T18:44:35Z",
          "updated": "2026-02-09T18:44:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08999v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08971v2",
          "title": "WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models",
          "summary": "While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://world-arena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.",
          "authors": [
            "Yu Shang",
            "Zhuohang Li",
            "Yiding Ma",
            "Weikang Su",
            "Xin Jin",
            "Ziyou Wang",
            "Lei Jin",
            "Xin Zhang",
            "Yinzhou Tang",
            "Haisheng Su",
            "Chen Gao",
            "Wei Wu",
            "Xihui Liu",
            "Dhruv Shah",
            "Zhaoxiang Zhang",
            "Zhibo Chen",
            "Jun Zhu",
            "Yonghong Tian",
            "Tat-Seng Chua",
            "Wenwu Zhu",
            "Yong Li"
          ],
          "published": "2026-02-09T18:09:20Z",
          "updated": "2026-02-11T10:50:05Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08971v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08963v1",
          "title": "Reduced-order Control and Geometric Structure of Learned Lagrangian Latent Dynamics",
          "summary": "Model-based controllers can offer strong guarantees on stability and convergence by relying on physically accurate dynamic models. However, these are rarely available for high-dimensional mechanical systems such as deformable objects or soft robots. While neural architectures can learn to approximate complex dynamics, they are either limited to low-dimensional systems or provide only limited formal control guarantees due to a lack of embedded physical structure. This paper introduces a latent control framework based on learned structure-preserving reduced-order dynamics for high-dimensional Lagrangian systems. We derive a reduced tracking law for fully actuated systems and adopt a Riemannian perspective on projection-based model-order reduction to study the resulting latent and projected closed-loop dynamics. By quantifying the sources of modeling error, we derive interpretable conditions for stability and convergence. We extend the proposed controller and analysis to underactuated systems by introducing learned actuation patterns. Experimental results on simulated and real-world systems validate our theoretical investigation and the accuracy of our controllers.",
          "authors": [
            "Katharina Friedl",
            "Noémie Jaquier",
            "Seungyeon Kim",
            "Jens Lundell",
            "Danica Kragic"
          ],
          "published": "2026-02-09T18:00:04Z",
          "updated": "2026-02-09T18:00:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08963v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08962v1",
          "title": "Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting",
          "summary": "Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D",
          "authors": [
            "Guangxun Zhu",
            "Xuan Liu",
            "Nicolas Pugeault",
            "Chongfeng Wei",
            "Edmond S. L. Ho"
          ],
          "published": "2026-02-09T17:58:53Z",
          "updated": "2026-02-09T17:58:53Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08962v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09076v1",
          "title": "Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception",
          "summary": "Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.",
          "authors": [
            "Nhat Le",
            "Daeun Song",
            "Xuesu Xiao"
          ],
          "published": "2026-02-09T16:56:23Z",
          "updated": "2026-02-09T16:56:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09076v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08845v1",
          "title": "Finite-Time Teleoperation of Euler-Lagrange Systems via Energy-Shaping",
          "summary": "This paper proposes a family of finite-time controllers for the bilateral teleoperation of fully actuated nonlinear Euler-Lagrange systems. Based on the energy-shaping framework and under the standard assumption of passive interactions with the human and the environment, the controllers ensure that the position error and velocities globally converge to zero in the absence of time delays. In this case, the closed-loop system admits a homogeneous approximation of negative degree, and thus the control objective is achieved in finite-time. The proposed controllers are simple, continuous-time proportional-plus-damping-injection schemes, validated through both simulation and experimental results.",
          "authors": [
            "Lazaro F. Torres",
            "Carlos I. Aldana",
            "Emmanuel Nuño",
            "Emmanuel Cruz-Zavala"
          ],
          "published": "2026-02-09T16:12:08Z",
          "updated": "2026-02-09T16:12:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08845v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08842v2",
          "title": "karl. - A Research Vehicle for Automated and Connected Driving",
          "summary": "As highly automated driving is transitioning from single-vehicle closed-access testing to commercial deployments of public ride-hailing in selected areas (e.g., Waymo), automated driving and connected cooperative intelligent transport systems (C-ITS) remain active fields of research. Even though simulation is omnipresent in the development and validation life cycle of automated and connected driving technology, the complex nature of public road traffic and software that masters it still requires real-world integration and testing with actual vehicles. Dedicated vehicles for research and development allow testing and validation of software and hardware components under real-world conditions early on. They also enable collecting and publishing real-world datasets that let others conduct research without vehicle access, and support early demonstration of futuristic use cases. In this paper, we present karl., our new research vehicle for automated and connected driving. Apart from major corporations, few institutions worldwide have access to their own L4-capable research vehicles, restricting their ability to carry out independent research. This paper aims to help bridge that gap by sharing the reasoning, design choices, and technical details that went into making karl. a flexible and powerful platform for research, engineering, and validation in the context of automated and connected driving. More impressions of karl. are available at https://karl.ac.",
          "authors": [
            "Jean-Pierre Busch",
            "Lukas Ostendorf",
            "Guido Linden",
            "Lennart Reiher",
            "Till Beemelmanns",
            "Bastian Lampe",
            "Timo Woopen",
            "Lutz Eckstein"
          ],
          "published": "2026-02-09T16:09:24Z",
          "updated": "2026-02-10T14:50:04Z",
          "primary_category": "cs.AR",
          "categories": [
            "cs.AR",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08842v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08821v1",
          "title": "Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems",
          "summary": "The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV. The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally. Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work. The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.",
          "authors": [
            "Robin Dehler",
            "Oliver Schumann",
            "Jona Ruof",
            "Michael Buchholz"
          ],
          "published": "2026-02-09T15:56:11Z",
          "updated": "2026-02-09T15:56:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08821v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08799v1",
          "title": "A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles",
          "summary": "Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services. This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs. With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server. The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.",
          "authors": [
            "Robin Dehler",
            "Michael Buchholz"
          ],
          "published": "2026-02-09T15:39:28Z",
          "updated": "2026-02-09T15:39:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08799v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08784v1",
          "title": "GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion",
          "summary": "Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.",
          "authors": [
            "Santiago Montiel-Marín",
            "Miguel Antunes-García",
            "Fabio Sánchez-García",
            "Angel Llamazares",
            "Holger Caesar",
            "Luis M. Bergasa"
          ],
          "published": "2026-02-09T15:25:19Z",
          "updated": "2026-02-09T15:25:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08784v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08776v1",
          "title": "Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch",
          "summary": "Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to \"Intent Cloning\" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a \"virtual equilibrium point\", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \\href{https://xucj98.github.io/mind-the-gap-page/}{project page}.",
          "authors": [
            "Cuijie Xu",
            "Shurui Zheng",
            "Zihao Su",
            "Yuanfan Xu",
            "Tinghao Yi",
            "Xudong Zhang",
            "Jian Wang",
            "Yu Wang",
            "Jinchen Yu"
          ],
          "published": "2026-02-09T15:18:12Z",
          "updated": "2026-02-09T15:18:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08776v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08653v1",
          "title": "High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning",
          "summary": "Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms. We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances. Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.",
          "authors": [
            "Jiarui Zhang",
            "Chengyong Lei",
            "Chengjiang Dai",
            "Lijie Wang",
            "Zhichao Han",
            "Fei Gao"
          ],
          "published": "2026-02-09T13:47:02Z",
          "updated": "2026-02-09T13:47:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08653v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08602v1",
          "title": "Mimic Intent, Not Just Trajectories",
          "summary": "While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \\textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \\textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \\textit{Intent token} that facilitates planning and transfer, and multi-scale \\textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \\textit{next-scale autoregression}, performing progressive \\textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \\textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.",
          "authors": [
            "Renming Huang",
            "Chendong Zeng",
            "Wenjing Tang",
            "Jingtian Cai",
            "Cewu Lu",
            "Panpan Cai"
          ],
          "published": "2026-02-09T12:44:35Z",
          "updated": "2026-02-09T12:44:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08602v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08599v1",
          "title": "A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation",
          "summary": "Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.",
          "authors": [
            "Kenghou Hoi",
            "Yuze Wu",
            "Annan Ding",
            "Junjie Wang",
            "Anke Zhao",
            "Chengqian Zhang",
            "Fei Gao"
          ],
          "published": "2026-02-09T12:40:34Z",
          "updated": "2026-02-09T12:40:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08599v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08594v2",
          "title": "MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation",
          "summary": "Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise. Project page: baai-humanoid.github.io/MOSAIC.",
          "authors": [
            "Zhenguo Sun",
            "Bo-Sheng Huang",
            "Yibo Peng",
            "Xukun Li",
            "Jingyu Ma",
            "Yu Sun",
            "Zhe Li",
            "Haojun Jiang",
            "Biao Gao",
            "Zhenshan Bing",
            "Xinlong Wang",
            "Alois Knoll"
          ],
          "published": "2026-02-09T12:35:01Z",
          "updated": "2026-02-11T16:45:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08594v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08571v1",
          "title": "Head-to-Head autonomous racing at the limits of handling in the A2RL challenge",
          "summary": "Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety. This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.",
          "authors": [
            "Simon Hoffmann",
            "Simon Sagmeister",
            "Tobias Betz",
            "Joscha Bongard",
            "Sascha Büttner",
            "Dominic Ebner",
            "Daniel Esser",
            "Georg Jank",
            "Sven Goblirsch",
            "Alexander Langmann",
            "Maximilian Leitenstern",
            "Levent Ögretmen",
            "Phillip Pitschi",
            "Ann-Kathrin Schwehn",
            "Cornelius Schröder",
            "Marcel Weinmann",
            "Frederik Werner",
            "Boris Lohmann",
            "Johannes Betz",
            "Markus Lienkamp"
          ],
          "published": "2026-02-09T12:12:02Z",
          "updated": "2026-02-09T12:12:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08571v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08557v1",
          "title": "Constrained Sampling to Guide Universal Manipulation RL",
          "summary": "We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.",
          "authors": [
            "Marc Toussaint",
            "Cornelius V. Braun",
            "Eckart Cobo-Briesewitz",
            "Sayantan Auddy",
            "Armand Jordana",
            "Justin Carpentier"
          ],
          "published": "2026-02-09T11:54:45Z",
          "updated": "2026-02-09T11:54:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08557v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08537v1",
          "title": "UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation",
          "summary": "Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.",
          "authors": [
            "Haoming Ye",
            "Yunxiao Xiao",
            "Cewu Lu",
            "Panpan Cai"
          ],
          "published": "2026-02-09T11:35:21Z",
          "updated": "2026-02-09T11:35:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08537v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08518v1",
          "title": "Characteristics, Management, and Utilization of Muscles in Musculoskeletal Humanoids: Empirical Study on Kengoro and Musashi",
          "summary": "Various musculoskeletal humanoids have been developed so far, and numerous studies on control mechanisms have been conducted to leverage the advantages of their biomimetic bodies. However, there has not been sufficient and unified discussion on the diverse properties inherent in these musculoskeletal structures, nor on how to manage and utilize them. Therefore, this study categorizes and analyzes the characteristics of muscles, as well as their management and utilization methods, based on the various research conducted on the musculoskeletal humanoids we have developed, Kengoro and Musashi. We classify the features of the musculoskeletal structure into five properties: Redundancy, Independency, Anisotropy, Variable Moment Arm, and Nonlinear Elasticity. We then organize the diverse advantages and disadvantages of musculoskeletal humanoids that arise from the combination of these properties. In particular, we discuss body schema learning and reflex control, along with muscle grouping and body schema adaptation. Also, we describe the implementation of movements through an integrated system and discuss future challenges and prospects.",
          "authors": [
            "Kento Kawaharazuka",
            "Kei Okada",
            "Masayuki Inaba"
          ],
          "published": "2026-02-09T11:07:16Z",
          "updated": "2026-02-09T11:07:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08518v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08466v1",
          "title": "Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment",
          "summary": "Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.",
          "authors": [
            "Ning Hu",
            "Senhao Cao",
            "Maochen Li"
          ],
          "published": "2026-02-09T10:14:39Z",
          "updated": "2026-02-09T10:14:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08466v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08450v1",
          "title": "UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials",
          "summary": "This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations. Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection. The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.",
          "authors": [
            "Stefan Ivić",
            "Luka Lanča",
            "Karlo Jakac",
            "Ante Sikirica",
            "Stella Dumenčić",
            "Matej Mališa",
            "Zvonimir Mrle",
            "Bojan Crnković"
          ],
          "published": "2026-02-09T10:01:53Z",
          "updated": "2026-02-09T10:01:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08450v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08444v1",
          "title": "Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions",
          "summary": "Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery. The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.",
          "authors": [
            "Samsaptak Ghosh",
            "M. Felix Orlando",
            "Sohom Chakrabarty"
          ],
          "published": "2026-02-09T09:57:10Z",
          "updated": "2026-02-09T09:57:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08444v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08440v2",
          "title": "SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios",
          "summary": "A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.",
          "authors": [
            "Tian Gao",
            "Celine Tan",
            "Catherine Glossop",
            "Timothy Gao",
            "Jiankai Sun",
            "Kyle Stachowicz",
            "Shirley Wu",
            "Oier Mees",
            "Dorsa Sadigh",
            "Sergey Levine",
            "Chelsea Finn"
          ],
          "published": "2026-02-09T09:54:02Z",
          "updated": "2026-02-13T08:14:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08440v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08425v2",
          "title": "Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence",
          "summary": "Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/",
          "authors": [
            "Jinxian Zhou",
            "Ruihai Wu",
            "Yiwei Liu",
            "Yiwen Hou",
            "Xunzhe Zhou",
            "Checheng Yu",
            "Licheng Zhong",
            "Lin Shao"
          ],
          "published": "2026-02-09T09:30:23Z",
          "updated": "2026-02-10T07:21:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08425v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08421v1",
          "title": "Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric",
          "summary": "Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.",
          "authors": [
            "Farhad Keramat",
            "Salma Salimi",
            "Tomi Westerlund"
          ],
          "published": "2026-02-09T09:26:32Z",
          "updated": "2026-02-09T09:26:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08421v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08417v1",
          "title": "Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion",
          "summary": "Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph. Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts. For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion. To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear. Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.",
          "authors": [
            "Wentao Zhao",
            "Yihe Niu",
            "Zikun Chen",
            "Rui Li",
            "Yanbo Wang",
            "Tianchen Deng",
            "Jingchuan Wang"
          ],
          "published": "2026-02-09T09:19:47Z",
          "updated": "2026-02-09T09:19:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08417v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08392v1",
          "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
          "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.",
          "authors": [
            "Xin Wu",
            "Zhixuan Liang",
            "Yue Ma",
            "Mengkang Hu",
            "Zhiyuan Qin",
            "Xiu Li"
          ],
          "published": "2026-02-09T08:47:14Z",
          "updated": "2026-02-09T08:47:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08392v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08370v1",
          "title": "Learning Human-Like Badminton Skills for Humanoid Robots",
          "summary": "Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a \"mimic\" to a capable \"striker.\" Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.",
          "authors": [
            "Yeke Chen",
            "Shihao Dong",
            "Xiaoyu Ji",
            "Jingkai Sun",
            "Zeren Luo",
            "Liu Zhao",
            "Jiahui Zhang",
            "Wanyue Li",
            "Ji Ma",
            "Bowen Xu",
            "Yimin Han",
            "Yudong Zhao",
            "Peng Lu"
          ],
          "published": "2026-02-09T08:09:52Z",
          "updated": "2026-02-09T08:09:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08370v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08334v1",
          "title": "Vec-QMDP: Vectorized POMDP Planning on CPUs for Real-Time Autonomous Driving",
          "summary": "Planning under uncertainty for real-world robotics tasks, such as autonomous driving, requires reasoning in enormous high-dimensional belief spaces, rendering the problem computationally intensive. While parallelization offers scalability, existing hybrid CPU-GPU solvers face critical bottlenecks due to host-device synchronization latency and branch divergence on SIMT architectures, limiting their utility for real-time planning and hindering real-robot deployment. We present Vec-QMDP, a CPU-native parallel planner that aligns POMDP search with modern CPUs' SIMD architecture, achieving $227\\times$--$1073\\times$ speedup over state-of-the-art serial planners. Vec-QMDP adopts a Data-Oriented Design (DOD), refactoring scattered, pointer-based data structures into contiguous, cache-efficient memory layouts. We further introduce a hierarchical parallelism scheme: distributing sub-trees across independent CPU cores and SIMD lanes, enabling fully vectorized tree expansion and collision checking. Efficiency is maximized with the help of UCB load balancing across trees and a vectorized STR-tree for coarse-level collision checking. Evaluated on large-scale autonomous driving benchmarks, Vec-QMDP achieves state-of-the-art planning performance with millisecond-level latency, establishing CPUs as a high-performance computing platform for large-scale planning under uncertainty.",
          "authors": [
            "Xuanjin Jin",
            "Yanxin Dong",
            "Bin Sun",
            "Huan Xu",
            "Zhihui Hao",
            "XianPeng Lang",
            "Panpan Cai"
          ],
          "published": "2026-02-09T07:15:19Z",
          "updated": "2026-02-09T07:15:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08334v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08328v1",
          "title": "Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation",
          "summary": "Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture. In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion. In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.",
          "authors": [
            "Yi-Hsuan Hsiao",
            "Quang Phuc Kieu",
            "Zhongtao Guan",
            "Suhan Kim",
            "Jiaze Cai",
            "Owen Matteson",
            "Jonathan P. How",
            "Elizabeth Farrell Helbling",
            "YuFeng Chen"
          ],
          "published": "2026-02-09T07:03:53Z",
          "updated": "2026-02-09T07:03:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08328v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08326v1",
          "title": "Personalized Autonomous Driving via Optimal Control with Clearance Constraints from Questionnaires",
          "summary": "Driving without considering the preferred separation distance from surrounding vehicles may cause discomfort for users. To address this limitation, we propose a planning framework that explicitly incorporates user preferences regarding the desired level of safe clearance from surrounding vehicles. We design a questionnaire purposefully tailored to capture user preferences relevant to our framework, while minimizing unnecessary questions. Specifically, the questionnaire considers various interaction-relevant factors, including the surrounding vehicle's size, speed, position, and maneuvers of surrounding vehicles, as well as the maneuvers of the ego vehicle. The response indicates the user-preferred clearance for the scenario defined by the question and is incorporated as constraints in the optimal control problem. However, it is impractical to account for all possible scenarios that may arise in a driving environment within a single optimal control problem, as the resulting computational complexity renders real-time implementation infeasible. To overcome this limitation, we approximate the original problem by decomposing it into multiple subproblems, each dealing with one fixed scenario. We then solve these subproblems in parallel and select one using the cost function from the original problem. To validate our work, we conduct simulations using different user responses to the questionnaire. We assess how effectively our planner reflects user preferences compared to preference-agnostic baseline planners by measuring preference alignment.",
          "authors": [
            "Yongjae Lim",
            "Dabin Kim",
            "H. Jin Kim"
          ],
          "published": "2026-02-09T07:00:36Z",
          "updated": "2026-02-09T07:00:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08326v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08298v1",
          "title": "Benchmarking Autonomous Vehicles: A Driver Foundation Model Framework",
          "summary": "Autonomous vehicles (AVs) are poised to revolutionize global transportation systems. However, its widespread acceptance and market penetration remain significantly below expectations. This gap is primarily driven by persistent challenges in safety, comfort, commuting efficiency and energy economy when compared to the performance of experienced human drivers. We hypothesize that these challenges can be addressed through the development of a driver foundation model (DFM). Accordingly, we propose a framework for establishing DFMs to comprehensively benchmark AVs. Specifically, we describe a large-scale dataset collection strategy for training a DFM, discuss the core functionalities such a model should possess, and explore potential technical solutions to realize these functionalities. We further present the utility of the DFM across the operational spectrum, from defining human-centric safety envelopes to establishing benchmarks for energy economy. Overall, We aim to formalize the DFM concept and introduce a new paradigm for the systematic specification, verification and validation of AVs.",
          "authors": [
            "Yuxin Zhang",
            "Cheng Wang",
            "Hubert P. H. Shum"
          ],
          "published": "2026-02-09T06:07:48Z",
          "updated": "2026-02-09T06:07:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08298v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08285v1",
          "title": "ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects",
          "summary": "Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.",
          "authors": [
            "Josh Pinskier",
            "Sarah Baldwin",
            "Stephen Rodan",
            "David Howard"
          ],
          "published": "2026-02-09T05:35:08Z",
          "updated": "2026-02-09T05:35:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08285v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08278v1",
          "title": "DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer",
          "summary": "Dexterous manipulation remains one of the most challenging problems in robotics, requiring coherent control of high-DoF hands and arms under complex, contact-rich dynamics. A major barrier is embodiment variability: different dexterous hands exhibit distinct kinematics and dynamics, forcing prior methods to train separate policies or rely on shared action spaces with per-embodiment decoder heads. We present DexFormer, an end-to-end, dynamics-aware cross-embodiment policy built on a modified transformer backbone that conditions on historical observations. By using temporal context to infer morphology and dynamics on the fly, DexFormer adapts to diverse hand configurations and produces embodiment-appropriate control actions. Trained over a variety of procedurally generated dexterous-hand assets, DexFormer acquires a generalizable manipulation prior and exhibits strong zero-shot transfer to Leap Hand, Allegro Hand, and Rapid Hand. Our results show that a single policy can generalize across heterogeneous hand embodiments, establishing a scalable foundation for cross-embodiment dexterous manipulation. Project website: https://davidlxu.github.io/DexFormer-web/.",
          "authors": [
            "Ke Zhang",
            "Lixin Xu",
            "Chengyi Song",
            "Junzhe Xu",
            "Xiaoyi Lin",
            "Zeyu Jiang",
            "Renjing Xu"
          ],
          "published": "2026-02-09T05:16:48Z",
          "updated": "2026-02-09T05:16:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08278v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08266v1",
          "title": "Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes",
          "summary": "In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.",
          "authors": [
            "Seunghoon Jeong",
            "Eunho Lee",
            "Jeongyun Kim",
            "Ayoung Kim"
          ],
          "published": "2026-02-09T04:50:36Z",
          "updated": "2026-02-09T04:50:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08266v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08251v1",
          "title": "Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control",
          "summary": "Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.",
          "authors": [
            "Yuanzhu Zhan",
            "Yufei Jiang",
            "Muqing Cao",
            "Junyi Geng"
          ],
          "published": "2026-02-09T04:10:39Z",
          "updated": "2026-02-09T04:10:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08251v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08245v1",
          "title": "STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction",
          "summary": "Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.",
          "authors": [
            "Jinhao Li",
            "Yuxuan Cong",
            "Yingqiao Wang",
            "Hao Xia",
            "Shan Huang",
            "Yijia Zhang",
            "Ningyi Xu",
            "Guohao Dai"
          ],
          "published": "2026-02-09T03:50:40Z",
          "updated": "2026-02-09T03:50:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08245v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08189v1",
          "title": "Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments",
          "summary": "Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.",
          "authors": [
            "Seoyeon Jang",
            "Alex Junho Lee",
            "I Made Aswin Nahrendra",
            "Hyun Myung"
          ],
          "published": "2026-02-09T01:15:29Z",
          "updated": "2026-02-09T01:15:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08189v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15891v1",
          "title": "Learning to Drive in New Cities Without Human Demonstrations",
          "summary": "While autonomous vehicles have achieved reliable performance within specific operating regions, their deployment to new cities remains costly and slow. A key bottleneck is the need to collect many human demonstration trajectories when adapting driving policies to new cities that differ from those seen in training in terms of road geometry, traffic rules, and interaction patterns. In this paper, we show that self-play multi-agent reinforcement learning can adapt a driving policy to a substantially different target city using only the map and meta-information, without requiring any human demonstrations from that city. We introduce NO data Map-based self-play for Autonomous Driving (NOMAD), which enables policy adaptation in a simulator constructed based on the target-city map. Using a simple reward function, NOMAD substantially improves both task success rate and trajectory realism in target cities, demonstrating an effective and scalable alternative to data-intensive city-transfer methods. Project Page: https://nomaddrive.github.io/",
          "authors": [
            "Zilin Wang",
            "Saeed Rahmani",
            "Daphne Cornelisse",
            "Bidipta Sarkar",
            "Alexander David Goldie",
            "Jakob Nicolaus Foerster",
            "Shimon Whiteson"
          ],
          "published": "2026-02-09T00:31:20Z",
          "updated": "2026-02-09T00:31:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15891v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08167v1",
          "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning",
          "summary": "Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.",
          "authors": [
            "Milan Ganai",
            "Katie Luo",
            "Jonas Frey",
            "Clark Barrett",
            "Marco Pavone"
          ],
          "published": "2026-02-09T00:10:17Z",
          "updated": "2026-02-09T00:10:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08167v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08123v1",
          "title": "Adding More Value Than Work: Practical Guidelines for Integrating Robots into Intercultural Competence Learning",
          "summary": "While social robots have demonstrated effectiveness in supporting students' intercultural competence development, it is unclear how they can effectively be adopted for integrated use in K-12 schools. We conducted two phases of design workshops with teachers, where they co-designed robot-mediated intercultural activities while considering student needs and school integration concerns. Using thematic analysis, we identify appropriate scenarios and roles for classroom robots, explore how robots could complement rather than replace teachers, and consider how to address ethical and compliance considerations. Our findings provide practical design guidelines for the HRI community to develop social robots that can effectively support intercultural education in K-12 schools.",
          "authors": [
            "Zhennan Yi",
            "Sophia Sakakibara Capello",
            "Randy Gomez",
            "Selma Šabanović"
          ],
          "published": "2026-02-08T21:01:33Z",
          "updated": "2026-02-08T21:01:33Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08123v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08116v1",
          "title": "From Ellipsoids to Midair Control of Dynamic Hitches",
          "summary": "The ability to dynamically manipulate interaction between cables, carried by pairs of aerial vehicles attached to the ends of each cable, can greatly improve the versatility and agility of cable-assisted aerial manipulation. Such interlacing cables create hitches by winding two or more cables around each other, which can enclose payloads or can further develop into knots. Dynamic modeling and control of such hitches is key to mastering the inter-cable manipulation in context of cable-suspended aerial manipulation. This paper introduces an ellipsoid-based kinematic model to connect the geometric nature of a hitch created by two cables and the dynamics of the hitch driven by four aerial vehicles, which reveals the control-affine form of the system. As the constraint for maintaining tension of a cable is also control-affine, we design a quadratic programming-based controller that combines Control Lyapunov and High-Order Control Barrier Functions (CLF-HOCBF-QP) to precisely track a desired hitch position and system shape while enforcing safety constraints like cable tautness. We convert desired geometric reference configurations into target robot positions and introduce a composite error into the Lyapunov function to ensure a relative degree of one to the input. Numerical simulations validate our approach, demonstrating stable, high-speed tracking of dynamic references.",
          "authors": [
            "Jiawei Xu",
            "Subhrajit Bhattacharya",
            "David Saldaña"
          ],
          "published": "2026-02-08T20:44:06Z",
          "updated": "2026-02-08T20:44:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08116v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08058v1",
          "title": "Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling",
          "summary": "In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.",
          "authors": [
            "Xihang Yu",
            "Rajat Talak",
            "Lorenzo Shaikewitz",
            "Luca Carlone"
          ],
          "published": "2026-02-08T17:04:54Z",
          "updated": "2026-02-08T17:04:54Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08058v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.08006v1",
          "title": "ForecastOcc: Vision-based Semantic Occupancy Forecasting",
          "summary": "Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.",
          "authors": [
            "Riya Mohan",
            "Juana Valeria Hurtado",
            "Rohit Mohan",
            "Abhinav Valada"
          ],
          "published": "2026-02-08T15:16:06Z",
          "updated": "2026-02-08T15:16:06Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.08006v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07984v1",
          "title": "Analyzing the Impact of Simulation Fidelity on the Evaluation of Autonomous Driving Motion Control",
          "summary": "Simulation is crucial in the development of autonomous driving software. In particular, assessing control algorithms requires an accurate vehicle dynamics simulation. However, recent publications use models with varying levels of detail. This disparity makes it difficult to compare individual control algorithms. Therefore, this paper aims to investigate the influence of the fidelity of vehicle dynamics modeling on the closed-loop behavior of trajectory-following controllers. For this purpose, we introduce a comprehensive Autoware-compatible vehicle model. By simplifying this, we derive models with varying fidelity. Evaluating over 550 simulation runs allows us to quantify each model's approximation quality compared to real-world data. Furthermore, we investigate whether the influence of model simplifications changes with varying margins to the acceleration limit of the vehicle. From this, we deduce to which degree a vehicle model can be simplified to evaluate control algorithms depending on the specific application. The real-world data used to validate the simulation environment originate from the Indy Autonomous Challenge race at the Autodromo Nazionale di Monza in June 2023. They show the fastest fully autonomous lap of TUM Autonomous Motorsport, with vehicle speeds reaching 267 kph and lateral accelerations of up to 15 mps2.",
          "authors": [
            "Simon Sagmeister",
            "Panagiotis Kounatidis",
            "Sven Goblirsch",
            "Markus Lienkamp"
          ],
          "published": "2026-02-08T14:21:14Z",
          "updated": "2026-02-08T14:21:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07984v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07938v1",
          "title": "Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps",
          "summary": "Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.",
          "authors": [
            "Rabbia Asghar",
            "Lukas Rummelhard",
            "Wenqian Liu",
            "Anne Spalanzani",
            "Christian Laugier"
          ],
          "published": "2026-02-08T12:13:06Z",
          "updated": "2026-02-08T12:13:06Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07938v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07932v1",
          "title": "Feasibility-Guided Planning over Multi-Specialized Locomotion Policies",
          "summary": "Planning over unstructured terrain presents a significant challenge in the field of legged robotics. Although recent works in reinforcement learning have yielded various locomotion strategies, planning over multiple experts remains a complex issue. Existing approaches encounter several constraints: traditional planners are unable to integrate skill-specific policies, whereas hierarchical learning frameworks often lose interpretability and require retraining whenever new policies are added. In this paper, we propose a feasibility-guided planning framework that successfully incorporates multiple terrain-specific policies. Each policy is paired with a Feasibility-Net, which learned to predict feasibility tensors based on the local elevation maps and task vectors. This integration allows classical planning algorithms to derive optimal paths. Through both simulated and real-world experiments, we demonstrate that our method efficiently generates reliable plans across diverse and challenging terrains, while consistently aligning with the capabilities of the underlying policies.",
          "authors": [
            "Ying-Sheng Luo",
            "Lu-Ching Wang",
            "Hanjaya Mandala",
            "Yu-Lun Chou",
            "Guilherme Christmann",
            "Yu-Chung Chen",
            "Yung-Shun Chan",
            "Chun-Yi Lee",
            "Wei-Chao Chen"
          ],
          "published": "2026-02-08T11:58:50Z",
          "updated": "2026-02-08T11:58:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07932v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07924v1",
          "title": "Optimized Human-Robot Co-Dispatch Planning for Petro-Site Surveillance under Varying Criticalities",
          "summary": "Securing petroleum infrastructure requires balancing autonomous system efficiency with human judgment for threat escalation, a challenge unaddressed by classical facility location models assuming homogeneous resources. This paper formulates the Human-Robot Co-Dispatch Facility Location Problem (HRCD-FLP), a capacitated facility location variant incorporating tiered infrastructure criticality, human-robot supervision ratio constraints, and minimum utilization requirements. We evaluate command center selection across three technology maturity scenarios. Results show transitioning from conservative (1:3 human-robot supervision) to future autonomous operations (1:10) yields significant cost reduction while maintaining complete critical infrastructure coverage. For small problems, exact methods dominate in both cost and computation time; for larger problems, the proposed heuristic achieves feasible solutions in under 3 minutes with approximately 14% optimality gap where comparison is possible. From systems perspective, our work demonstrate that optimized planning for human-robot teaming is key to achieve both cost-effective and mission-reliable deployments.",
          "authors": [
            "Nur Ahmad Khatim",
            "Mansur Arief"
          ],
          "published": "2026-02-08T11:46:40Z",
          "updated": "2026-02-08T11:46:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07924v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07913v1",
          "title": "Multi-Agent Route Planning as a QUBO Problem",
          "summary": "Multi-Agent Route Planning considers selecting vehicles, each associated with a single predefined route, such that the spatial coverage of a road network is increased while redundant overlaps are limited. This paper gives a formal problem definition, proves NP-hardness by reduction from the Weighted Set Packing problem, and derives a Quadratic Unconstrained Binary Optimization formulation whose coefficients directly encode unique coverage rewards and pairwise overlap penalties. A single penalty parameter controls the coverage-overlap trade-off. We distinguish between a soft regime, which supports multi-objective exploration, and a hard regime, in which the penalty is strong enough to effectively enforce near-disjoint routes. We describe a practical pipeline for generating city instances, constructing candidate routes, building the QUBO matrix, and solving it with an exact mixed-integer solver (Gurobi), simulated annealing, and D-Wave hybrid quantum annealing. Experiments on Barcelona instances with up to 10 000 vehicles reveal a clear coverage-overlap knee and show that Pareto-optimal solutions are mainly obtained under the hard-penalty regime, while D-Wave hybrid solvers and Gurobi achieve essentially identical objective values with only minor differences in runtime as problem size grows.",
          "authors": [
            "Renáta Rusnáková",
            "Martin Chovanec",
            "Juraj Gazda"
          ],
          "published": "2026-02-08T11:18:45Z",
          "updated": "2026-02-08T11:18:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "quant-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07913v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07901v1",
          "title": "Incremental Mapping with Measurement Synchronization & Compression",
          "summary": "Modern autonomous vehicles and robots utilize versatile sensors for localization and mapping. The fidelity of these maps is paramount, as an accurate environmental representation is a prerequisite for stable and precise localization. Factor graphs provide a powerful approach for sensor fusion, enabling the estimation of the maximum a posteriori solution. However, the discrete nature of graph-based representations, combined with asynchronous sensor measurements, complicates consistent state estimation. The design of an optimal factor graph topology remains an open challenge, especially in multi-sensor systems with asynchronous data. Conventional approaches rely on a rigid graph structure, which becomes inefficient with sensors of disparate rates. Although preintegration techniques can mitigate this for high-rate sensors, their applicability is limited. To address this problem, this work introduces a novel approach that incrementally constructs connected factor graphs, ensuring the incorporation of all available sensor data by choosing the optimal graph topology based on the external evaluation criteria. The proposed methodology facilitates graph compression, reducing the number of nodes (optimized variables) by ~30% on average while maintaining map quality at a level comparable to conventional approaches.",
          "authors": [
            "Mark Griguletskii",
            "Danil Belov",
            "Pavel Osinenko"
          ],
          "published": "2026-02-08T10:43:11Z",
          "updated": "2026-02-08T10:43:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07901v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07888v1",
          "title": "Research on a Camera Position Measurement Method based on a Parallel Perspective Error Transfer Model",
          "summary": "Camera pose estimation from sparse correspondences is a fundamental problem in geometric computer vision and remains particularly challenging in near-field scenarios, where strong perspective effects and heterogeneous measurement noise can significantly degrade the stability of analytic PnP solutions. In this paper, we present a geometric error propagation framework for camera pose estimation based on a parallel perspective approximation. By explicitly modeling how image measurement errors propagate through perspective geometry, we derive an error transfer model that characterizes the relationship between feature point distribution, camera depth, and pose estimation uncertainty. Building on this analysis, we develop a pose estimation method that leverages parallel perspective initialization and error-aware weighting within a Gauss-Newton optimization scheme, leading to improved robustness in proximity operations. Extensive experiments on both synthetic data and real-world images, covering diverse conditions such as strong illumination, surgical lighting, and underwater low-light environments, demonstrate that the proposed approach achieves accuracy and robustness comparable to state-of-the-art analytic and iterative PnP methods, while maintaining high computational efficiency. These results highlight the importance of explicit geometric error modeling for reliable camera pose estimation in challenging near-field settings.",
          "authors": [
            "Ning Hu",
            "Shuai Li",
            "Jindong Tan"
          ],
          "published": "2026-02-08T09:40:14Z",
          "updated": "2026-02-08T09:40:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07888v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07846v2",
          "title": "System-Level Error Propagation and Tail-Risk Amplification in Reference-Based Robotic Navigation",
          "summary": "Image guided robotic navigation systems often rely on reference based geometric perception pipelines, where accurate spatial mapping is established through multi stage estimation processes. In biplanar X ray guided navigation, such pipelines are widely used due to their real time capability and geometric interpretability. However, navigation reliability can be constrained by an overlooked system level failure mechanism in which installation induced structural perturbations introduced at the perception stage are progressively amplified along the perception reconstruction execution chain and dominate execution level error and tail risk behavior. This paper investigates this mechanism from a system level perspective and presents a unified error propagation modeling framework that characterizes how installation induced structural perturbations propagate and couple with pixel level observation noise through biplanar imaging, projection matrix estimation, triangulation, and coordinate mapping. Using first order analytic uncertainty propagation and Monte Carlo simulations, we analyze dominant sensitivity channels and quantify worst case error behavior beyond mean accuracy metrics. The results show that rotational installation error is a primary driver of system level error amplification, while translational misalignment of comparable magnitude plays a secondary role under typical biplanar geometries. Real biplanar X ray bench top experiments further confirm that the predicted amplification trends persist under realistic imaging conditions. These findings reveal a broader structural limitation of reference based multi stage geometric perception pipelines and provide a framework for system level reliability analysis and risk aware design in safety critical robotic navigation systems.",
          "authors": [
            "Ning Hu",
            "Maochen Li",
            "Senhao Cao"
          ],
          "published": "2026-02-08T07:24:18Z",
          "updated": "2026-02-15T11:13:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07846v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07845v1",
          "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
          "summary": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
          "authors": [
            "Yalcin Tur",
            "Jalal Naghiyev",
            "Haoquan Fang",
            "Wei-Chuan Tsai",
            "Jiafei Duan",
            "Dieter Fox",
            "Ranjay Krishna"
          ],
          "published": "2026-02-08T07:21:01Z",
          "updated": "2026-02-08T07:21:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07845v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07837v3",
          "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
          "summary": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.",
          "authors": [
            "Hongzhi Zang",
            "Shu'ang Yu",
            "Hao Lin",
            "Tianxing Zhou",
            "Zefang Huang",
            "Zhen Guo",
            "Xin Xu",
            "Jiakai Zhou",
            "Yuze Sheng",
            "Shizhe Zhang",
            "Feng Gao",
            "Wenhao Tang",
            "Yufeng Yue",
            "Quanlu Zhang",
            "Xinlei Chen",
            "Chao Yu",
            "Yu Wang"
          ],
          "published": "2026-02-08T06:23:43Z",
          "updated": "2026-02-12T08:08:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07837v3",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07776v1",
          "title": "CoLF: Learning Consistent Leader-Follower Policies for Vision-Language-Guided Multi-Robot Cooperative Transport",
          "summary": "In this study, we address vision-language-guided multi-robot cooperative transport, where each robot grounds natural-language instructions from onboard camera observations. A key challenge in this decentralized setting is perceptual misalignment across robots, where viewpoint differences and language ambiguity can yield inconsistent interpretations and degrade cooperative transport. To mitigate this problem, we adopt a dependent leader-follower design, where one robot serves as the leader and the other as the follower. Although such a leader-follower structure appears straightforward, learning with independent and symmetric agents often yields symmetric or unstable behaviors without explicit inductive biases. To address this challenge, we propose Consistent Leader-Follower (CoLF), a multi-agent reinforcement learning (MARL) framework for stable leader-follower role differentiation. CoLF consists of two key components: (1) an asymmetric policy design that induces leader-follower role differentiation, and (2) a mutual-information-based training objective that maximizes a variational lower bound, encouraging the follower to predict the leader's action from its local observation. The leader and follower policies are jointly optimized under the centralized training and decentralized execution (CTDE) framework to balance task execution and consistent cooperative behaviors. We validate CoLF in both simulation and real-robot experiments using two quadruped robots. The demonstration video is available at https://sites.google.com/view/colf/.",
          "authors": [
            "Joachim Yann Despature",
            "Kazuki Shibata",
            "Takamitsu Matsubara"
          ],
          "published": "2026-02-08T02:21:47Z",
          "updated": "2026-02-08T02:21:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07776v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07736v1",
          "title": "Global Symmetry and Orthogonal Transformations from Geometrical Moment $n$-tuples",
          "summary": "Detecting symmetry is crucial for effective object grasping for several reasons. Recognizing symmetrical features or axes within an object helps in developing efficient grasp strategies, as grasping along these axes typically results in a more stable and balanced grip, thereby facilitating successful manipulation. This paper employs geometrical moments to identify symmetries and estimate orthogonal transformations, including rotations and mirror transformations, for objects centered at the frame origin. It provides distinctive metrics for detecting symmetries and estimating orthogonal transformations, encompassing rotations, reflections, and their combinations. A comprehensive methodology is developed to obtain these functions in n-dimensional space, specifically moment \\( n \\)-tuples. Extensive validation tests are conducted on both 2D and 3D objects to ensure the robustness and reliability of the proposed approach. The proposed method is also compared to state-of-the-art work using iterative optimization for detecting multiple planes of symmetry. The results indicate that combining our method with the iterative one yields satisfactory outcomes in terms of the number of symmetry planes detected and computation time.",
          "authors": [
            "Omar Tahri"
          ],
          "published": "2026-02-08T00:07:11Z",
          "updated": "2026-02-08T00:07:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07736v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07680v2",
          "title": "Vision and Language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning",
          "summary": "Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.",
          "authors": [
            "Ross Greer",
            "Maitrayee Keskar",
            "Angel Martinez-Sanchez",
            "Parthib Roy",
            "Shashank Shriram",
            "Mohan Trivedi"
          ],
          "published": "2026-02-07T20:04:21Z",
          "updated": "2026-02-18T11:33:50Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07680v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07677v1",
          "title": "Affine Transformable Unmanned Ground Vehicle",
          "summary": "This paper develops the proof of concept for a novel affine transformable unmanned ground vehicle (ATUGV) with the capability of safe and aggressive deformation while carrying multiple payloads. The ATUGV is a multi-body system with mobile robots that can be used to power the ATUGV morphable motion, powered cells to enclose the mobile robots, unpowered cells to contain payloads, and a deformable structure to integrate cells through bars and joints. The objective is that all powered and unpowered cells motion can safely track a desired affine transformation, where an affine transformation can be decomposed into translation, rigid body rotation, and deformation. To this end, the paper first uses a deep neural network to structure cell interconnection in such a way that every cell can freely move over the deformation plane, and the entire structure can reconfigurably deform to track a desired affine transformation. Then, the mobile robots, contained by the powered cells and stepper motors, regulating the connections of the powered and unpowered cells, design the proper controls so that all cells safely track the desired affine transformation. The functionality of the proposed ATUGV is validated through hardware experimentation and simulation.",
          "authors": [
            "Aron Mathias",
            "Mohammad Ghufran",
            "Jack Hughes",
            "Hossein Rastgoftar"
          ],
          "published": "2026-02-07T19:56:27Z",
          "updated": "2026-02-07T19:56:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07677v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07668v1",
          "title": "Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making",
          "summary": "The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., \"turn after that red building\") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.",
          "authors": [
            "Ross Greer",
            "Laura Fleig",
            "Maitrayee Keskar",
            "Erika Maquiling",
            "Giovanni Tapia Lopez",
            "Angel Martinez-Sanchez",
            "Parthib Roy",
            "Jake Rattigan",
            "Mira Sur",
            "Alejandra Vidrio",
            "Thomas Marcotte",
            "Mohan Trivedi"
          ],
          "published": "2026-02-07T19:25:02Z",
          "updated": "2026-02-07T19:25:02Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07668v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07629v2",
          "title": "LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation",
          "summary": "We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time.",
          "authors": [
            "Nitesh Subedi",
            "Adam Haroon",
            "Samuel Tetteh",
            "Prajwal Koirala",
            "Cody Fleming",
            "Soumik Sarkar"
          ],
          "published": "2026-02-07T17:20:43Z",
          "updated": "2026-02-10T02:40:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07629v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07598v1",
          "title": "\"Meet My Sidekick!\": Effects of Separate Identities and Control of a Single Robot in HRI",
          "summary": "The presentation of a robot's capability and identity directly influences a human collaborator's perception and implicit trust in the robot. Unlike humans, a physical robot can simultaneously present different identities and have them reside and control different parts of the robot. This paper presents a novel study that investigates how users perceive a robot where different robot control domains (head and gripper) are presented as independent robots. We conducted a mixed design study where participants experienced one of three presentations: a single robot, two agents with shared full control (co-embodiment), or two agents with split control across robot control domains (split-embodiment). Participants underwent three distinct tasks -- a mundane data entry task where the robot provides motivational support, an individual sorting task with isolated robot failures, and a collaborative arrangement task where the robot causes a failure that directly affects the human participant. Participants perceived the robot as residing in the different control domains and were able to associate robot failure with different identities. This work signals how future robots can leverage different embodiment configurations to obtain the benefit of multiple robots within a single body.",
          "authors": [
            "Drake Moore",
            "Arushi Aggarwal",
            "Emily Taylor",
            "Sarah Zhang",
            "Taskin Padir",
            "Xiang Zhi Tan"
          ],
          "published": "2026-02-07T15:51:33Z",
          "updated": "2026-02-07T15:51:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07598v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07541v1",
          "title": "Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning",
          "summary": "As robots are expected to perform increasingly diverse tasks, they must understand not only low-level actions but also the higher-level structure that determines how a task should unfold. Existing vision-language-action (VLA) models struggle with this form of task-level reasoning. They either depend on prompt-based in-context decomposition, which is unstable and sensitive to linguistic variations, or end-to-end long-horizon training, which requires large-scale demonstrations and entangles task-level reasoning with low-level control. We present in-parameter structured task reasoning (iSTAR), a framework for enhancing VLA models via functional differentiation induced by in-parameter structural reasoning. Instead of treating VLAs as monolithic policies, iSTAR embeds task-level semantic structure directly into model parameters, enabling differentiated task-level inference without external planners or handcrafted prompt inputs. This injected structure takes the form of implicit dynamic scene-graph knowledge that captures object relations, subtask semantics, and task-level dependencies in parameter space. Across diverse manipulation benchmarks, iSTAR achieves more reliable task decompositions and higher success rates than both in-context and end-to-end VLA baselines, demonstrating the effectiveness of parameter-space structural reasoning for functional differentiation and improved generalization across task variations.",
          "authors": [
            "Jingyi Hou",
            "Leyu Zhou",
            "Chenchen Jing",
            "Jinghan Yang",
            "Xinbo Yu",
            "Wei He"
          ],
          "published": "2026-02-07T13:31:11Z",
          "updated": "2026-02-07T13:31:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07541v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07506v2",
          "title": "VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots",
          "summary": "Humanoid facial expression shadowing enables robots to realistically imitate human facial expressions in real time, which is critical for lifelike, facially expressive humanoid robots and affective human-robot interaction. Existing progress in humanoid facial expression imitation remains limited, often failing to achieve either real-time performance or realistic expressiveness due to offline video-based inference designs and insufficient ability to capture and transfer subtle expression details. To address these limitations, we present VividFace, a real-time and realistic facial expression shadowing system for humanoid robots. An optimized imitation framework X2CNet++ enhances expressiveness by fine-tuning the human-to-humanoid facial motion transfer module and introducing a feature-adaptation training strategy for better alignment across different image sources. Real-time shadowing is further enabled by a video-stream-compatible inference pipeline and a streamlined workflow based on asynchronous I/O for efficient communication across devices. VividFace produces vivid humanoid faces by mimicking human facial expressions within 0.05 seconds, while generalizing across diverse facial configurations. Extensive real-world demonstrations validate its practical utility. Videos are available at: https://lipzh5.github.io/VividFace/.",
          "authors": [
            "Peizhen Li",
            "Longbing Cao",
            "Xiao-Ming Wu",
            "Yang Zhang"
          ],
          "published": "2026-02-07T11:51:50Z",
          "updated": "2026-02-14T10:07:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07506v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07439v1",
          "title": "TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control",
          "summary": "Recent advances in humanoid whole-body motion tracking have enabled the execution of diverse and highly coordinated motions on real hardware. However, existing controllers are commonly driven either by predefined motion trajectories, which offer limited flexibility when user intent changes, or by continuous human teleoperation, which requires constant human involvement and limits autonomy. This work addresses the problem of how to drive a universal humanoid controller in a real-time and interactive manner. We present TextOp, a real-time text-driven humanoid motion generation and control framework that supports streaming language commands and on-the-fly instruction modification during execution. TextOp adopts a two-level architecture in which a high-level autoregressive motion diffusion model continuously generates short-horizon kinematic trajectories conditioned on the current text input, while a low-level motion tracking policy executes these trajectories on a physical humanoid robot. By bridging interactive motion generation with robust whole-body control, TextOp unlocks free-form intent expression and enables smooth transitions across multiple challenging behaviors such as dancing and jumping, within a single continuous motion execution. Extensive real-robot experiments and offline evaluations demonstrate instant responsiveness, smooth whole-body motion, and precise control. The project page and the open-source code are available at https://text-op.github.io/",
          "authors": [
            "Weiji Xie",
            "Jiakun Zheng",
            "Jinrui Han",
            "Jiyuan Shi",
            "Weinan Zhang",
            "Chenjia Bai",
            "Xuelong Li"
          ],
          "published": "2026-02-07T08:42:11Z",
          "updated": "2026-02-07T08:42:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07439v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07434v1",
          "title": "Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots",
          "summary": "Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \\underline{\\textit{S}}peech, \\underline{\\textit{E}}motion, and \\underline{\\textit{M}}otion, we present \\textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \\underline{\\textit{e}}dge-deployed versions (\\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.",
          "authors": [
            "Songhua Yang",
            "Xuetao Li",
            "Xuanye Fei",
            "Mengde Li",
            "Miao Li"
          ],
          "published": "2026-02-07T08:32:54Z",
          "updated": "2026-02-07T08:32:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07434v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07413v2",
          "title": "Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity",
          "summary": "There has been rapid and dramatic progress in learning complex visuo-motor manipulation skills from demonstrations, thanks in part to expressive policy classes that employ diffusion- and transformer-based backbones. However, these design choices require significant data and computational resources and remain far from reliable, particularly within the context of multi-fingered dexterous manipulation. Fundamentally, they model skills as reactive mappings and rely on fixed-horizon action chunking to mitigate jitter, creating a rigid trade-off between temporal coherence and reactivity. In this work, we introduce Unified Behavioral Models (UBMs), a framework that learns to represent dexterous skills as coupled dynamical systems that capture how visual features of the environment (visual flow) and proprioceptive states of the robot (action flow) co-evolve. By capturing such behavioral dynamics, UBMs can ensure temporal coherence by construction rather than by heuristic averaging. To operationalize these models, we propose Koopman-UBM, a first instantiation of UBMs that leverages Koopman Operator theory to effectively learn a unified representation in which the joint flow of latent visual and proprioceptive features is governed by a structured linear system. We demonstrate that Koopman-UBM can be viewed as an implicit planner: given an initial condition, it computes the desired robot behavior with the resulting flow of visual features over the entire skill horizon. To enable reactivity, we introduce an online replanning strategy in which the model acts as its own runtime monitor that automatically triggers replanning when predicted and observed visual flow diverge. Across seven simulated and two real-world tasks, we demonstrate that K-UBM matches or exceeds the performance of SOTA baselines, while offering faster inference, smooth execution, robustness to occlusions, and flexible replanning.",
          "authors": [
            "Yunhai Han",
            "Linhao Bai",
            "Ziyu Xiao",
            "Zhaodong Yang",
            "Yogita Choudhary",
            "Krishna Jha",
            "Chuizheng Kong",
            "Shreyas Kousik",
            "Harish Ravichandar"
          ],
          "published": "2026-02-07T07:18:00Z",
          "updated": "2026-02-10T17:20:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07413v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07395v1",
          "title": "Haptically Experienced Animacy Facilitates Emotion Regulation: A Theory-Driven Investigation",
          "summary": "Emotion regulation (ER) is essential to mental well-being but often difficult to access, especially in high-intensity moments or for individuals with clinical vulnerabilities. While existing technology-based ER tools offer value, they typically rely on self-reflection (e.g., emotion tracking, journaling) or co-regulation through verbal modalities (reminders, text-based conversational tools), which may not be accessible or effective when most needed. The biological role of the touch modality makes it an intriguing alternate pathway, but empirical evidence is limited and under-theorized. Building on our prior theoretical framework describing how a comforting haptic co-regulating adjunct (CHORA) can support ER, we developed a zoomorphic robot CHORA with looped biomimetic breathing and heartbeat behaviors. We evaluated its effects in a mixed-methods in-lab study (N=30), providing physiological, self-report, custom questionnaire, and retrospective interview data. Our findings demonstrate the regulatory effects of haptically experienced animacy, corroborate prior work, and validate CHORA's {theoretically grounded} potential to facilitate four ER strategies.",
          "authors": [
            "Preeti Vyas",
            "Bereket Guta",
            "Tim G. Zhou",
            "Noor Naila Himam",
            "Andero Uusberg",
            "Karon E. MacLean"
          ],
          "published": "2026-02-07T06:22:05Z",
          "updated": "2026-02-07T06:22:05Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.CY",
            "cs.ET",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07395v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07388v1",
          "title": "Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation",
          "summary": "Generative model-based policies have shown strong performance in imitation-based robotic manipulation by learning action distributions from demonstrations. However, in long-horizon tasks, visually similar observations often recur across execution stages while requiring distinct actions, which leads to ambiguous predictions when policies are conditioned only on instantaneous observations, termed multi-modal action ambiguity (MA2). To address this challenge, we propose the Trace-Focused Diffusion Policy (TF-DP), a simple yet effective diffusion-based framework that explicitly conditions action generation on the robot's execution history. TF-DP represents historical motion as an explicit execution trace and projects it into the visual observation space, providing stage-aware context when current observations alone are insufficient. In addition, the induced trace-focused field emphasizes task-relevant regions associated with historical motion, improving robustness to background visual disturbances. We evaluate TF-DP on real-world robotic manipulation tasks exhibiting pronounced multi-modal action ambiguity and visually cluttered conditions. Experimental results show that TF-DP improves temporal consistency and robustness, outperforming the vanilla diffusion policy by 80.56 percent on tasks with multi-modal action ambiguity and by 86.11 percent under visual disturbances, while maintaining inference efficiency with only a 6.4 percent runtime increase. These results demonstrate that execution-trace conditioning offers a scalable and principled approach for robust long-horizon robotic manipulation within a single policy.",
          "authors": [
            "Yuxuan Hu",
            "Xiangyu Chen",
            "Chuhao Zhou",
            "Yuxi Liu",
            "Gen Li",
            "Jindou Jia",
            "Jianfei Yang"
          ],
          "published": "2026-02-07T06:06:43Z",
          "updated": "2026-02-07T06:06:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07388v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07363v1",
          "title": "UEREBot: Learning Safe Quadrupedal Locomotion under Unstructured Environments and High-Speed Dynamic Obstacles",
          "summary": "Quadruped robots are increasingly deployed in unstructured environments. Safe locomotion in these settings requires long-horizon goal progress, passability over uneven terrain and static constraints, and collision avoidance against high-speed dynamic obstacles. A single system cannot fully satisfy all three objectives simultaneously: planning-based decisions can be too slow, while purely reactive decisions can sacrifice goal progress and passability. To resolve this conflict, we propose UEREBot (Unstructured-Environment Reflexive Evasion Robot), a hierarchical framework that separates slow planning from instantaneous reflexive evasion and coordinates them during execution. UEREBot formulates the task as a constrained optimal control problem blueprint. It adopts a spatial--temporal planner that provides reference guidance toward the goal and threat signals. It then uses a threat-aware handoff to fuse navigation and reflex actions into a nominal command, and a control barrier function shield as a final execution safeguard. We evaluate UEREBot in Isaac Lab simulation and deploy it on a Unitree Go2 quadruped equipped with onboard perception. Across diverse environments with complex static structure and high-speed dynamic obstacles, UEREBot achieves higher avoidance success and more stable locomotion while maintaining goal progress than representative baselines, demonstrating improved safety--progress trade-offs.",
          "authors": [
            "Zihao Xu",
            "Runyu Lei",
            "Zihao Li",
            "Boxi Lin",
            "Ce Hao",
            "Jin Song Dong"
          ],
          "published": "2026-02-07T05:09:48Z",
          "updated": "2026-02-07T05:09:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07363v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07343v1",
          "title": "Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation",
          "summary": "Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.",
          "authors": [
            "Ruturaj Reddy",
            "Hrishav Bakul Barua",
            "Junn Yong Loo",
            "Thanh Thi Nguyen",
            "Ganesh Krishnasamy"
          ],
          "published": "2026-02-07T03:52:04Z",
          "updated": "2026-02-07T03:52:04Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07343v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07341v1",
          "title": "Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions",
          "summary": "This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.",
          "authors": [
            "Yicheng Yang",
            "Ruijiao Li",
            "Lifeng Wang",
            "Shuai Zheng",
            "Shunzheng Ma",
            "Keyu Zhang",
            "Tuoyu Sun",
            "Chenyun Dai",
            "Jie Ding",
            "Zhuo Zou"
          ],
          "published": "2026-02-07T03:47:21Z",
          "updated": "2026-02-07T03:47:21Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07341v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07339v1",
          "title": "RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving",
          "summary": "Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.",
          "authors": [
            "Ruturaj Reddy",
            "Hrishav Bakul Barua",
            "Junn Yong Loo",
            "Thanh Thi Nguyen",
            "Ganesh Krishnasamy"
          ],
          "published": "2026-02-07T03:44:50Z",
          "updated": "2026-02-07T03:44:50Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07339v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07326v1",
          "title": "Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing",
          "summary": "Grasping under limited sensing remains a fundamental challenge for real-world robotic manipulation, as vision and high-resolution tactile sensors often introduce cost, fragility, and integration complexity. This work demonstrates that reliable multifingered grasping can be achieved under extremely minimal sensing by relying solely on uniaxial fingertip force feedback and joint proprioception, without vision or multi-axis/tactile sensing. To enable such blind grasping, we employ an efficient teacher-student training pipeline in which a reinforcement-learned teacher exploits privileged simulation-only observations to generate demonstrations for distilling a transformer-based student policy operating under partial observation. The student policy is trained to act using only sensing modalities available at real-world deployment. We validate the proposed approach on real hardware across 18 objects, including both in-distribution and out-of-distribution cases, achieving a 98.3~$\\%$ overall grasp success rate. These results demonstrate strong robustness and generalization beyond the simulation training distribution, while significantly reducing sensing requirements for real-world grasping systems.",
          "authors": [
            "Edgar Lee",
            "Junho Choi",
            "Taemin Kim",
            "Changjoo Nam",
            "Seokhwan Jeong"
          ],
          "published": "2026-02-07T02:57:47Z",
          "updated": "2026-02-07T02:57:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07326v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07322v1",
          "title": "Action-to-Action Flow Matching",
          "summary": "Diffusion-based policies have recently achieved remarkable success in robotics by formulating action prediction as a conditional denoising process. However, the standard practice of sampling from random Gaussian noise often requires multiple iterative steps to produce clean actions, leading to high inference latency that incurs a major bottleneck for real-time control. In this paper, we challenge the necessity of uninformed noise sampling and propose Action-to-Action flow matching (A2A), a novel policy paradigm that shifts from random sampling to initialization informed by the previous action. Unlike existing methods that treat proprioceptive action feedback as static conditions, A2A leverages historical proprioceptive sequences, embedding them into a high-dimensional latent space as the starting point for action generation. This design bypasses costly iterative denoising while effectively capturing the robot's physical dynamics and temporal continuity. Extensive experiments demonstrate that A2A exhibits high training efficiency, fast inference speed, and improved generalization. Notably, A2A enables high-quality action generation in as few as a single inference step (0.56 ms latency), and exhibits superior robustness to visual perturbations and enhanced generalization to unseen configurations. Lastly, we also extend A2A to video generation, demonstrating its broader versatility in temporal modeling. Project site: https://lorenzo-0-0.github.io/A2A_Flow_Matching.",
          "authors": [
            "Jindou Jia",
            "Gen Li",
            "Xiangyu Chen",
            "Tuo An",
            "Yuxuan Hu",
            "Jingliang Li",
            "Xinying Guo",
            "Jianfei Yang"
          ],
          "published": "2026-02-07T02:39:49Z",
          "updated": "2026-02-07T02:39:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07322v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07264v1",
          "title": "aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones",
          "summary": "Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term \"simulation-to-reality gap\". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.",
          "authors": [
            "Jacopo Panerati",
            "Sina Sajjadi",
            "Sina Soleymanpour",
            "Varunkumar Mehta",
            "Iraj Mantegh"
          ],
          "published": "2026-02-06T23:29:33Z",
          "updated": "2026-02-06T23:29:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.SE"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07264v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07243v1",
          "title": "Realistic Synthetic Household Data Generation at Scale",
          "summary": "Advancements in foundation models have catalyzed research in Embodied AI to develop interactive agents capable of environmental reasoning and interaction. Developing such agents requires diverse, large-scale datasets. Prior frameworks generate synthetic data for long-term human-robot interactions but fail to model the bidirectional influence between human behavior and household environments. Our proposed generative framework creates household datasets at scale through loosely coupled generation of long-term human-robot interactions and environments. Human personas influence environment generation, while environment schematics and semantics shape human-robot interactions. The generated 3D data includes rich static context such as object and environment semantics, and temporal context capturing human and agent behaviors over extended periods. Our flexible tool allows users to define dataset characteristics via natural language prompts, enabling configuration of environment and human activity data through natural language specifications. The tool creates variations of user-defined configurations, enabling scalable data generation. We validate our framework through statistical evaluation using multi-modal embeddings and key metrics: cosine similarity, mutual information gain, intervention analysis, and iterative improvement validation. Statistical comparisons show good alignment with real-world datasets (HOMER) with cosine similarity (0.60), while synthetic datasets (Wang et al.) show moderate alignment (0.27). Intervention analysis across age, organization, and sleep pattern changes shows statistically significant effects (p < 0.001) with large effect sizes (Cohen's d = 0.51-1.12), confirming bidirectional coupling translates persona traits into measurable environmental and behavioral differences. These contributions enable development and testing of household smart devices at scale.",
          "authors": [
            "Siddharth Singh",
            "Ifrah Idrees",
            "Abraham Dauhajre"
          ],
          "published": "2026-02-06T22:49:37Z",
          "updated": "2026-02-06T22:49:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.GR"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07243v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07227v1",
          "title": "Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation",
          "summary": "Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\\%$ on \\texttt{HalfCheetah-v5} and $+53\\%$ on \\texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.",
          "authors": [
            "Nethmi Jayasinghe",
            "Diana Gontero",
            "Spencer T. Brown",
            "Vinod K. Sangwan",
            "Mark C. Hersam",
            "Amit Ranjan Trivedi"
          ],
          "published": "2026-02-06T22:16:00Z",
          "updated": "2026-02-06T22:16:00Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07227v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07209v1",
          "title": "Continuum Robot Localization using Distributed Time-of-Flight Sensors",
          "summary": "Localization and mapping of an environment are crucial tasks for any robot operating in unstructured environments. Time-of-flight (ToF) sensors (e.g.,~lidar) have proven useful in mobile robotics, where high-resolution sensors can be used for simultaneous localization and mapping. In soft and continuum robotics, however, these high-resolution sensors are too large for practical use. This, combined with the deformable nature of such robots, has resulted in continuum robot (CR) localization and mapping in unstructured environments being a largely untouched area. In this work, we present a localization technique for CRs that relies on small, low-resolution ToF sensors distributed along the length of the robot. By fusing measurement information with a robot shape prior, we show that accurate localization is possible despite each sensor experiencing frequent degenerate scenarios. We achieve an average localization error of 2.5cm in position and 7.2° in rotation across all experimental conditions with a 53cm long robot. We demonstrate that the results are repeated across multiple environments, in both simulation and real-world experiments, and study robustness in the estimation to deviations in the prior map.",
          "authors": [
            "Spencer Teetaert",
            "Giammarco Caroleo",
            "Marco Pontin",
            "Sven Lilge",
            "Jessica Burgner-Kahrs",
            "Timothy D. Barfoot",
            "Perla Maiolino"
          ],
          "published": "2026-02-06T21:42:37Z",
          "updated": "2026-02-06T21:42:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07209v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07158v1",
          "title": "A compliant ankle-actuated compass walker with triggering timing control",
          "summary": "Passive dynamic walkers are widely adopted as a mathematical model to represent biped walking. The stable locomotion of these models is limited to tilted surfaces, requiring gravitational energy. Various techniques, such as actuation through the ankle and hip joints, have been proposed to extend the applicability of these models to level ground and rough terrain with improved locomotion efficiency. However, most of these techniques rely on impulsive energy injection schemes and torsional springs, which are quite challenging to implement in a physical platform. Here, a new model is proposed, named triggering controlled ankle actuated compass gait (TC-AACG), which allows non-instantaneous compliant ankle pushoff. The proposed technique can be implemented in physical platforms via series elastic actuators (SEAs). Our systematic examination shows that the proposed approach extends the locomotion capabilities of a biped model compared to impulsive ankle pushoff approach. We provide extensive simulation analysis investigating the locomotion speed, mechanical cost of transport, and basin of attraction of the proposed model.",
          "authors": [
            "Deniz Kerimoglu",
            "Ismail Uyanik"
          ],
          "published": "2026-02-06T20:00:19Z",
          "updated": "2026-02-06T20:00:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07158v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06949v1",
          "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
          "summary": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
          "authors": [
            "Shenyuan Gao",
            "William Liang",
            "Kaiyuan Zheng",
            "Ayaan Malik",
            "Seonghyeon Ye",
            "Sihyun Yu",
            "Wei-Cheng Tseng",
            "Yuzhu Dong",
            "Kaichun Mo",
            "Chen-Hsuan Lin",
            "Qianli Ma",
            "Seungjun Nah",
            "Loic Magne",
            "Jiannan Xiang",
            "Yuqi Xie",
            "Ruijie Zheng",
            "Dantong Niu",
            "You Liang Tan",
            "K. R. Zentner",
            "George Kurian",
            "Suneel Indupuru",
            "Pooya Jannaty",
            "Jinwei Gu",
            "Jun Zhang",
            "Jitendra Malik",
            "Pieter Abbeel",
            "Ming-Yu Liu",
            "Yuke Zhu",
            "Joel Jang",
            "Linxi \"Jim\" Fan"
          ],
          "published": "2026-02-06T18:49:43Z",
          "updated": "2026-02-06T18:49:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06949v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06925v1",
          "title": "Strategizing at Speed: A Learned Model Predictive Game for Multi-Agent Drone Racing",
          "summary": "Autonomous drone racing pushes the boundaries of high-speed motion planning and multi-agent strategic decision-making. Success in this domain requires drones not only to navigate at their limits but also to anticipate and counteract competitors' actions. In this paper, we study a fundamental question that arises in this domain: how deeply should an agent strategize before taking an action? To this end, we compare two planning paradigms: the Model Predictive Game (MPG), which finds interaction-aware strategies at the expense of longer computation times, and contouring Model Predictive Control (MPC), which computes strategies rapidly but does not reason about interactions. We perform extensive experiments to study this trade-off, revealing that MPG outperforms MPC at moderate velocities but loses its advantage at higher speeds due to latency. To address this shortcoming, we propose a Learned Model Predictive Game (LMPG) approach that amortizes model predictive gameplay to reduce latency. In both simulation and hardware experiments, we benchmark our approach against MPG and MPC in head-to-head races, finding that LMPG outperforms both baselines.",
          "authors": [
            "Andrei-Carlo Papuc",
            "Lasse Peters",
            "Sihao Sun",
            "Laura Ferranti",
            "Javier Alonso-Mora"
          ],
          "published": "2026-02-06T18:20:13Z",
          "updated": "2026-02-06T18:20:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.GT"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06925v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06868v1",
          "title": "Consensus-based optimization (CBO): Towards Global Optimality in Robotics",
          "summary": "Zero-order optimization has recently received significant attention for designing optimal trajectories and policies for robotic systems. However, most existing methods (e.g., MPPI, CEM, and CMA-ES) are local in nature, as they rely on gradient estimation. In this paper, we introduce consensus-based optimization (CBO) to robotics, which is guaranteed to converge to a global optimum under mild assumptions. We provide theoretical analysis and illustrative examples that give intuition into the fundamental differences between CBO and existing methods. To demonstrate the scalability of CBO for robotics problems, we consider three challenging trajectory optimization scenarios: (1) a long-horizon problem for a simple system, (2) a dynamic balance problem for a highly underactuated system, and (3) a high-dimensional problem with only a terminal cost. Our results show that CBO is able to achieve lower costs with respect to existing methods on all three challenging settings. This opens a new framework to study global trajectory optimization in robotics.",
          "authors": [
            "Xudong Sun",
            "Armand Jordana",
            "Massimo Fornasier",
            "Jalal Etesami",
            "Majid Khadiv"
          ],
          "published": "2026-02-06T16:55:10Z",
          "updated": "2026-02-06T16:55:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06868v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06864v1",
          "title": "SURE: Safe Uncertainty-Aware Robot-Environment Interaction using Trajectory Optimization",
          "summary": "Robotic tasks involving contact interactions pose significant challenges for trajectory optimization due to discontinuous dynamics. Conventional formulations typically assume deterministic contact events, which limit robustness and adaptability in real-world settings. In this work, we propose SURE, a robust trajectory optimization framework that explicitly accounts for contact timing uncertainty. By allowing multiple trajectories to branch from possible pre-impact states and later rejoin a shared trajectory, SURE achieves both robustness and computational efficiency within a unified optimization framework. We evaluate SURE on two representative tasks with unknown impact times. In a cart-pole balancing task involving uncertain wall location, SURE achieves an average improvement of 21.6% in success rate when branch switching is enabled during control. In an egg-catching experiment using a robotic manipulator, SURE improves the success rate by 40%. These results demonstrate that SURE substantially enhances robustness compared to conventional nominal formulations.",
          "authors": [
            "Zhuocheng Zhang",
            "Haizhou Zhao",
            "Xudong Sun",
            "Aaron M. Johnson",
            "Majid Khadiv"
          ],
          "published": "2026-02-06T16:50:59Z",
          "updated": "2026-02-06T16:50:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06864v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06834v1",
          "title": "Perception-Control Coupled Visual Servoing for Textureless Objects Using Keypoint-Based EKF",
          "summary": "Visual servoing is fundamental to robotic applications, enabling precise positioning and control. However, applying it to textureless objects remains a challenge due to the absence of reliable visual features. Moreover, adverse visual conditions, such as occlusions, often corrupt visual feedback, leading to reduced accuracy and instability in visual servoing. In this work, we build upon learning-based keypoint detection for textureless objects and propose a method that enhances robustness by tightly integrating perception and control in a closed loop. Specifically, we employ an Extended Kalman Filter (EKF) that integrates per-frame keypoint measurements to estimate 6D object pose, which drives pose-based visual servoing (PBVS) for control. The resulting camera motion, in turn, enhances the tracking of subsequent keypoints, effectively closing the perception-control loop. Additionally, unlike standard PBVS, we propose a probabilistic control law that computes both camera velocity and its associated uncertainty, enabling uncertainty-aware control for safe and reliable operation. We validate our approach on real-world robotic platforms using quantitative metrics and grasping experiments, demonstrating that our method outperforms traditional visual servoing techniques in both accuracy and practical application.",
          "authors": [
            "Allen Tao",
            "Jun Yang",
            "Stanko Oparnica",
            "Wenjie Xue"
          ],
          "published": "2026-02-06T16:21:30Z",
          "updated": "2026-02-06T16:21:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06834v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06827v1",
          "title": "DynaRetarget: Dynamically-Feasible Retargeting using Sampling-Based Trajectory Optimization",
          "summary": "In this paper, we introduce DynaRetarget, a complete pipeline for retargeting human motions to humanoid control policies. The core component of DynaRetarget is a novel Sampling-Based Trajectory Optimization (SBTO) framework that refines imperfect kinematic trajectories into dynamically feasible motions. SBTO incrementally advances the optimization horizon, enabling optimization over the entire trajectory for long-horizon tasks. We validate DynaRetarget by successfully retargeting hundreds of humanoid-object demonstrations and achieving higher success rates than the state of the art. The framework also generalizes across varying object properties, such as mass, size, and geometry, using the same tracking objective. This ability to robustly retarget diverse demonstrations opens the door to generating large-scale synthetic datasets of humanoid loco-manipulation trajectories, addressing a major bottleneck in real-world data collection.",
          "authors": [
            "Victor Dhedin",
            "Ilyass Taouil",
            "Shafeef Omar",
            "Dian Yu",
            "Kun Tao",
            "Angela Dai",
            "Majid Khadiv"
          ],
          "published": "2026-02-06T16:14:27Z",
          "updated": "2026-02-06T16:14:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06827v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06811v1",
          "title": "A 26-Gram Butterfly-Inspired Robot Achieving Autonomous Tailless Flight",
          "summary": "Flapping-wing micro air vehicles (FWMAVs) have demonstrated remarkable bio-inspired agility, yet tailless two-winged configurations remain largely unexplored due to their complex fluid-structure and wing-body coupling. Here we present \\textit{AirPulse}, a 26-gram butterfly-inspired FWMAV that achieves fully onboard, closed-loop, untethered flight without auxiliary control surfaces. The AirPulse robot replicates key biomechanical traits of butterfly flight, including low wing aspect ratio, compliant carbon-fiber-reinforced wings, and low-frequency, high-amplitude flapping that induces cyclic variations in the center of gravity and moment of inertia, producing characteristic body undulation. We establish a quantitative mapping between flapping modulation parameters and force-torque generation, and introduce the Stroke Timing Asymmetry Rhythm (STAR) generator, enabling smooth, stable, and linearly parameterized wingstroke asymmetry for flapping control. Integrating these with an attitude controller, the AirPulse robot maintains pitch and yaw stability despite strong oscillatory dynamics. Free-flight experiments demonstrate stable climbing and turning maneuvers via either angle offset or stroke timing modulation, marking the first onboard controlled flight of the lightest two-winged, tailless butterfly-inspired FWMAV reported in peer-reviewed literature. This work corroborates a foundational platform for lightweight, collision-proof FWMAVs, bridging biological inspiration with practical aerial robotics. Their non-invasive maneuverability is ideally suited for real-world applications, such as confined-space inspection and ecological monitoring, inaccessible to traditional drones, while their biomechanical fidelity provides a physical model to decode the principles underlying the erratic yet efficient flight of real butterflies.",
          "authors": [
            "Weibin Gu",
            "Chenrui Feng",
            "Lian Liu",
            "Chen Yang",
            "Xingchi Jiao",
            "Yuhe Ding",
            "Xiaofei Shi",
            "Chao Gao",
            "Alessandro Rizzo",
            "Guyue Zhou"
          ],
          "published": "2026-02-06T15:58:50Z",
          "updated": "2026-02-06T15:58:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06811v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06807v1",
          "title": "SuReNav: Superpixel Graph-based Constraint Relaxation for Navigation in Over-constrained Environments",
          "summary": "We address the over-constrained planning problem in semi-static environments. The planning objective is to find a best-effort solution that avoids all hard constraint regions while minimally traversing the least risky areas. Conventional methods often rely on pre-defined area costs, limiting generalizations. Further, the spatial continuity of navigation spaces makes it difficult to identify regions that are passable without overestimation. To overcome these challenges, we propose SuReNav, a superpixel graph-based constraint relaxation and navigation method that imitates human-like safe and efficient navigation. Our framework consists of three components: 1) superpixel graph map generation with regional constraints, 2) regional-constraint relaxation using graph neural network trained on human demonstrations for safe and efficient navigation, and 3) interleaving relaxation, planning, and execution for complete navigation. We evaluate our method against state-of-the-art baselines on 2D semantic maps and 3D maps from OpenStreetMap, achieving the highest human-likeness score of complete navigation while maintaining a balanced trade-off between efficiency and safety. We finally demonstrate its scalability and generalization performance in real-world urban navigation with a quadruped robot, Spot.",
          "authors": [
            "Keonyoung Koh",
            "Moonkyeong Jung",
            "Samuel Seungsup Lee",
            "Daehyung Park"
          ],
          "published": "2026-02-06T15:55:38Z",
          "updated": "2026-02-06T15:55:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06807v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07101v2",
          "title": "Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting",
          "summary": "UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.",
          "authors": [
            "Zinan Lv",
            "Yeqian Qian",
            "Chen Sang",
            "Hao Liu",
            "Danping Zou",
            "Ming Yang"
          ],
          "published": "2026-02-06T15:51:03Z",
          "updated": "2026-02-18T01:46:10Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07101v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06749v1",
          "title": "Constraint Manifold Exploration for Efficient Continuous Coverage Estimation",
          "summary": "Many automated manufacturing processes rely on industrial robot arms to move process-specific tools along workpiece surfaces. In applications like grinding, sanding, spray painting, or inspection, they need to cover a workpiece fully while keeping their tools perpendicular to its surface. While there are approaches to generate trajectories for these applications, there are no sufficient methods for analyzing the feasibility of full surface coverage. This work proposes a sampling-based approach for continuous coverage estimation that explores reachable surface regions in the configuration space. We define an extended ambient configuration space that allows for the representation of tool position and orientation constraints. A continuation-based approach is used to explore it using two different sampling strategies. A thorough evaluation across different kinematics and environments analyzes their runtime and efficiency. This validates our ability to accurately and efficiently calculate surface coverage for complex surfaces in complicated environments.",
          "authors": [
            "Robert Wilbrandt",
            "Rüdiger Dillmann"
          ],
          "published": "2026-02-06T14:47:28Z",
          "updated": "2026-02-06T14:47:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06749v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06698v1",
          "title": "Crowd-FM: Learned Optimal Selection of Conditional Flow Matching-generated Trajectories for Crowd Navigation",
          "summary": "Safe and computationally efficient local planning for mobile robots in dense, unstructured human crowds remains a fundamental challenge. Moreover, ensuring that robot trajectories are similar to how a human moves will increase the acceptance of the robot in human environments. In this paper, we present Crowd-FM, a learning-based approach to address both safety and human-likeness challenges. Our approach has two novel components. First, we train a Conditional Flow-Matching (CFM) policy over a dataset of optimally controlled trajectories to learn a set of collision-free primitives that a robot can choose at any given scenario. The chosen optimal control solver can generate multi-modal collision-free trajectories, allowing the CFM policy to learn a diverse set of maneuvers. Secondly, we learn a score function over a dataset of human demonstration trajectories that provides a human-likeness score for the flow primitives. At inference time, computing the optimal trajectory requires selecting the one with the highest score. Our approach improves the state-of-the-art by showing that our CFM policy alone can produce collision-free navigation with a higher success rate than existing learning-based baselines. Furthermore, when augmented with inference-time refinement, our approach can outperform even expensive optimisation-based planning approaches. Finally, we validate that our scoring network can select trajectories closer to the expert data than a manually designed cost function.",
          "authors": [
            "Antareep Singha",
            "Laksh Nanwani",
            "Mathai Mathew P.",
            "Samkit Jain",
            "Phani Teja Singamaneni",
            "Arun Kumar Singh",
            "K. Madhava Krishna"
          ],
          "published": "2026-02-06T13:36:46Z",
          "updated": "2026-02-06T13:36:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06698v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06653v1",
          "title": "RAPID: Reconfigurable, Adaptive Platform for Iterative Design",
          "summary": "Developing robotic manipulation policies is iterative and hypothesis-driven: researchers test tactile sensing, gripper geometries, and sensor placements through real-world data collection and training. Yet even minor end-effector changes often require mechanical refitting and system re-integration, slowing iteration. We present RAPID, a full-stack reconfigurable platform designed to reduce this friction. RAPID is built around a tool-free, modular hardware architecture that unifies handheld data collection and robot deployment, and a matching software stack that maintains real-time awareness of the underlying hardware configuration through a driver-level Physical Mask derived from USB events. This modular hardware architecture reduces reconfiguration to seconds and makes systematic multi-modal ablation studies practical, allowing researchers to sweep diverse gripper and sensing configurations without repeated system bring-up. The Physical Mask exposes modality presence as an explicit runtime signal, enabling auto-configuration and graceful degradation under sensor hot-plug events, so policies can continue executing when sensors are physically added or removed. System-centric experiments show that RAPID reduces the setup time for multi-modal configurations by two orders of magnitude compared to traditional workflows and preserves policy execution under runtime sensor hot-unplug events. The hardware designs, drivers, and software stack are open-sourced at https://rapid-kit.github.io/ .",
          "authors": [
            "Zi Yin",
            "Fanhong Li",
            "Shurui Zheng",
            "Jia Liu"
          ],
          "published": "2026-02-06T12:28:46Z",
          "updated": "2026-02-06T12:28:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06653v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06643v2",
          "title": "Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations",
          "summary": "Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to controlled environments. In this paper, we present the Humanoid Manipulation Interface (HuMI), a portable and efficient framework for learning diverse whole-body manipulation tasks across various environments. HuMI enables robot-free data collection by capturing rich whole-body motion using portable hardware. This data drives a hierarchical learning pipeline that translates human motions into dexterous and feasible humanoid skills. Extensive experiments across five whole-body tasks--including kneeling, squatting, tossing, walking, and bimanual manipulation--demonstrate that HuMI achieves a 3x increase in data collection efficiency compared to teleoperation and attains a 70% success rate in unseen environments.",
          "authors": [
            "Ruiqian Nai",
            "Boyuan Zheng",
            "Junming Zhao",
            "Haodong Zhu",
            "Sicong Dai",
            "Zunhao Chen",
            "Yihang Hu",
            "Yingdong Hu",
            "Tong Zhang",
            "Chuan Wen",
            "Yang Gao"
          ],
          "published": "2026-02-06T12:10:47Z",
          "updated": "2026-02-12T15:32:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06643v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06639v1",
          "title": "Efficient and Robust Modeling of Nonlinear Mechanical Systems",
          "summary": "The development of efficient and robust dynamic models is fundamental in the field of systems and control engineering. In this paper, a new formulation for the dynamic model of nonlinear mechanical systems, that can be applied to different automotive and robotic case studies, is proposed, together with a modeling procedure allowing to automatically obtain the model formulation. Compared with the Euler-Lagrange formulation, the proposed model is shown to give superior performances in terms of robustness against measurement noise for systems exhibiting dependence on some external variables, as well as in terms of execution time when computing the inverse dynamics of the system.",
          "authors": [
            "Davide Tebaldi",
            "Roberto Zanasi"
          ],
          "published": "2026-02-06T12:02:20Z",
          "updated": "2026-02-06T12:02:20Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06639v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06620v1",
          "title": "Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique",
          "summary": "In contact-rich tasks, while position trajectories are often easy to obtain, appropriate force commands are typically unknown. Although it is conceivable to generate force commands using a pretrained foundation model such as Vision-Language-Action (VLA) models, force control is highly dependent on the specific hardware of the robot, which makes the application of such models challenging. To bridge this gap, we propose a force generative model that estimates force commands from given position trajectories. However, when dealing with unseen position trajectories, the model struggles to generate accurate force commands. To address this, we introduce a feedback control mechanism. Our experiments reveal that feedback control does not converge when the force generative model has memory. We therefore adopt a model without memory, enabling stable feedback control. This approach allows the system to generate force commands effectively, even for unseen position trajectories, improving generalization for real-world robot writing tasks.",
          "authors": [
            "Hiroshi Sato",
            "Sho Sakaino",
            "Toshiaki Tsuji"
          ],
          "published": "2026-02-06T11:25:08Z",
          "updated": "2026-02-06T11:25:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06620v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06575v1",
          "title": "Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation",
          "summary": "Vision-language-action (VLA) models typically inject proprioception only as a late conditioning signal, which prevents robot state from shaping instruction understanding and from influencing which visual tokens are attended throughout the policy. We introduce ThinkProprio, which converts proprioception into a sequence of text tokens in the VLM embedding space and fuses them with the task instruction at the input. This early fusion lets embodied state participate in subsequent visual reasoning and token selection, biasing computation toward action-critical evidence while suppressing redundant visual tokens. In a systematic ablation over proprioception encoding, state entry point, and action-head conditioning, we find that text tokenization is more effective than learned projectors, and that retaining roughly 15% of visual tokens can match the performance of using the full token set. Across CALVIN, LIBERO, and real-world manipulation, ThinkProprio matches or improves over strong baselines while reducing end-to-end inference latency over 50%.",
          "authors": [
            "Fangyuan Wang",
            "Peng Zhou",
            "Jiaming Qi",
            "Shipeng Lyu",
            "David Navarro-Alarcon",
            "Guodong Guo"
          ],
          "published": "2026-02-06T10:16:22Z",
          "updated": "2026-02-06T10:16:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06575v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06572v1",
          "title": "The Law of Task-Achieving Body Motion: Axiomatizing Success of Robot Manipulation Actions",
          "summary": "Autonomous agents that perform everyday manipulation actions need to ensure that their body motions are semantically correct with respect to a task request, causally effective within their environment, and feasible for their embodiment. In order to enable robots to verify these properties, we introduce the Law of Task-Achieving Body Motion as an axiomatic correctness specification for body motions. To that end we introduce scoped Task-Environment-Embodiment (TEE) classes that represent world states as Semantic Digital Twins (SDTs) and define applicable physics models to decompose task achievement into three predicates: SatisfiesRequest for semantic request satisfaction over SDT state evolution; Causes for causal sufficiency under the scoped physics model; and CanPerform for safety and feasibility verification at the embodiment level. This decomposition yields a reusable, implementation-independent interface that supports motion synthesis and the verification of given body motions. It also supports typed failure diagnosis (semantic, causal, embodiment and out-of-scope), feasibility across robots and environments, and counterfactual reasoning about robot body motions. We demonstrate the usability of the law in practice by instantiating it for articulated container manipulation in kitchen environments on three contrasting mobile manipulation platforms",
          "authors": [
            "Malte Huerkamp",
            "Jonas Dech",
            "Michael Beetz"
          ],
          "published": "2026-02-06T10:12:56Z",
          "updated": "2026-02-06T10:12:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06572v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06556v1",
          "title": "LIBERO-X: Robustness Litmus for Vision-Language-Action Models",
          "summary": "Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.",
          "authors": [
            "Guodong Wang",
            "Chenkai Zhang",
            "Qingjie Liu",
            "Jinjin Zhang",
            "Jiancheng Cai",
            "Junjie Liu",
            "Xinmin Liu"
          ],
          "published": "2026-02-06T09:59:12Z",
          "updated": "2026-02-06T09:59:12Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06556v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15886v1",
          "title": "Optimization of an Augmented R-CUBE mechanism for Cervical Surgery",
          "summary": "In some surgical operations targeting the spine, it is required to drill cavities in the vertebrae for the insertion of pedicle screws. A new mechanical architecture is proposed for this application. It is based on an augmented version of the full translational R-CUBE mechanism, with improved linkages to implement additional rotational motion. Using this concept, a mechanism presented with a 3T2R motion that is required for the manipulation of the surgical drill. It is mainly composed three stages: one translational, one transmitting and one rotational. Their respective kinematic and velocity models are separately derived, then combined. Based on the drilling trajectories obtained from a real patient case, the mechanism is optimized for generating the highest kinematic performances.",
          "authors": [
            "Terence Essomba",
            "Yu-Wen Wu",
            "Abdelbadia Chaker",
            "Med Amine Laribi"
          ],
          "published": "2026-02-06T09:56:57Z",
          "updated": "2026-02-06T09:56:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15886v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15885v1",
          "title": "A novel Integrated Motion Tracking Device (IMTD) for Objective Laparoscopic Training Assessment: Development and Validation",
          "summary": "This paper presents a novel, compact four-degree-of-freedom motion-tracking device (IMTD) designed for training and evaluation in laparoscopic surgery. The device's kinematics, mechanical design, instrumentation, and prototypes are developed and presented to meet the specific requirements of laparoscopic training context, including movement around a fixed center of motion and seamless integration into standard box trainers. The system IMTD's tracking accuracy and reliability are compared to a motion capture system (MoCap), assessing its ability to capture both angular and translational motions of surgical instruments. The study then focuses on key performance parameters including precision, fluidity, speed, and overall motion efficiency. The results highlight the system's effectiveness in tracking surgical gestures, providing valuable insights into its potential as a tool for training and performance evaluation in minimally invasive surgery. Additionally, IMTD's low cost and integrated design allow for easy integration and implementation in training rooms, offering a practical and accessible solution for general use. By offering objective, real-time feedback, the system can significantly contribute to improving surgical skills and shortening the learning curve for novice students, while also providing a foundation for future development of gesture scoring algorithms and standardized training protocols.",
          "authors": [
            "Siwar Bouzid",
            "Abdelbadia Chaker",
            "Marc Arsicault",
            "Sami Bennour",
            "Med Amine Laribi"
          ],
          "published": "2026-02-06T09:53:48Z",
          "updated": "2026-02-06T09:53:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15885v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.09046v1",
          "title": "Feasible Static Workspace Optimization of Tendon Driven Continuum Robot based on Euclidean norm",
          "summary": "This paper focuses on the optimal design of a tendon-driven continuum robot (TDCR) based on its feasible static workspace (FSW). The TDCR under consideration is a two-segment robot driven by eight tendons, with four tendon actuators per segment. Tendon forces are treated as design variables, while the feasible static workspace (FSW) serves as the optimization objective. To determine the robot's feasible static workspace, a genetic algorithm optimization approach is employed to maximize a Euclidian norm of the TDCR's tip position over the workspace. During the simulations, the robot is subjected to external loads, including torques and forces. The results demonstrate the effectiveness of the proposed method in identifying optimal tendon forces to maximize the feasible static workspace, even under the influence of external forces and torques.",
          "authors": [
            "Mohammad Jabari",
            "Carmen Visconte",
            "Giuseppe Quaglia",
            "Med Amine Laribi"
          ],
          "published": "2026-02-06T09:51:40Z",
          "updated": "2026-02-06T09:51:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.09046v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06541v1",
          "title": "Primary Experimental Feedback on a Co-manipulated Robotic System for Assisted Cervical Surgery",
          "summary": "Robotic-assisted surgery has emerged as a promising approach to improve surgical ergonomics, precision, and workflow efficiency, particularly in complex procedures such as cervical spine surgery. In this study, we evaluate the performance of a collaborative robotic system designed to assist surgeons in drilling tasks by assessing its accuracy in executing predefined trajectories. A total of 14 drillings were performed by eight experienced cervical surgeons, utilizing a robotic-assisted setup aimed at ensuring stability and alignment. The primary objective of this study is to quantify the deviations in the position and orientation of the drilling tool relative to the planned trajectory, providing insights into the system's reliability and potential impact on clinical outcomes. While the primary function of robotic assistance in surgery is to enhance surgeon comfort and procedural guidance rather than solely optimizing precision, understanding the system's accuracy remains crucial for its effective integration into surgical practices part of this primary experimental feedback, the study offers an in-depth analysis of the co-manipulated robotic system's performance, focusing on the experimental setup and error evaluation methods. The findings of this study will contribute to the ongoing development of robotic-assisted cervical surgery, highlighting both its advantages and areas for improvement in achieving safer and more efficient surgical workflows",
          "authors": [
            "Seifeddine Sellemi",
            "Abdelbadia Chaker",
            "Tanguy Vendeuvre",
            "Terence Essomba",
            "Med Amine Laribi"
          ],
          "published": "2026-02-06T09:47:28Z",
          "updated": "2026-02-06T09:47:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06541v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06521v1",
          "title": "DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving",
          "summary": "End-to-end (E2E) autonomous driving has recently attracted increasing interest in unifying Vision-Language-Action (VLA) with World Models to enhance decision-making and forward-looking imagination. However, existing methods fail to effectively unify future scene evolution and action planning within a single architecture due to inadequate sharing of latent states, limiting the impact of visual imagination on action decisions. To address this limitation, we propose DriveWorld-VLA, a novel framework that unifies world modeling and planning within a latent space by tightly integrating VLA and world models at the representation level, which enables the VLA planner to benefit directly from holistic scene-evolution modeling and reducing reliance on dense annotated supervision. Additionally, DriveWorld-VLA incorporates the latent states of the world model as core decision-making states for the VLA planner, facilitating the planner to assess how candidate actions impact future scene evolution. By conducting world modeling entirely in the latent space, DriveWorld-VLA supports controllable, action-conditioned imagination at the feature level, avoiding expensive pixel-level rollouts. Extensive open-loop and closed-loop evaluations demonstrate the effectiveness of DriveWorld-VLA, which achieves state-of-the-art performance with 91.3 PDMS on NAVSIMv1, 86.8 EPDMS on NAVSIMv2, and 0.16 3-second average collision rate on nuScenes. Code and models will be released in https://github.com/liulin815/DriveWorld-VLA.git.",
          "authors": [
            "Feiyang jia",
            "Lin Liu",
            "Ziying Song",
            "Caiyan Jia",
            "Hangjun Ye",
            "Xiaoshuai Hao",
            "Long Chen"
          ],
          "published": "2026-02-06T09:25:48Z",
          "updated": "2026-02-06T09:25:48Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06521v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06512v1",
          "title": "Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation",
          "summary": "While generalist robot policies hold significant promise for learning diverse manipulation skills through imitation, their performance is often hindered by the long-tail distribution of training demonstrations. Policies learned on such data, which is heavily skewed towards a few data-rich head tasks, frequently exhibit poor generalization when confronted with the vast number of data-scarce tail tasks. In this work, we conduct a comprehensive analysis of the pervasive long-tail challenge inherent in policy learning. Our analysis begins by demonstrating the inefficacy of conventional long-tail learning strategies (e.g., re-sampling) for improving the policy's performance on tail tasks. We then uncover the underlying mechanism for this failure, revealing that data scarcity on tail tasks directly impairs the policy's spatial reasoning capability. To overcome this, we introduce Approaching-Phase Augmentation (APA), a simple yet effective scheme that transfers knowledge from data-rich head tasks to data-scarce tail tasks without requiring external demonstrations. Extensive experiments in both simulation and real-world manipulation tasks demonstrate the effectiveness of APA. Our code and demos are publicly available at: https://mldxy.github.io/Project-VLA-long-tail/.",
          "authors": [
            "Junhong Zhu",
            "Ji Zhang",
            "Jingkuan Song",
            "Lianli Gao",
            "Heng Tao Shen"
          ],
          "published": "2026-02-06T09:10:02Z",
          "updated": "2026-02-06T09:10:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06512v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06508v1",
          "title": "World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy",
          "summary": "Recent progress in robotic world models has leveraged video diffusion transformers to predict future observations conditioned on historical states and actions. While these models can simulate realistic visual outcomes, they often exhibit poor action-following precision, hindering their utility for downstream robotic learning. In this work, we introduce World-VLA-Loop, a closed-loop framework for the joint refinement of world models and Vision-Language-Action (VLA) policies. We propose a state-aware video world model that functions as a high-fidelity interactive simulator by jointly predicting future observations and reward signals. To enhance reliability, we introduce the SANS dataset, which incorporates near-success trajectories to improve action-outcome alignment within the world model. This framework enables a closed-loop for reinforcement learning (RL) post-training of VLA policies entirely within a virtual environment. Crucially, our approach facilitates a co-evolving cycle: failure rollouts generated by the VLA policy are iteratively fed back to refine the world model precision, which in turn enhances subsequent RL optimization. Evaluations across simulation and real-world tasks demonstrate that our framework significantly boosts VLA performance with minimal physical interaction, establishing a mutually beneficial relationship between world modeling and policy learning for general-purpose robotics. Project page: https://showlab.github.io/World-VLA-Loop/.",
          "authors": [
            "Xiaokang Liu",
            "Zechen Bai",
            "Hai Ci",
            "Kevin Yuchen Ma",
            "Mike Zheng Shou"
          ],
          "published": "2026-02-06T08:57:55Z",
          "updated": "2026-02-06T08:57:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06508v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06504v1",
          "title": "MultiGraspNet: A Multitask 3D Vision Model for Multi-gripper Robotic Grasping",
          "summary": "Vision-based models for robotic grasping automate critical, repetitive, and draining industrial tasks. Existing approaches are typically limited in two ways: they either target a single gripper and are potentially applied on costly dual-arm setups, or rely on custom hybrid grippers that require ad-hoc learning procedures with logic that cannot be transferred across tasks, restricting their general applicability. In this work, we present MultiGraspNet, a novel multitask 3D deep learning method that predicts feasible poses simultaneously for parallel and vacuum grippers within a unified framework, enabling a single robot to handle multiple end effectors. The model is trained on the richly annotated GraspNet-1Billion and SuctionNet-1Billion datasets, which have been aligned for the purpose, and generates graspability masks quantifying the suitability of each scene point for successful grasps. By sharing early-stage features while maintaining gripper-specific refiners, MultiGraspNet effectively leverages complementary information across grasping modalities, enhancing robustness and adaptability in cluttered scenes. We characterize MultiGraspNet's performance with an extensive experimental analysis, demonstrating its competitiveness with single-task models on relevant benchmarks. We run real-world experiments on a single-arm multi-gripper robotic setup showing that our approach outperforms the vacuum baseline, grasping 16% percent more seen objects and 32% more of the novel ones, while obtaining competitive results for the parallel task.",
          "authors": [
            "Stephany Ortuno-Chanelo",
            "Paolo Rabino",
            "Enrico Civitelli",
            "Tatiana Tommasi",
            "Raffaello Camoriano"
          ],
          "published": "2026-02-06T08:56:21Z",
          "updated": "2026-02-06T08:56:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06504v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06459v1",
          "title": "User-Centric Object Navigation: A Benchmark with Integrated User Habits for Personalized Embodied Object Search",
          "summary": "In the evolving field of robotics, the challenge of Object Navigation (ON) in household environments has attracted significant interest. Existing ON benchmarks typically place objects in locations guided by general scene priors, without accounting for the specific placement habits of individual users. This omission limits the adaptability of navigation agents in personalized household environments. To address this, we introduce User-centric Object Navigation (UcON), a new benchmark that incorporates user-specific object placement habits, referred to as user habits. This benchmark requires agents to leverage these user habits for more informed decision-making during navigation. UcON encompasses approximately 22,600 user habits across 489 object categories. UcON is, to our knowledge, the first benchmark that explicitly formalizes and evaluates habit-conditioned object navigation at scale and covers the widest range of target object categories. Additionally, we propose a habit retrieval module to extract and utilize habits related to target objects, enabling agents to infer their likely locations more effectively. Experimental results demonstrate that current SOTA methods exhibit substantial performance degradation under habit-driven object placement, while integrating user habits consistently improves success rates. Code is available at https://github.com/whcpumpkin/User-Centric-Object-Navigation.",
          "authors": [
            "Hongcheng Wang",
            "Jinyu Zhu",
            "Hao Dong"
          ],
          "published": "2026-02-06T07:43:05Z",
          "updated": "2026-02-06T07:43:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06459v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06445v1",
          "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
          "summary": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
          "authors": [
            "Weidong Huang",
            "Jingwen Zhang",
            "Jiongye Li",
            "Shibowen Zhang",
            "Jiayang Wu",
            "Jiayi Wang",
            "Hangxin Liu",
            "Yaodong Yang",
            "Yao Su"
          ],
          "published": "2026-02-06T07:14:43Z",
          "updated": "2026-02-06T07:14:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06445v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06427v1",
          "title": "Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters",
          "summary": "Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.",
          "authors": [
            "Yuxiang Zhao",
            "Yirong Yang",
            "Yanqing Zhu",
            "Yanfen Shen",
            "Chiyu Wang",
            "Zhining Gu",
            "Pei Shi",
            "Wei Guo",
            "Mu Xu"
          ],
          "published": "2026-02-06T06:52:23Z",
          "updated": "2026-02-06T06:52:23Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06427v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06400v1",
          "title": "TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction",
          "summary": "3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely heavily on 3D voxel volumes or a set of 3D Gaussians, hindering the model's ability to efficiently and effectively capture fine-grained geometric details in the 3D driving environment. This paper introduces TFusionOcc, a novel object-centric multi-sensor fusion framework for predicting 3D semantic occupancy. By leveraging multi-stage multi-sensor fusion, Student's t-distribution, and the T-Mixture model (TMM), together with more geometrically flexible primitives, such as the deformable superquadric (superquadric with inverse warp), the proposed method achieved state-of-the-art (SOTA) performance on the nuScenes benchmark. In addition, extensive experiments were conducted on the nuScenes-C dataset to demonstrate the robustness of the proposed method in different camera and lidar corruption scenarios. The code will be available at: https://github.com/DanielMing123/TFusionOcc",
          "authors": [
            "Zhenxing Ming",
            "Julie Stephany Berrio",
            "Mao Shan",
            "Stewart Worrall"
          ],
          "published": "2026-02-06T05:43:42Z",
          "updated": "2026-02-06T05:43:42Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06400v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06382v1",
          "title": "Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels",
          "summary": "Achieving robust vision-based humanoid locomotion remains challenging due to two fundamental issues: the sim-to-real gap introduces significant perception noise that degrades performance on fine-grained tasks, and training a unified policy across diverse terrains is hindered by conflicting learning objectives. To address these challenges, we present an end-to-end framework for vision-driven humanoid locomotion. For robust sim-to-real transfer, we develop a high-fidelity depth sensor simulation that captures stereo matching artifacts and calibration uncertainties inherent in real-world sensing. We further propose a vision-aware behavior distillation approach that combines latent space alignment with noise-invariant auxiliary tasks, enabling effective knowledge transfer from privileged height maps to noisy depth observations. For versatile terrain adaptation, we introduce terrain-specific reward shaping integrated with multi-critic and multi-discriminator learning, where dedicated networks capture the distinct dynamics and motion priors of each terrain type. We validate our approach on two humanoid platforms equipped with different stereo depth cameras. The resulting policy demonstrates robust performance across diverse environments, seamlessly handling extreme challenges such as high platforms and wide gaps, as well as fine-grained tasks including bidirectional long-term staircase traversal.",
          "authors": [
            "Wandong Sun",
            "Yongbo Su",
            "Leoric Huang",
            "Alex Zhang",
            "Dwyane Wei",
            "Mu San",
            "Daniel Tian",
            "Ellie Cao",
            "Finn Yan",
            "Ethan Xie",
            "Zongwu Xie"
          ],
          "published": "2026-02-06T04:34:20Z",
          "updated": "2026-02-06T04:34:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06382v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06380v1",
          "title": "A Consistency-Improved LiDAR-Inertial Bundle Adjustment",
          "summary": "Simultaneous Localization and Mapping (SLAM) using 3D LiDAR has emerged as a cornerstone for autonomous navigation in robotics. While feature-based SLAM systems have achieved impressive results by leveraging edge and planar structures, they often suffer from the inconsistent estimator associated with feature parameterization and estimated covariance. In this work, we present a consistency-improved LiDAR-inertial bundle adjustment (BA) with tailored parameterization and estimator. First, we propose a stereographic-projection representation parameterizing the planar and edge features, and conduct a comprehensive observability analysis to support its integrability with consistent estimator. Second, we implement a LiDAR-inertial BA with Maximum a Posteriori (MAP) formulation and First-Estimate Jacobians (FEJ) to preserve the accurate estimated covariance and observability properties of the system. Last, we apply our proposed BA method to a LiDAR-inertial odometry.",
          "authors": [
            "Xinran Li",
            "Shuaikang Zheng",
            "Pengcheng Zheng",
            "Xinyang Wang",
            "Jiacheng Li",
            "Zhitian Li",
            "Xudong Zou"
          ],
          "published": "2026-02-06T04:28:12Z",
          "updated": "2026-02-06T04:28:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06380v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06366v1",
          "title": "Towards Adaptive Environment Generation for Training Embodied Agents",
          "summary": "Embodied agents struggle to generalize to new environments, even when those environments share similar underlying structures to their training settings. Most current approaches to generating these training environments follow an open-loop paradigm, without considering the agent's current performance. While procedural generation methods can produce diverse scenes, diversity without feedback from the agent is inefficient. The generated environments may be trivially easy, providing limited learning signal. To address this, we present a proof-of-concept for closed-loop environment generation that adapts difficulty to the agent's current capabilities. Our system employs a controllable environment representation, extracts fine-grained performance feedback beyond binary success or failure, and implements a closed-loop adaptation mechanism that translates this feedback into environment modifications. This feedback-driven approach generates training environments that more challenging in the ways the agent needs to improve, enabling more efficient learning and better generalization to novel settings.",
          "authors": [
            "Teresa Yeo",
            "Dulaj Weerakoon",
            "Dulanga Weerakoon",
            "Archan Misra"
          ],
          "published": "2026-02-06T03:50:46Z",
          "updated": "2026-02-06T03:50:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06366v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06356v1",
          "title": "Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation",
          "summary": "Vision-Language Navigation (VLN) requires embodied agents to interpret natural language instructions and navigate through complex continuous 3D environments. However, the dominant imitation learning paradigm suffers from exposure bias, where minor deviations during inference lead to compounding errors. While DAgger-style approaches attempt to mitigate this by correcting error states, we identify a critical limitation: Instruction-State Misalignment. Forcing an agent to learn recovery actions from off-track states often creates supervision signals that semantically conflict with the original instruction. In response to these challenges, we introduce BudVLN, an online framework that learns from on-policy rollouts by constructing supervision to match the current state distribution. BudVLN performs retrospective rectification via counterfactual re-anchoring and decision-conditioned supervision synthesis, using a geodesic oracle to synthesize corrective trajectories that originate from valid historical states, ensuring semantic consistency. Experiments on the standard R2R-CE and RxR-CE benchmarks demonstrate that BudVLN consistently mitigates distribution shift and achieves state-of-the-art performance in both Success Rate and SPL.",
          "authors": [
            "Gang He",
            "Zhenyang Liu",
            "Kepeng Xu",
            "Li Xu",
            "Tong Qiao",
            "Wenxin Yu",
            "Chang Wu",
            "Weiying Xie"
          ],
          "published": "2026-02-06T03:36:27Z",
          "updated": "2026-02-06T03:36:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06356v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06341v1",
          "title": "HiWET: Hierarchical World-Frame End-Effector Tracking for Long-Horizon Humanoid Loco-Manipulation",
          "summary": "Humanoid loco-manipulation requires executing precise manipulation tasks while maintaining dynamic stability amid base motion and impacts. Existing approaches typically formulate commands in body-centric frames, fail to inherently correct cumulative world-frame drift induced by legged locomotion. We reformulate the problem as world-frame end-effector tracking and propose HiWET, a hierarchical reinforcement learning framework that decouples global reasoning from dynamic execution. The high-level policy generates subgoals that jointly optimize end-effector accuracy and base positioning in the world frame, while the low-level policy executes these commands under stability constraints. We introduce a Kinematic Manifold Prior (KMP) that embeds the manipulation manifold into the action space via residual learning, reducing exploration dimensionality and mitigating kinematically invalid behaviors. Extensive simulation and ablation studies demonstrate that HiWET achieves precise and stable end-effector tracking in long-horizon world-frame tasks. We validate zero-shot sim-to-real transfer of the low-level policy on a physical humanoid, demonstrating stable locomotion under diverse manipulation commands. These results indicate that explicit world-frame reasoning combined with hierarchical control provides an effective and scalable solution for long-horizon humanoid loco-manipulation.",
          "authors": [
            "Zhanxiang Cao",
            "Liyun Yan",
            "Yang Zhang",
            "Sirui Chen",
            "Jianming Ma",
            "Tianyue Zhan",
            "Shengcheng Fu",
            "Yufei Jia",
            "Cewu Lu",
            "Yue Gao"
          ],
          "published": "2026-02-06T03:11:56Z",
          "updated": "2026-02-06T03:11:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06341v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06339v1",
          "title": "Action Hallucination in Generative Visual-Language-Action Models",
          "summary": "Robot Foundation Models such as Vision-Language-Action models are rapidly reshaping how robot policies are trained and deployed, replacing hand-designed planners with end-to-end generative action models. While these systems demonstrate impressive generalization, it remains unclear whether they fundamentally resolve the long-standing challenges of robotics. We address this question by analyzing action hallucinations that violate physical constraints and their extension to plan-level failures. Focusing on latent-variable generative policies, we show that hallucinations often arise from structural mismatches between feasible robot behavior and common model architectures. We study three such barriers -- topological, precision, and horizon -- and show how they impose unavoidable tradeoffs. Our analysis provides mechanistic explanations for reported empirical failures of generative robot policies and suggests principled directions for improving reliability and trustworthiness, without abandoning their expressive power.",
          "authors": [
            "Harold Soh",
            "Eugene Lim"
          ],
          "published": "2026-02-06T03:05:30Z",
          "updated": "2026-02-06T03:05:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06339v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06296v1",
          "title": "Internalized Morphogenesis: A Self-Organizing Model for Growth, Replication, and Regeneration via Local Token Exchange in Modular Systems",
          "summary": "This study presents an internalized morphogenesis model for autonomous systems, such as swarm robotics and micro-nanomachines, that eliminates the need for external spatial computation. Traditional self-organizing models often require calculations across the entire coordinate space, including empty areas, which is impractical for resource-constrained physical modules. Our proposed model achieves complex morphogenesis through strictly local interactions between adjacent modules within the \"body.\" By extending the \"Ishida token model,\" modules exchange integer values using an RD-inspired discrete analogue without solving differential equations. The internal potential, derived from token accumulation and aging, guides autonomous growth, shrinkage, and replication. Simulations on a hexagonal grid demonstrated the emergence of limb-like extensions, self-division, and robust regeneration capabilities following structural amputation. A key feature is the use of the body boundary as a natural sink for information entropy (tokens) to maintain a dynamic equilibrium. These results indicate that sophisticated morphological behaviors can emerge from minimal, internal-only rules. This framework offers a computationally efficient and biologically plausible approach to developing self-repairing, adaptive, and autonomous hardware.",
          "authors": [
            "Takeshi Ishida"
          ],
          "published": "2026-02-06T01:41:04Z",
          "updated": "2026-02-06T01:41:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "q-bio.QM"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06296v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06294v1",
          "title": "Robots That Generate Planarity Through Geometry",
          "summary": "Constraining motion to a flat surface is a fundamental requirement for equipment across science and engineering. Modern precision robotic motion systems, such as gantries, rely on the flatness of components, including guide rails and granite surface plates. However, translating this static flatness into motion requires precise internal alignment and tight-tolerance components that create long, error-sensitive reference chains. Here, we show that by using the geometric inversion of a sphere into a plane, we can produce robotic motion systems that derive planarity entirely from link lengths and connectivity. This allows planar motion to emerge from self-referencing geometric constraints, and without external metrology. We demonstrate these Flat-Plane Mechanisms (FPMs) from micron to meter scales and show that fabrication errors can be attenuated by an order of magnitude in the resulting flatness. Finally, we present a robotic FPM-based 3-axis positioning system that can be used for metrology surface scans ($\\pm 12$-mm) and 3D printing inside narrow containers. This work establishes an alternative geometric foundation for planar motion that can be realized across size scales and opens new possibilities in metrology, fabrication, and micro-positioning.",
          "authors": [
            "Jakub F. Kowalewski",
            "Abdulaziz O. Alrashed",
            "Jacob Alpert",
            "Rishi Ponnapalli",
            "Lucas R. Meza",
            "Jeffrey Ian Lipton"
          ],
          "published": "2026-02-06T01:23:34Z",
          "updated": "2026-02-06T01:23:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06294v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07074v1",
          "title": "Airspace-aware Contingency Landing Planning",
          "summary": "This paper develops a real-time, search-based aircraft contingency landing planner that minimizes traffic disruptions while accounting for ground risk. The airspace model captures dense air traffic departure and arrival flows, helicopter corridors, and prohibited zones and is demonstrated with a Washington, D.C., area case study. Historical Automatic Dependent Surveillance-Broadcast (ADS-B) data are processed to estimate air traffic density. A low-latency computational geometry algorithm generates proximity-based heatmaps around high-risk corridors and restricted regions. Airspace risk is quantified as the cumulative exposure time of a landing trajectory within congested regions, while ground risk is assessed from overflown population density to jointly guide trajectory selection. A landing site selection module further mitigates disruption to nominal air traffic operations. Benchmarking against minimum-risk Dubins solutions demonstrates that the proposed planner achieves lower joint risk and reduced airspace disruption while maintaining real-time performance. Under airspace-risk-only conditions, the planner generates trajectories within an average of 2.9 seconds on a laptop computer. Future work will incorporate dynamic air traffic updates to enable spatiotemporal contingency landing planning that minimizes the need for real-time traffic rerouting.",
          "authors": [
            "H. Emre Tekaslan",
            "Ella M. Atkins"
          ],
          "published": "2026-02-06T01:04:10Z",
          "updated": "2026-02-06T01:04:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07074v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06273v1",
          "title": "A High-Fidelity Robotic Manipulator Teleoperation Framework for Human-Centered Augmented Reality Evaluation",
          "summary": "Validating Augmented Reality (AR) tracking and interaction models requires precise, repeatable ground-truth motion. However, human users cannot reliably perform consistent motion due to biomechanical variability. Robotic manipulators are promising to act as human motion proxies if they can mimic human movements. In this work, we design and implement ARBot, a real-time teleoperation platform that can effectively capture natural human motion and accurately replay the movements via robotic manipulators. ARBot includes two capture models: stable wrist motion capture via a custom CV and IMU pipeline, and natural 6-DOF control via a mobile application. We design a proactively-safe QP controller to ensure smooth, jitter-free execution of the robotic manipulator, enabling it to function as a high-fidelity record and replay physical proxy. We open-source ARBot and release a benchmark dataset of 132 human and synthetic trajectories captured using ARBot to support controllable and scalable AR evaluation.",
          "authors": [
            "Harsh Chhajed",
            "Tian Guo"
          ],
          "published": "2026-02-06T00:21:06Z",
          "updated": "2026-02-06T00:21:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06273v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06265v1",
          "title": "MORPH Wheel: A Passive Variable-Radius Wheel Embedding Mechanical Behavior Logic for Input-Responsive Transformation",
          "summary": "This paper introduces the Mechacnially prOgrammed Radius-adjustable PHysical (MORPH) wheel, a fully passive variable-radius wheel that embeds mechanical behavior logic for torque-responsive transformation. Unlike conventional variable transmission systems relying on actuators, sensors, and active control, the MORPH wheel achieves passive adaptation solely through its geometry and compliant structure. The design integrates a torque-response coupler and spring-loaded connecting struts to mechanically adjust the wheel radius between 80 mm and 45 mm in response to input torque, without any electrical components. The MORPH wheel provides three unique capabilities rarely achieved simultaneously in previous passive designs: (1) bidirectional operation with unlimited rotation through a symmetric coupler; (2) high torque capacity exceeding 10 N with rigid power transmission in drive mode; and (3) precise and repeatable transmission ratio control governed by deterministic kinematics. A comprehensive analytical model was developed to describe the wheel's mechanical behavior logic, establishing threshold conditions for mode switching between direct drive and radius transformation. Experimental validation confirmed that the measured torque-radius and force-displacement characteristics closely follow theoretical predictions across wheel weights of 1.8-2.8kg. Robot-level demonstrations on varying loads (0-25kg), slopes, and unstructured terrains further verified that the MORPH wheel passively adjusts its radius to provide optimal transmission ratio. The MORPH wheel exemplifies a mechanically programmed structure, embedding intelligent, context-dependent behavior directly into its physical design. This approach offers a new paradigm for passive variable transmission and mechanical intelligence in robotic mobility systems operating in unpredictable or control-limited environments.",
          "authors": [
            "JaeHyung Jang",
            "JuYeong Seo",
            "Dae-Young Lee",
            "Jee-Hwan Ryu"
          ],
          "published": "2026-02-05T23:46:26Z",
          "updated": "2026-02-05T23:46:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06265v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06243v1",
          "title": "A Dialogue-Based Human-Robot Interaction Protocol for Wheelchair and Robotic Arm Integrated Control",
          "summary": "People with lower and upper body disabilities can benefit from wheelchairs and robotic arms to improve mobility and independence. Prior assistive interfaces, such as touchscreens and voice-driven predefined commands, often remain unintuitive and struggle to capture complex user intent. We propose a natural, dialogue based human robot interaction protocol that simulates an intelligent agent capable of communicating with users to understand intent and execute assistive actions. In a pilot study, five participants completed five assistive tasks (cleaning, drinking, feeding, drawer opening, and door opening) through dialogue-based interaction with a wheelchair and robotic arm. As a baseline, participants were required to open a door using the manual control (a wheelchair joystick and a game controller for the arm) and complete a questionnaire to gather their feedback. By analyzing the post-study questionnaires, we found that most participants enjoyed the dialogue-based interaction and assistive robot autonomy.",
          "authors": [
            "Guangping Liu",
            "Nicholas Hawkins",
            "Billy Madden",
            "Tipu Sultan",
            "Madi Babaiasl"
          ],
          "published": "2026-02-05T22:47:14Z",
          "updated": "2026-02-05T22:47:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06243v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06219v1",
          "title": "Coupled Local and Global World Models for Efficient First Order RL",
          "summary": "World models offer a promising avenue for more faithfully capturing complex dynamics, including contacts and non-rigidity, as well as complex sensory information, such as visual perception, in situations where standard simulators struggle. However, these models are computationally complex to evaluate, posing a challenge for popular RL approaches that have been successfully used with simulators to solve complex locomotion tasks but yet struggle with manipulation. This paper introduces a method that bypasses simulators entirely, training RL policies inside world models learned from robots' interactions with real environments. At its core, our approach enables policy training with large-scale diffusion models via a novel decoupled first-order gradient (FoG) method: a full-scale world model generates accurate forward trajectories, while a lightweight latent-space surrogate approximates its local dynamics for efficient gradient computation. This coupling of a local and global world model ensures high-fidelity unrolling alongside computationally tractable differentiation. We demonstrate the efficacy of our method on the Push-T manipulation task, where it significantly outperforms PPO in sample efficiency. We further evaluate our approach through an ego-centric object manipulation task with a quadruped. Together, these results demonstrate that learning inside data-driven world models is a promising pathway for solving hard-to-model RL tasks in image space without reliance on hand-crafted physics simulators.",
          "authors": [
            "Joseph Amigo",
            "Rooholla Khorrambakht",
            "Nicolas Mansard",
            "Ludovic Righetti"
          ],
          "published": "2026-02-05T21:57:41Z",
          "updated": "2026-02-05T21:57:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06219v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06214v2",
          "title": "Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models",
          "summary": "End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are waypoint-based, which makes action-based policies harder to train and compare, slowing their progress. To bridge this waypoint-action gap, we propose a novel, differentiable vehicle-model framework that rolls out predicted action sequences to their corresponding ego-frame waypoint trajectories while supervising in waypoint space. Our approach enables action-based architectures to be trained and evaluated, for the first time, within waypoint-based benchmarks without modifying the underlying evaluation protocol. We extensively evaluate our framework across multiple challenging benchmarks and observe consistent improvements over the baselines. In particular, on NAVSIM \\texttt{navhard} our approach achieves state-of-the-art performance. Our code will be made publicly available upon acceptance.",
          "authors": [
            "Jorge Daniel Rodríguez-Vidal",
            "Gabriel Villalonga",
            "Diego Porres",
            "Antonio M. López Peña"
          ],
          "published": "2026-02-05T21:42:13Z",
          "updated": "2026-02-09T11:33:20Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06214v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06207v1",
          "title": "Bioinspired Kirigami Capsule Robot for Minimally Invasive Gastrointestinal Biopsy",
          "summary": "Wireless capsule endoscopy (WCE) has transformed gastrointestinal (GI) diagnostics by enabling noninvasive visualization of the digestive tract, yet its diagnostic yield remains constrained by the absence of biopsy capability, as histological analysis is still the gold standard for confirming disease. Conventional biopsy using forceps, needles, or rotating blades is invasive, limited in reach, and carries risks of perforation or mucosal trauma, while fluid- or microbiota-sampling capsules cannot provide structured tissue for pathology, leaving a critical gap in swallowable biopsy solutions. Here we present the Kiri-Capsule, a kirigami-inspired capsule robot that integrates deployable PI-film flaps actuated by a compact dual-cam mechanism to achieve minimally invasive and repeatable tissue collection. The kirigami surface remains flat during locomotion but transforms into sharp protrusions upon cam-driven stretching, enabling controlled penetration followed by rotary scraping, with specimens retained in internal fan-shaped cavities. Bench tests confirmed that PI films exhibit a Young's modulus of approximately 20 MPa and stable deployment angles (about 34$^\\circ$ at 15% strain), while ex vivo porcine studies demonstrated shallow penetration depths (median $\\sim$0.61 mm, range 0.46--0.66 mm) and biopsy yields comparable to standard forceps (mean $\\sim$10.9 mg for stomach and $\\sim$18.9 mg for intestine), with forces within safe ranges reported for GI biopsy. These findings demonstrate that the Kiri-Capsule bridges passive imaging and functional biopsy, providing a swallowable, depth-controlled, and histology-ready solution that advances capsule-based diagnostics toward safe and effective clinical application.",
          "authors": [
            "Ruizhou Zhao",
            "Yichen Chu",
            "Shuwei Zhao",
            "Wenchao Yue",
            "Raymond Shing-Yan Tang",
            "Hongliang Ren"
          ],
          "published": "2026-02-05T21:37:15Z",
          "updated": "2026-02-05T21:37:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06207v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06203v1",
          "title": "AnyThermal: Towards Learning Universal Representations for Thermal Perception",
          "summary": "We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific training from small-scale data result in utility limited to a specific environment and task. Unlike prior methods, AnyThermal can be used for a wide range of environments (indoor, aerial, off-road, urban) and tasks, all without task-specific training. Our key insight is to distill the feature representations from visual foundation models such as DINOv2 into a thermal encoder using thermal data from these multiple environments. To bridge the diversity gap of the existing RGB-Thermal datasets, we introduce the TartanRGBT platform, the first open-source data collection platform with synced RGB-Thermal image acquisition. We use this payload to collect the TartanRGBT dataset - a diverse and balanced dataset collected in 4 environments. We demonstrate the efficacy of AnyThermal and TartanRGBT, achieving state-of-the-art results with improvements of up to 36% across diverse environments and downstream tasks on existing datasets.",
          "authors": [
            "Parv Maheshwari",
            "Jay Karhade",
            "Yogesh Chawla",
            "Isaiah Adu",
            "Florian Heisen",
            "Andrew Porco",
            "Andrew Jong",
            "Yifei Liu",
            "Santosh Pitla",
            "Sebastian Scherer",
            "Wenshan Wang"
          ],
          "published": "2026-02-05T21:27:26Z",
          "updated": "2026-02-05T21:27:26Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06203v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06191v1",
          "title": "Active Localization of Unstable Systems with Coarse Information",
          "summary": "We study localization and control for unstable systems under coarse, single-bit sensing. Motivated by understanding the fundamental limitations imposed by such minimal feedback, we identify sufficient conditions under which the initial state can be recovered despite instability and extremely sparse measurements. Building on these conditions, we develop an active localization algorithm that integrates a set-based estimator with a control strategy derived from Voronoi partitions, which provably estimates the initial state while ensuring the agent remains in informative regions. Under the derived conditions, the proposed approach guarantees exponential contraction of the initial-state uncertainty, and the result is further supported by numerical experiments. These findings can offer theoretical insight into localization in robotics, where sensing is often limited to coarse abstractions such as keyframes, segmentations, or line-based features.",
          "authors": [
            "Ege Yuceel",
            "Daniel Liberzon",
            "Sayan Mitra"
          ],
          "published": "2026-02-05T20:56:16Z",
          "updated": "2026-02-05T20:56:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06191v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15884v1",
          "title": "The SLAM Confidence Trap",
          "summary": "The SLAM community has fallen into a \"Confidence Trap\" by prioritizing benchmark scores over principled uncertainty estimation. This yields systems that are geometrically accurate but probabilitistically inconsistent and brittle. We advocate for a paradigm shift where the consistent, real-time computation of uncertainty becomes a primary metric of success.",
          "authors": [
            "Sebastian Sansoni",
            "Santiago Ramón Tosetti Sanz"
          ],
          "published": "2026-02-05T19:37:24Z",
          "updated": "2026-02-05T19:37:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15884v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06038v1",
          "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction",
          "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.",
          "authors": [
            "Xiaopan Zhang",
            "Zejin Wang",
            "Zhixu Li",
            "Jianpeng Yao",
            "Jiachen Li"
          ],
          "published": "2026-02-05T18:59:45Z",
          "updated": "2026-02-05T18:59:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06038v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06035v1",
          "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
          "summary": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.",
          "authors": [
            "Sirui Xu",
            "Samuel Schulter",
            "Morteza Ziyadi",
            "Xialin He",
            "Xiaohan Fei",
            "Yu-Xiong Wang",
            "Liangyan Gui"
          ],
          "published": "2026-02-05T18:59:27Z",
          "updated": "2026-02-05T18:59:27Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.GR",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06035v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06023v1",
          "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
          "summary": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.",
          "authors": [
            "Christopher A. McClurg",
            "Alan R. Wagner"
          ],
          "published": "2026-02-05T18:56:49Z",
          "updated": "2026-02-05T18:56:49Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06023v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06001v1",
          "title": "Visuo-Tactile World Models",
          "summary": "We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.",
          "authors": [
            "Carolina Higuera",
            "Sergio Arnaud",
            "Byron Boots",
            "Mustafa Mukadam",
            "Francois Robert Hogan",
            "Franziska Meier"
          ],
          "published": "2026-02-05T18:46:33Z",
          "updated": "2026-02-05T18:46:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06001v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05948v1",
          "title": "Location-Aware Dispersion on Anonymous Graphs",
          "summary": "The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\\sf{col}(v) \\in C = \\{c_1, \\dots, c_t\\}, t\\leq n$. A set $R = \\{r_1, \\dots, r_k\\}$ of $k \\leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\\mathsf{col}(r_i) \\in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\\leq n$ without knowing $k,n$. Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.",
          "authors": [
            "Himani",
            "Supantha Pandit",
            "Gokarna Sharma"
          ],
          "published": "2026-02-05T18:02:24Z",
          "updated": "2026-02-05T18:02:24Z",
          "primary_category": "cs.DC",
          "categories": [
            "cs.DC",
            "cs.DS",
            "cs.MA",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05948v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05922v1",
          "title": "From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits",
          "summary": "Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.",
          "authors": [
            "Aziz Mohamed Mili",
            "Louis Catar",
            "Paul Gérard",
            "Ilyass Tabiai",
            "David St-Onge"
          ],
          "published": "2026-02-05T17:34:49Z",
          "updated": "2026-02-05T17:34:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05922v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05895v1",
          "title": "Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools",
          "summary": "This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.",
          "authors": [
            "Qi Li",
            "Karsten Berns"
          ],
          "published": "2026-02-05T17:14:06Z",
          "updated": "2026-02-05T17:14:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05895v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05863v2",
          "title": "Constrained Group Relative Policy Optimization",
          "summary": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.",
          "authors": [
            "Roger Girgis",
            "Rodrigue de Schaetzen",
            "Luke Rowe",
            "Azalée Robitaille",
            "Christopher Pal",
            "Liam Paull"
          ],
          "published": "2026-02-05T16:44:23Z",
          "updated": "2026-02-06T18:22:59Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.CL",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05863v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05855v1",
          "title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion",
          "summary": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.",
          "authors": [
            "Dennis Bank",
            "Joost Cordes",
            "Thomas Seel",
            "Simon F. G. Ehlers"
          ],
          "published": "2026-02-05T16:38:42Z",
          "updated": "2026-02-05T16:38:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05855v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05827v1",
          "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
          "summary": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.",
          "authors": [
            "Hai Zhang",
            "Siqi Liang",
            "Li Chen",
            "Yuxian Li",
            "Yukuan Xu",
            "Yichao Zhong",
            "Fu Zhang",
            "Hongyang Li"
          ],
          "published": "2026-02-05T16:16:13Z",
          "updated": "2026-02-05T16:16:13Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05827v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05791v1",
          "title": "Scalable and General Whole-Body Control for Cross-Humanoid Locomotion",
          "summary": "Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.",
          "authors": [
            "Yufei Xue",
            "YunFeng Lin",
            "Wentao Dong",
            "Yang Tang",
            "Jingbo Wang",
            "Jiangmiao Pang",
            "Ming Zhou",
            "Minghuan Liu",
            "Weinan Zhang"
          ],
          "published": "2026-02-05T15:48:15Z",
          "updated": "2026-02-05T15:48:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05791v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05760v1",
          "title": "Task-Oriented Robot-Human Handovers on Legged Manipulators",
          "summary": "Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human's intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers.",
          "authors": [
            "Andreea Tulbure",
            "Carmen Scheidemann",
            "Elias Steiner",
            "Marco Hutter"
          ],
          "published": "2026-02-05T15:28:04Z",
          "updated": "2026-02-05T15:28:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05760v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15882v1",
          "title": "FUTURE-VLA: Forecasting Unified Trajectories Under Real-time Execution",
          "summary": "General vision-language models increasingly support unified spatiotemporal reasoning over long video streams, yet deploying such capabilities on robots remains constrained by the prohibitive latency of processing long-horizon histories and generating high-dimensional future predictions. To bridge this gap, we present FUTURE-VLA, a unified architecture that reformulates long-horizon control and future forecasting as a monolithic sequence-generation task. Adopting a dual-sided efficiency paradigm, FUTURE-VLA leverages a temporally adaptive compression strategy to maximize spatiotemporal information density, enabling the ingestion of extensive multi-view histories while maintaining constant inference latency. Simultaneously, it performs latent-space autoregression to align actionable dynamics with reviewable visual look-aheads in a single forward pass. These real-time predictive capabilities further enable a prediction-guided Human-In-the-Loop mechanism via interactive execution gating, allowing operators to dynamically validate behaviors based on interpretable future previews. Extensive evaluations demonstrate that FUTURE-VLA establishes new state-of-the-art performance, attaining success rates of 99.2% on LIBERO, 75.4% on RoboTwin, and 78.0% on a real-world Piper platform, all with a $16\\times$ extended spatiotemporal window while maintaining the inference latency of a single-frame baseline.",
          "authors": [
            "Jingjing Fan",
            "Yushan Liu",
            "Shoujie Li",
            "Botao Ren",
            "Siyuan Li",
            "Xiao-Ping Zhang",
            "Wenbo Ding",
            "Zhidong Deng"
          ],
          "published": "2026-02-05T14:27:43Z",
          "updated": "2026-02-05T14:27:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15882v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05683v1",
          "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking",
          "summary": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.",
          "authors": [
            "Chuwei Wang",
            "Eduardo Sebastián",
            "Amanda Prorok",
            "Anastasia Bizyaeva"
          ],
          "published": "2026-02-05T14:09:09Z",
          "updated": "2026-02-05T14:09:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05683v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05608v1",
          "title": "HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments",
          "summary": "Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.",
          "authors": [
            "Yufei Zhu",
            "Shih-Min Yang",
            "Martin Magnusson",
            "Allan Wang"
          ],
          "published": "2026-02-05T12:46:37Z",
          "updated": "2026-02-05T12:46:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05608v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05596v1",
          "title": "TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards",
          "summary": "With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.",
          "authors": [
            "Hokyun Lee",
            "Woo-Jeong Baek",
            "Junhyeok Cha",
            "Jaeheung Park"
          ],
          "published": "2026-02-05T12:30:49Z",
          "updated": "2026-02-05T12:30:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05596v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05557v1",
          "title": "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds",
          "summary": "We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.",
          "authors": [
            "Michael Schwingshackl",
            "Fabio F. Oberweger",
            "Mario Niedermeyer",
            "Huemer Johannes",
            "Markus Murschitz"
          ],
          "published": "2026-02-05T11:29:09Z",
          "updated": "2026-02-05T11:29:09Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05557v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05555v1",
          "title": "IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools",
          "summary": "We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.",
          "authors": [
            "Panagiotis Sapoutzoglou",
            "Orestis Vaggelis",
            "Athina Zacharia",
            "Evangelos Sartinas",
            "Maria Pateraki"
          ],
          "published": "2026-02-05T11:28:57Z",
          "updated": "2026-02-05T11:28:57Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05555v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05552v1",
          "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator",
          "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.",
          "authors": [
            "Bessie Dominguez-Dager",
            "Sergio Suescun-Ferrandiz",
            "Felix Escalona",
            "Francisco Gomez-Donoso",
            "Miguel Cazorla"
          ],
          "published": "2026-02-05T11:23:11Z",
          "updated": "2026-02-05T11:23:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05552v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05516v1",
          "title": "Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments",
          "summary": "This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.",
          "authors": [
            "Runxiao Liu",
            "Pengda Mao",
            "Xiangli Le",
            "Shuang Gu",
            "Yapeng Chen",
            "Quan Quan"
          ],
          "published": "2026-02-05T10:16:05Z",
          "updated": "2026-02-05T10:16:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05516v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05513v1",
          "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter",
          "summary": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.",
          "authors": [
            "Xukun Li",
            "Yu Sun",
            "Lei Zhang",
            "Bosheng Huang",
            "Yibo Peng",
            "Yuan Meng",
            "Haojun Jiang",
            "Shaoxuan Xie",
            "Guacai Yao",
            "Alois Knoll",
            "Zhenshan Bing",
            "Xinlong Wang",
            "Zhenguo Sun"
          ],
          "published": "2026-02-05T10:13:34Z",
          "updated": "2026-02-05T10:13:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05513v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05468v1",
          "title": "TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation",
          "summary": "Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.",
          "authors": [
            "Pranav Ponnivalavan",
            "Satoshi Funabashi",
            "Alexander Schmitz",
            "Tetsuya Ogata",
            "Shigeki Sugano"
          ],
          "published": "2026-02-05T09:16:06Z",
          "updated": "2026-02-05T09:16:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05468v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05467v1",
          "title": "MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation",
          "summary": "Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.",
          "authors": [
            "Dekang Qi",
            "Shuang Zeng",
            "Xinyuan Chang",
            "Feng Xiong",
            "Shichao Xie",
            "Xiaolong Wu",
            "Mu Xu"
          ],
          "published": "2026-02-05T09:15:34Z",
          "updated": "2026-02-05T09:15:34Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.CL",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05467v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05456v1",
          "title": "Ontology-Driven Robotic Specification Synthesis",
          "summary": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.",
          "authors": [
            "Maksym Figat",
            "Ryan M. Mackey",
            "Michel D. Ingham"
          ],
          "published": "2026-02-05T08:59:23Z",
          "updated": "2026-02-05T08:59:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05456v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05441v1",
          "title": "Benchmarking Affordance Generalization with BusyBox",
          "summary": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features. In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.",
          "authors": [
            "Dean Fortier",
            "Timothy Adamson",
            "Tess Hellebrekers",
            "Teresa LaScala",
            "Kofi Ennin",
            "Michael Murray",
            "Andrey Kolobov",
            "Galen Mullins"
          ],
          "published": "2026-02-05T08:31:27Z",
          "updated": "2026-02-05T08:31:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05441v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05325v2",
          "title": "RoboPaint: From Human Demonstration to Any Robot and Any View",
          "summary": "Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently \"painted\" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.",
          "authors": [
            "Jiacheng Fan",
            "Zhiyue Zhao",
            "Yiqian Zhang",
            "Chao Chen",
            "Peide Wang",
            "Hengdi Zhang",
            "Zhengxue Cheng"
          ],
          "published": "2026-02-05T05:45:12Z",
          "updated": "2026-02-07T02:08:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05325v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05324v1",
          "title": "A Data Driven Structural Decomposition of Dynamic Games via Best Response Maps",
          "summary": "Dynamic games are powerful tools to model multi-agent decision-making, yet computing Nash (generalized Nash) equilibria remains a central challenge in such settings. Complexity arises from tightly coupled optimality conditions, nested optimization structures, and poor numerical conditioning. Existing game-theoretic solvers address these challenges by directly solving the joint game, typically requiring explicit modeling of all agents' objective functions and constraints, while learning-based approaches often decouple interaction through prediction or policy approximation, sacrificing equilibrium consistency. This paper introduces a conceptually novel formulation for dynamic games by restructuring the equilibrium computation. Rather than solving a fully coupled game or decoupling agents through prediction or policy approximation, a data-driven structural reduction of the game is proposed that removes nested optimization layers and derivative coupling by embedding an offline-compiled best-response map as a feasibility constraint. Under standard regularity conditions, when the best-response operator is exact, any converged solution of the reduced problem corresponds to a local open-loop Nash (GNE) equilibrium of the original game; with a learned surrogate, the solution is approximately equilibrium-consistent up to the best-response approximation error. The proposed formulation is supported by mathematical proofs, accompanying a large-scale Monte Carlo study in a two-player open-loop dynamic game motivated by the autonomous racing problem. Comparisons are made against state-of-the-art joint game solvers, and results are reported on solution quality, computational cost, and constraint satisfaction.",
          "authors": [
            "Mahdis Rabbani",
            "Navid Mojahed",
            "Shima Nazari"
          ],
          "published": "2026-02-05T05:44:53Z",
          "updated": "2026-02-05T05:44:53Z",
          "primary_category": "cs.GT",
          "categories": [
            "cs.GT",
            "cs.MA",
            "cs.RO",
            "eess.SY",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05324v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05311v1",
          "title": "Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates",
          "summary": "Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \\emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.",
          "authors": [
            "Chengxiao Wang",
            "Haoze Wu",
            "Gagandeep Singh"
          ],
          "published": "2026-02-05T05:08:01Z",
          "updated": "2026-02-05T05:08:01Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05311v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05310v1",
          "title": "Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework",
          "summary": "Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.",
          "authors": [
            "Jipeng Kong",
            "Xinzhe Liu",
            "Yuhang Lin",
            "Jinrui Han",
            "Sören Schwertfeger",
            "Chenjia Bai",
            "Xuelong Li"
          ],
          "published": "2026-02-05T05:05:03Z",
          "updated": "2026-02-05T05:05:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05310v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06088v1",
          "title": "Transformer-Based Reinforcement Learning for Autonomous Orbital Collision Avoidance in Partially Observable Environments",
          "summary": "We introduce a Transformer-based Reinforcement Learning framework for autonomous orbital collision avoidance that explicitly models the effects of partial observability and imperfect monitoring in space operations. The framework combines a configurable encounter simulator, a distance-dependent observation model, and a sequential state estimator to represent uncertainty in relative motion. A central contribution of this work is the use of transformer-based Partially Observable Markov Decision Process (POMDP) architecture, which leverage long-range temporal attention to interpret noisy and intermittent observations more effectively than traditional architectures. This integration provides a foundation for training collision avoidance agents that can operate more reliably under imperfect monitoring environments.",
          "authors": [
            "Thomas Georges",
            "Adam Abdin"
          ],
          "published": "2026-02-05T04:57:58Z",
          "updated": "2026-02-05T04:57:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06088v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06087v1",
          "title": "Dynamic Modeling, Parameter Identification and Numerical Analysis of Flexible Cables in Flexibly Connected Dual-AUV Systems",
          "summary": "This research presents a dynamic modeling framework and parameter identification methods for describing the highly nonlinear behaviors of flexibly connected dual-AUV systems. The modeling framework is established based on the lumped mass method, integrating axial elasticity, bending stiffness, added mass and hydrodynamic forces, thereby accurately capturing the time-varying response of the forces and cable configurations. To address the difficulty of directly measuring material-related and hydrodynamic coefficients, this research proposes a parameter identification method that combines the physical model with experimental data. High-precision inversion of the equivalent Youngs modulus and hydrodynamic coefficients is performed through tension experiments under multiple configurations, effectively demonstrating that the identified model maintains predictive consistency in various operational conditions. Further numerical analysis indicates that the dynamic properties of flexible cable exhibit significant nonlinear characteristics, which are highly dependent on material property variations and AUV motion conditions. This nonlinear dynamic behavior results in two typical response states, slack and taut, which are jointly determined by boundary conditions and hydrodynamic effects, significantly affecting the cable configuration and endpoint loads. In this research, the dynamics of flexible cables under complex boundary conditions is revealed, providing a theoretical foundation for the design, optimization and further control research of similar systems.",
          "authors": [
            "Kuo Chen",
            "Minghao Dou",
            "Qianqi Liu",
            "Yang An",
            "Kai Ren",
            "Zeming WU",
            "Yu Tian",
            "Jie Sun",
            "Xinping Wang",
            "Zhier Chen",
            "Jiancheng Yu"
          ],
          "published": "2026-02-05T04:19:59Z",
          "updated": "2026-02-05T04:19:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06087v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05273v1",
          "title": "Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions",
          "summary": "Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for \"I'm thirsty\") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\\% and more than 95\\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.",
          "authors": [
            "Hengxuan Xu",
            "Fengbo Lan",
            "Zhixin Zhao",
            "Shengjie Wang",
            "Mengqiao Liu",
            "Jieqian Sun",
            "Yu Cheng",
            "Tao Zhang"
          ],
          "published": "2026-02-05T03:58:34Z",
          "updated": "2026-02-05T03:58:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05273v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05265v1",
          "title": "Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot",
          "summary": "Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.",
          "authors": [
            "Kalvik Jakkala",
            "Jason O'Kane"
          ],
          "published": "2026-02-05T03:45:55Z",
          "updated": "2026-02-05T03:45:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05265v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05257v1",
          "title": "RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation",
          "summary": "Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.",
          "authors": [
            "Diya He",
            "Qingchen Liu",
            "Cong Zhang",
            "Jiahu Qin"
          ],
          "published": "2026-02-05T03:26:15Z",
          "updated": "2026-02-05T03:26:15Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05257v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05233v1",
          "title": "MobileManiBench: Simplifying Model Verification for Mobile Manipulation",
          "summary": "Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.",
          "authors": [
            "Wenbo Wang",
            "Fangyun Wei",
            "QiXiu Li",
            "Xi Chen",
            "Yaobo Liang",
            "Chang Xu",
            "Jiaolong Yang",
            "Baining Guo"
          ],
          "published": "2026-02-05T02:49:52Z",
          "updated": "2026-02-05T02:49:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05233v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05198v1",
          "title": "Informative Path Planning with Guaranteed Estimation Uncertainty",
          "summary": "Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region. We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.",
          "authors": [
            "Kalvik Jakkala",
            "Saurav Agarwal",
            "Jason O'Kane",
            "Srinivas Akella"
          ],
          "published": "2026-02-05T01:51:38Z",
          "updated": "2026-02-05T01:51:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05198v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05156v1",
          "title": "PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation",
          "summary": "We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.",
          "authors": [
            "Dong Ho Kang",
            "Aaron Kim",
            "Mingyo Seo",
            "Kazuto Yokoyama",
            "Tetsuya Narita",
            "Luis Sentis"
          ],
          "published": "2026-02-05T00:17:08Z",
          "updated": "2026-02-05T00:17:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05156v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05142v2",
          "title": "Modelling Pedestrian Behaviour in Autonomous Vehicle Encounters Using Naturalistic Dataset",
          "summary": "Understanding how pedestrians adjust their movement when interacting with autonomous vehicles (AVs) is essential for improving safety in mixed traffic. This study examines micro-level pedestrian behaviour during midblock encounters in the NuScenes dataset using a hybrid discrete choice-machine learning framework based on the Residual Logit (ResLogit) model. The model incorporates temporal, spatial, kinematic, and perceptual indicators. These include relative speed, visual looming, remaining distance, and directional collision risk proximity (CRP) measures. Results suggest that some of these variables may meaningfully influence movement adjustments, although predictive performance remains moderate. Marginal effects and elasticities indicate strong directional asymmetries in risk perception, with frontal and rear CRP showing opposite influences. The remaining distance exhibits a possible mid-crossing threshold. Relative speed cues appear to have a comparatively less effect. These patterns may reflect multiple behavioural tendencies driven by both risk perception and movement efficiency.",
          "authors": [
            "Rulla Al-Haideri",
            "Bilal Farooq"
          ],
          "published": "2026-02-04T23:58:38Z",
          "updated": "2026-02-09T22:14:21Z",
          "primary_category": "physics.soc-ph",
          "categories": [
            "physics.soc-ph",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05142v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05121v1",
          "title": "Trojan Attacks on Neural Network Controllers for Robotic Systems",
          "summary": "Neural network controllers are increasingly deployed in robotic systems for tasks such as trajectory tracking and pose stabilization. However, their reliance on potentially untrusted training pipelines or supply chains introduces significant security vulnerabilities. This paper investigates backdoor (Trojan) attacks against neural controllers, using a differential-drive mobile robot platform as a case study. In particular, assuming that the robot's tracking controller is implemented as a neural network, we design a lightweight, parallel Trojan network that can be embedded within the controller. This malicious module remains dormant during normal operation but, upon detecting a highly specific trigger condition defined by the robot's pose and goal parameters, compromises the primary controller's wheel velocity commands, resulting in undesired and potentially unsafe robot behaviours. We provide a proof-of-concept implementation of the proposed Trojan network, which is validated through simulation under two different attack scenarios. The results confirm the effectiveness of the proposed attack and demonstrate that neural network-based robotic control systems are subject to potentially critical security threats.",
          "authors": [
            "Farbod Younesi",
            "Walter Lucia",
            "Amr Youssef"
          ],
          "published": "2026-02-04T23:12:22Z",
          "updated": "2026-02-04T23:12:22Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05121v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05105v1",
          "title": "GAMMS: Graph based Adversarial Multiagent Modeling Simulator",
          "summary": "As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/",
          "authors": [
            "Rohan Patil",
            "Jai Malegaonkar",
            "Xiao Jiang",
            "Andre Dion",
            "Gaurav S. Sukhatme",
            "Henrik I. Christensen"
          ],
          "published": "2026-02-04T22:38:51Z",
          "updated": "2026-02-04T22:38:51Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.RO",
            "cs.SE"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05105v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05092v1",
          "title": "A Framework for Combining Optimization-Based and Analytic Inverse Kinematics",
          "summary": "Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.",
          "authors": [
            "Thomas Cohn",
            "Lihan Tang",
            "Alexandre Amice",
            "Russ Tedrake"
          ],
          "published": "2026-02-04T22:23:32Z",
          "updated": "2026-02-04T22:23:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05092v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05091v1",
          "title": "Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal",
          "summary": "Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.",
          "authors": [
            "Agni Bandyopadhyay",
            "Günther Waxenegger-Wilfing"
          ],
          "published": "2026-02-04T22:22:40Z",
          "updated": "2026-02-04T22:22:40Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.LG",
            "cs.RO",
            "physics.space-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05091v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05089v2",
          "title": "Beware Untrusted Simulators -- Reward-Free Backdoor Attacks in Reinforcement Learning",
          "summary": "Simulated environments are a key piece in the success of Reinforcement Learning (RL), allowing practitioners and researchers to train decision making agents without running expensive experiments on real hardware. Simulators remain a security blind spot, however, enabling adversarial developers to alter the dynamics of their released simulators for malicious purposes. Therefore, in this work we highlight a novel threat, demonstrating how simulator dynamics can be exploited to stealthily implant action-level backdoors into RL agents. The backdoor then allows an adversary to reliably activate targeted actions in an agent upon observing a predefined ``trigger'', leading to potentially dangerous consequences. Traditional backdoor attacks are limited in their strong threat models, assuming the adversary has near full control over an agent's training pipeline, enabling them to both alter and observe agent's rewards. As these assumptions are infeasible to implement within a simulator, we propose a new attack ``Daze'' which is able to reliably and stealthily implant backdoors into RL agents trained for real world tasks without altering or even observing their rewards. We provide formal proof of Daze's effectiveness in guaranteeing attack success across general RL tasks along with extensive empirical evaluations on both discrete and continuous action space domains. We additionally provide the first example of RL backdoor attacks transferring to real, robotic hardware. These developments motivate further research into securing all components of the RL training pipeline to prevent malicious attacks.",
          "authors": [
            "Ethan Rathbun",
            "Wo Wei Lin",
            "Alina Oprea",
            "Christopher Amato"
          ],
          "published": "2026-02-04T22:17:23Z",
          "updated": "2026-02-09T17:46:50Z",
          "primary_category": "cs.CR",
          "categories": [
            "cs.CR",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05089v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.17685v1",
          "title": "Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling",
          "summary": "This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.",
          "authors": [
            "Agni Bandyopadhyay",
            "Gunther Waxenegger-Wilfing"
          ],
          "published": "2026-02-04T22:15:14Z",
          "updated": "2026-02-04T22:15:14Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO",
            "physics.space-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.17685v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05079v1",
          "title": "Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking",
          "summary": "The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.",
          "authors": [
            "Vinal Asodia",
            "Iman Sharifi",
            "Saber Fallah"
          ],
          "published": "2026-02-04T21:56:27Z",
          "updated": "2026-02-04T21:56:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05079v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05075v1",
          "title": "Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance",
          "summary": "As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR. Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters. The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches. This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.",
          "authors": [
            "Agni Bandyopadhyay",
            "Gunther Waxenegger-Wilfing"
          ],
          "published": "2026-02-04T21:49:20Z",
          "updated": "2026-02-04T21:49:20Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.LG",
            "cs.RO",
            "physics.space-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05075v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05051v1",
          "title": "ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation",
          "summary": "Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.",
          "authors": [
            "Songyuan Zhang",
            "Oswin So",
            "H. M. Sabbir Ahmad",
            "Eric Yang Yu",
            "Matthew Cleaveland",
            "Mitchell Black",
            "Chuchu Fan"
          ],
          "published": "2026-02-04T21:03:11Z",
          "updated": "2026-02-04T21:03:11Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05051v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05049v1",
          "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models",
          "summary": "Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .",
          "authors": [
            "Yiye Chen",
            "Yanan Jian",
            "Xiaoyi Dong",
            "Shuxin Cao",
            "Jing Wu",
            "Patricio Vela",
            "Benjamin E. Lundell",
            "Dongdong Chen"
          ],
          "published": "2026-02-04T20:59:29Z",
          "updated": "2026-02-04T20:59:29Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05049v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05029v1",
          "title": "Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping",
          "summary": "Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.",
          "authors": [
            "Octavio Arriaga",
            "Proneet Sharma",
            "Jichen Guo",
            "Marc Otto",
            "Siddhant Kadwe",
            "Rebecca Adam"
          ],
          "published": "2026-02-04T20:33:50Z",
          "updated": "2026-02-04T20:33:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05029v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.05010v1",
          "title": "Signal or 'Noise': Human Reactions to Robot Errors in the Wild",
          "summary": "In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but \"noisy.\" We discuss lessons, benefits, and challenges for using social signals in real-world HRI.",
          "authors": [
            "Maia Stiber",
            "Sameer Khan",
            "Russell Taylor",
            "Chien-Ming Huang"
          ],
          "published": "2026-02-04T19:52:14Z",
          "updated": "2026-02-04T19:52:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.05010v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04992v1",
          "title": "Applying Ground Robot Fleets in Urban Search: Understanding Professionals' Operational Challenges and Design Opportunities",
          "summary": "Urban searches demand rapid, defensible decisions and sustained physical effort under high cognitive and situational load. Incident commanders must plan, coordinate, and document time-critical operations, while field searchers execute evolving tasks in uncertain environments. With recent advances in technology, ground-robot fleets paired with computer-vision-based situational awareness and LLM-powered interfaces offer the potential to ease these operational burdens. However, no dedicated studies have examined how public safety professionals perceive such technologies or envision their integration into existing practices, risking building technically sophisticated yet impractical solutions. To address this gap, we conducted focus-group sessions with eight police officers across five local departments in Virginia. Our findings show that ground robots could reduce professionals' reliance on paper references, mental calculations, and ad-hoc coordination, alleviating cognitive and physical strain in four key challenge areas: (1) partitioning the workforce across multiple search hypotheses, (2) retaining group awareness and situational awareness, (3) building route planning that fits the lost-person profile, and (4) managing cognitive and physical fatigue under uncertainty. We further identify four design opportunities and requirements for future ground-robot fleet integration in public-safety operations: (1) scalable multi-robot planning and control interfaces, (2) agency-specific route optimization, (3) real-time replanning informed by debrief updates, and (4) vision-assisted cueing that preserves operational trust while reducing cognitive workload. We conclude with design implications for deployable, accountable, and human-centered urban-search support systems",
          "authors": [
            "Puqi Zhou",
            "Charles R. Twardy",
            "Cynthia Lum",
            "Myeong Lee",
            "David J. Porfirio",
            "Michael R. Hieb",
            "Chris Thomas",
            "Xuesu Xiao",
            "Sungsoo Ray Hong"
          ],
          "published": "2026-02-04T19:26:31Z",
          "updated": "2026-02-04T19:26:31Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04992v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04880v1",
          "title": "Capturing Visual Environment Structure Correlates with Control Performance",
          "summary": "The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control.",
          "authors": [
            "Jiahua Dong",
            "Yunze Man",
            "Pavel Tokmakov",
            "Yu-Xiong Wang"
          ],
          "published": "2026-02-04T18:59:12Z",
          "updated": "2026-02-04T18:59:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04880v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04851v1",
          "title": "PDF-HR: Pose Distance Fields for Humanoid Robots",
          "summary": "Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.",
          "authors": [
            "Yi Gu",
            "Yukang Gao",
            "Yangchen Zhou",
            "Xingyu Chen",
            "Yixiao Feng",
            "Mingle Zhao",
            "Yunyang Mo",
            "Zhaorui Wang",
            "Lixin Xu",
            "Renjing Xu"
          ],
          "published": "2026-02-04T18:38:51Z",
          "updated": "2026-02-04T18:38:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04851v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04799v1",
          "title": "Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software",
          "summary": "A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.",
          "authors": [
            "Nils Chur",
            "Thorsten Berger",
            "Einar Broch Johnsen",
            "Andrzej Wąsowski"
          ],
          "published": "2026-02-04T17:45:59Z",
          "updated": "2026-02-04T17:45:59Z",
          "primary_category": "cs.SE",
          "categories": [
            "cs.SE",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04799v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04787v1",
          "title": "PuppetAI: A Customizable Platform for Designing Tactile-Rich Affective Robot Interaction",
          "summary": "We introduce PuppetAI, a modular soft robot interaction platform. This platform offers a scalable cable-driven actuation system and a customizable, puppet-inspired robot gesture framework, supporting a multitude of interaction gesture robot design formats. The platform comprises a four-layer decoupled software architecture that includes perceptual processing, affective modeling, motion scheduling, and low-level actuation. We also implemented an affective expression loop that connects human input to the robot platform by producing real-time emotional gestural responses to human vocal input. For our own designs, we have worked with nuanced gestures enacted by \"soft robots\" with enhanced dexterity and \"pleasant-to-touch\" plush exteriors. By reducing operational complexity and production costs while enhancing customizability, our work creates an adaptable and accessible foundation for future tactile-based expressive robot research. Our goal is to provide a platform that allows researchers to independently construct or refine highly specific gestures and movements performed by social robots.",
          "authors": [
            "Jiaye Li",
            "Tongshun Chen",
            "Siyi Ma",
            "Elizabeth Churchill",
            "Ke Wu"
          ],
          "published": "2026-02-04T17:37:31Z",
          "updated": "2026-02-04T17:37:31Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04787v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04746v2",
          "title": "Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics",
          "summary": "In robotics, the concept of \"dull, dirty, and dangerous\" (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on \"dull,\" \"dirty,\" and \"dangerous\" work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.",
          "authors": [
            "Nozomi Nakajima",
            "Pedro Reynolds-Cuéllar",
            "Caitrin Lynch",
            "Kate Darling"
          ],
          "published": "2026-02-04T16:48:06Z",
          "updated": "2026-02-05T15:26:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04746v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04672v1",
          "title": "AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation",
          "summary": "Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.",
          "authors": [
            "Jin-Chuan Shi",
            "Binhong Ye",
            "Tao Liu",
            "Junzhe He",
            "Yangjinhui Xu",
            "Xiaoyang Liu",
            "Zeju Li",
            "Hao Chen",
            "Chunhua Shen"
          ],
          "published": "2026-02-04T15:42:58Z",
          "updated": "2026-02-04T15:42:58Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.GR",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04672v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04648v1",
          "title": "From Vision to Assistance: Gaze and Vision-Enabled Adaptive Control for a Back-Support Exoskeleton",
          "summary": "Back-support exoskeletons have been proposed to mitigate spinal loading in industrial handling, yet their effectiveness critically depends on timely and context-aware assistance. Most existing approaches rely either on load-estimation techniques (e.g., EMG, IMU) or on vision systems that do not directly inform control. In this work, we present a vision-gated control framework for an active lumbar occupational exoskeleton that leverages egocentric vision with wearable gaze tracking. The proposed system integrates real-time grasp detection from a first-person YOLO-based perception system, a finite-state machine (FSM) for task progression, and a variable admittance controller to adapt torque delivery to both posture and object state. A user study with 15 participants performing stooping load lifting trials under three conditions (no exoskeleton, exoskeleton without vision, exoskeleton with vision) shows that vision-gated assistance significantly reduces perceived physical demand and improves fluency, trust, and comfort. Quantitative analysis reveals earlier and stronger assistance when vision is enabled, while questionnaire results confirm user preference for the vision-gated mode. These findings highlight the potential of egocentric vision to enhance the responsiveness, ergonomics, safety, and acceptance of back-support exoskeletons.",
          "authors": [
            "Alessandro Leanza",
            "Paolo Franceschi",
            "Blerina Spahiu",
            "Loris Roveda"
          ],
          "published": "2026-02-04T15:23:42Z",
          "updated": "2026-02-04T15:23:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04648v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04635v1",
          "title": "Relational Scene Graphs for Object Grounding of Natural Language Commands",
          "summary": "Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot's knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited.",
          "authors": [
            "Julia Kuhn",
            "Francesco Verdoja",
            "Tsvetomila Mihaylova",
            "Ville Kyrki"
          ],
          "published": "2026-02-04T15:05:29Z",
          "updated": "2026-02-04T15:05:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04635v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04631v1",
          "title": "Radar-Inertial Odometry For Computationally Constrained Aerial Navigation",
          "summary": "Recently, the progress in the radar sensing technology consisting in the miniaturization of the packages and increase in measuring precision has drawn the interest of the robotics research community. Indeed, a crucial task enabling autonomy in robotics is to precisely determine the pose of the robot in space. To fulfill this task sensor fusion algorithms are often used, in which data from one or several exteroceptive sensors like, for example, LiDAR, camera, laser ranging sensor or GNSS are fused together with the Inertial Measurement Unit (IMU) measurements to obtain an estimate of the navigation states of the robot. Nonetheless, owing to their particular sensing principles, some exteroceptive sensors are often incapacitated in extreme environmental conditions, like extreme illumination or presence of fine particles in the environment like smoke or fog. Radars are largely immune to aforementioned factors thanks to the characteristics of electromagnetic waves they use. In this thesis, we present Radar-Inertial Odometry (RIO) algorithms to fuse the information from IMU and radar in order to estimate the navigation states of a (Uncrewed Aerial Vehicle) UAV capable of running on a portable resource-constrained embedded computer in real-time and making use of inexpensive, consumer-grade sensors. We present novel RIO approaches relying on the multi-state tightly-coupled Extended Kalman Filter (EKF) and Factor Graphs (FG) fusing instantaneous velocities of and distances to 3D points delivered by a lightweight, low-cost, off-the-shelf Frequency Modulated Continuous Wave (FMCW) radar with IMU readings. We also show a novel way to exploit advances in deep learning to retrieve 3D point correspondences in sparse and noisy radar point clouds.",
          "authors": [
            "Jan Michalczyk"
          ],
          "published": "2026-02-04T15:03:26Z",
          "updated": "2026-02-04T15:03:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04631v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04625v1",
          "title": "Can We Redesign a Shoulder Exosuit to Enhance Comfort and Usability Without Losing Assistance?",
          "summary": "Reduced shoulder mobility limits upper-limb function and the performance of activities of daily living across a wide range of conditions. Wearable exosuits have shown promise in assisting arm elevation, reducing muscle effort, and supporting functional movements; however, comfort is rarely prioritized as an explicit design objective, despite it strongly affects real-life, long-term usage. This study presents a redesigned soft shoulder exosuit (Soft Shoulder v2) developed to address comfort-related limitations identified in our previous version, while preserving assistive performance. In parallel, assistance was also improved, shifting from the coronal plane to the sagittal plane to better support functionally relevant hand positioning. A controlled comparison between the previous (v1) and redesigned (v2) modules was conducted in eight healthy participants, who performed static holding, dynamic lifting, and a functional pick and place task. Muscle activity, kinematics, and user-reported outcomes were assessed. Both versions increased endurance time, reduced deltoid activation, and preserved transparency during unpowered shoulder elevation. However, the difference between them emerged most clearly during functional tasks and comfort evaluation. The redesigned module facilitated forward arm positioning and increased transverse plane mobility by up to 30 deg, without increasing muscular demand. User-reported outcomes further indicated a substantial improvement in wearability, with markedly lower perceived pressure and higher ratings in effectiveness, ease of use, and comfort compared to the previous design. Taken together, these findings show that targeted, user-centered design refinements can improve comfort and functional interaction without compromising assistive performance, advancing the development of soft exosuits suitable for prolonged and daily use.",
          "authors": [
            "Roberto Ferroni",
            "Daniele Filippo Mauceri",
            "Jacopo Carpaneto",
            "Alessandra Pedrocchi",
            "Tommaso Proietti"
          ],
          "published": "2026-02-04T14:56:25Z",
          "updated": "2026-02-04T14:56:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04625v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04600v1",
          "title": "Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data",
          "summary": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios.",
          "authors": [
            "Jialiang Li",
            "Yi Qiao",
            "Yunhan Guo",
            "Changwen Chen",
            "Wenzhao Lian"
          ],
          "published": "2026-02-04T14:28:14Z",
          "updated": "2026-02-04T14:28:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04600v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04522v1",
          "title": "A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction",
          "summary": "Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers.",
          "authors": [
            "Bingkun Huang",
            "Xin Ma",
            "Nilanjan Chakraborty",
            "Riddhiman Laha"
          ],
          "published": "2026-02-04T13:10:57Z",
          "updated": "2026-02-04T13:10:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04522v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04517v1",
          "title": "S-MUSt3R: Sliding Multi-view 3D Reconstruction",
          "summary": "The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.",
          "authors": [
            "Leonid Antsfeld",
            "Boris Chidlovskii",
            "Yohann Cabon",
            "Vincent Leroy",
            "Jerome Revaud"
          ],
          "published": "2026-02-04T13:07:14Z",
          "updated": "2026-02-04T13:07:14Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04517v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04516v2",
          "title": "TACO: Temporal Consensus Optimization for Continual Neural Mapping",
          "summary": "Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.",
          "authors": [
            "Xunlan Zhou",
            "Hongrui Zhao",
            "Negar Mehr"
          ],
          "published": "2026-02-04T13:07:08Z",
          "updated": "2026-02-05T09:31:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04516v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04515v1",
          "title": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models",
          "summary": "Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.",
          "authors": [
            "Yu Bai",
            "MingMing Yu",
            "Chaojie Li",
            "Ziyi Bai",
            "Xinlong Wang",
            "Börje F. Karlsson"
          ],
          "published": "2026-02-04T13:04:56Z",
          "updated": "2026-02-04T13:04:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04515v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04487v1",
          "title": "The Supportiveness-Safety Tradeoff in LLM Well-Being Agents",
          "summary": "Large language models (LLMs) are being integrated into socially assistive robots (SARs) and other conversational agents providing mental health and well-being support. These agents are often designed to sound empathic and supportive in order to maximize user's engagement, yet it remains unclear how increasing the level of supportive framing in system prompts influences safety relevant behavior. We evaluated 6 LLMs across 3 system prompts with varying levels of supportiveness on 80 synthetic queries spanning 4 well-being domains (1440 responses). An LLM judge framework, validated against human ratings, assessed safety and care quality. Moderately supportive prompts improved empathy and constructive support while maintaining safety. In contrast, strongly validating prompts significantly degraded safety and, in some cases, care across all domains, with substantial variation across models. We discuss implications for prompt design, model selection, and domain specific safeguards in SARs deployment.",
          "authors": [
            "Himanshi Lalwani",
            "Hanan Salam"
          ],
          "published": "2026-02-04T12:15:43Z",
          "updated": "2026-02-04T12:15:43Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04487v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04458v2",
          "title": "Robot-Assisted Group Tours for Blind People",
          "summary": "Group interactions are essential to social functioning, yet effective engagement relies on the ability to recognize and interpret visual cues, making such engagement a significant challenge for blind people. In this paper, we investigate how a mobile robot can support group interactions for blind people. We used the scenario of a guided tour with mixed-visual groups involving blind and sighted visitors. Based on insights from an interview study with blind people (n=5) and museum experts (n=5), we designed and prototyped a robotic system that supported blind visitors to join group tours. We conducted a field study in a science museum where each blind participant (n=8) joined a group tour with one guide and two sighted participants (n=8). Findings indicated users' sense of safety from the robot's navigational support, concerns in the group participation, and preferences for obtaining environmental information. We present design implications for future robotic systems to support blind people's mixed-visual group participation.",
          "authors": [
            "Yaxin Hu",
            "Masaki Kuribayashi",
            "Allan Wang",
            "Seita Kayukawa",
            "Daisuke Sato",
            "Bilge Mutlu",
            "Hironobu Takagi",
            "Chieko Asakawa"
          ],
          "published": "2026-02-04T11:42:42Z",
          "updated": "2026-02-19T07:30:08Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04458v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04438v2",
          "title": "Gust Estimation and Rejection with a Disturbance Observer for Proprioceptive Underwater Soft Morphing Wings",
          "summary": "Unmanned underwater vehicles are increasingly employed for maintenance and surveying tasks at sea, but their operation in shallow waters is often hindered by hydrodynamic disturbances such as waves, currents, and turbulence. These unsteady flows can induce rapid changes in direction and speed, compromising vehicle stability and manoeuvrability. Marine organisms contend with such conditions by combining proprioceptive feedback with flexible fins and tails to reject disturbances. Inspired by this strategy, we propose soft morphing wings endowed with proprioceptive sensing to mitigate environmental perturbations. The wing's continuous deformation provides a natural means to infer dynamic disturbances: sudden changes in camber directly reflect variations in the oncoming flow. By interpreting this proprioceptive signal, a disturbance observer can reconstruct flow parameters in real time. To enable this, we develop and experimentally validate a dynamic model of a hydraulically actuated soft wing with controllable camber. We then show that curvature-based sensing allows accurate estimation of disturbances in the angle of attack. Finally, we demonstrate that a controller leveraging these proprioceptive estimates can reject disturbances in the lift response of the soft wing. By combining proprioceptive sensing with a disturbance observer, this technique mirrors biological strategies and provides a pathway for soft underwater vehicles to maintain stability in hazardous environments.",
          "authors": [
            "Tobias Cook",
            "Leo Micklem",
            "Huazhi Dong",
            "Yunjie Yang",
            "Michael Mistry",
            "Francesco Giorgio-Serchi"
          ],
          "published": "2026-02-04T11:13:51Z",
          "updated": "2026-02-08T11:52:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04438v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04419v2",
          "title": "Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning",
          "summary": "In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG's potential for real-world applications.",
          "authors": [
            "Heqing Yang",
            "Ziyuan Jiao",
            "Shu Wang",
            "Yida Niu",
            "Si Liu",
            "Hangxin Liu"
          ],
          "published": "2026-02-04T10:52:53Z",
          "updated": "2026-02-14T03:23:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04419v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04412v2",
          "title": "HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation",
          "summary": "Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher's robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at https://tonywang-0517.github.io/hord/.",
          "authors": [
            "Puyue Wang",
            "Jiawei Hu",
            "Yan Gao",
            "Junyan Wang",
            "Yu Zhang",
            "Gillian Dobbie",
            "Tao Gu",
            "Wafa Johal",
            "Ting Dang",
            "Hong Jia"
          ],
          "published": "2026-02-04T10:41:23Z",
          "updated": "2026-02-05T02:24:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04412v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04401v1",
          "title": "Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition",
          "summary": "Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall. Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change. We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall. The method uses a small calibration traversal with known correspondences and transfers thresholds to deployment via quantile normalisation of similarity score distributions. This quantile transfer ensures that thresholds remain stable across calibration sizes and query subsets, making the method robust to sampling variability. Experiments with multiple state-of-the-art VPR techniques and datasets show that the proposed approach consistently outperforms the state-of-the-art, delivering up to 25% higher recall in high-precision operating regimes. The method eliminates manual tuning by adapting to new environments and generalising across operating conditions. Our code will be released upon acceptance.",
          "authors": [
            "Dhyey Manish Rajani",
            "Michael Milford",
            "Tobias Fischer"
          ],
          "published": "2026-02-04T10:31:29Z",
          "updated": "2026-02-04T10:31:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04401v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04329v1",
          "title": "Safe and Stylized Trajectory Planning for Autonomous Driving via Diffusion Model",
          "summary": "Achieving safe and stylized trajectory planning in complex real-world scenarios remains a critical challenge for autonomous driving systems. This paper proposes the SDD Planner, a diffusion-based framework designed to effectively reconcile safety constraints with driving styles in real time. The framework integrates two core modules: a Multi-Source Style-Aware Encoder, which employs distance-sensitive attention to fuse dynamic agent data and environmental contexts for heterogeneous safety-style perception; and a Style-Guided Dynamic Trajectory Generator, which adaptively modulates priority weights within the diffusion denoising process to generate user-preferred yet safe trajectories. Extensive experiments demonstrate that SDD Planner achieves state-of-the-art performance. On the StyleDrive benchmark, it improves the SM-PDMS metric by 3.9% over WoTE, the strongest baseline. Furthermore, on the NuPlan Test14 and Test14-hard benchmarks, SDD Planner ranks first with overall scores of 91.76 and 80.32, respectively, outperforming leading methods such as PLUTO. Real-vehicle closed-loop tests further confirm that SDD Planner maintains high safety standards while aligning with preset driving styles, validating its practical applicability for real-world deployment.",
          "authors": [
            "Shuo Pei",
            "Yong Wang",
            "Yuanchen Zhu",
            "Chen Sun",
            "Qin Li",
            "Yanan Zhao",
            "Huachun Tan"
          ],
          "published": "2026-02-04T08:46:05Z",
          "updated": "2026-02-04T08:46:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04329v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04315v1",
          "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
          "summary": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
          "authors": [
            "Guoqing Ma",
            "Siheng Wang",
            "Zeyu Zhang",
            "Shan Yu",
            "Hao Tang"
          ],
          "published": "2026-02-04T08:30:27Z",
          "updated": "2026-02-04T08:30:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04315v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04256v1",
          "title": "AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models",
          "summary": "End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of end-to-end driving models in diverse and unseen scenarios. However, existing VLM-based approaches still face challenges, including suboptimal lane perception, language understanding biases, and difficulties in handling corner cases. To address these issues, we propose AppleVLM, an advanced perception and planning-enhanced VLM model for robust end-to-end driving. AppleVLM introduces a novel vision encoder and a planning strategy encoder to improve perception and decision-making. Firstly, the vision encoder fuses spatial-temporal information from multi-view images across multiple timesteps using a deformable transformer mechanism, enhancing robustness to camera variations and facilitating scalable deployment across different vehicle platforms. Secondly, unlike traditional VLM-based approaches, AppleVLM introduces a dedicated planning modality that encodes explicit Bird's-Eye-View spatial information, mitigating language biases in navigation instructions. Finally, a VLM decoder fine-tuned by a hierarchical Chain-of-Thought integrates vision, language, and planning features to output robust driving waypoints. We evaluate AppleVLM in closed-loop experiments on two CARLA benchmarks, achieving state-of-the-art driving performance. Furthermore, we deploy AppleVLM on an AGV platform and successfully showcase real-world end-to-end autonomous driving in complex outdoor environments.",
          "authors": [
            "Yuxuan Han",
            "Kunyuan Wu",
            "Qianyi Shao",
            "Renxiang Xiao",
            "Zilu Wang",
            "Cansen Jiang",
            "Yi Xiao",
            "Liang Hu",
            "Yunjiang Lou"
          ],
          "published": "2026-02-04T06:37:14Z",
          "updated": "2026-02-04T06:37:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04256v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04251v1",
          "title": "Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions",
          "summary": "Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments. 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM. This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM. We analyze performance optimization of representative methods across four critical dimensions: rendering quality, tracking accuracy, reconstruction speed, and memory consumption, delving into their design principles and breakthroughs. Furthermore, we examine methods for enhancing the robustness of 3DGS-SLAM in complex environments such as motion blur and dynamic environments. Finally, we discuss future challenges and development trends in this area. This survey aims to provide a technical reference for researchers and foster the development of next-generation SLAM systems characterized by high fidelity, efficiency, and robustness.",
          "authors": [
            "Li Wang",
            "Ruixuan Gong",
            "Yumo Han",
            "Lei Yang",
            "Lu Yang",
            "Ying Li",
            "Bin Xu",
            "Huaping Liu",
            "Rong Fu"
          ],
          "published": "2026-02-04T06:20:22Z",
          "updated": "2026-02-04T06:20:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04251v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04243v1",
          "title": "Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation",
          "summary": "Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptability and coverage. Inspired by human active perception, where humans dynamically adjust their viewpoint to capture the most relevant and least noisy information, we propose MAE-Select, a novel framework for active viewpoint selection in single-camera robotic systems. MAE-Select fully leverages pre-trained multi-view masked autoencoder representations and dynamically selects the next most informative viewpoint at each time chunk without requiring labeled viewpoints. Extensive experiments demonstrate that MAE-Select improves the capabilities of single-camera systems and, in some cases, even surpasses multi-camera setups. The project will be available at https://mae-select.github.io.",
          "authors": [
            "Pengfei Yi",
            "Yifan Han",
            "Junyan Li",
            "Litao Liu",
            "Wenzhao Lian"
          ],
          "published": "2026-02-04T06:05:39Z",
          "updated": "2026-02-04T06:05:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04243v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04240v1",
          "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
          "summary": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention. In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation. To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.",
          "authors": [
            "Suzeyu Chen",
            "Leheng Li",
            "Ying-Cong Chen"
          ],
          "published": "2026-02-04T05:59:24Z",
          "updated": "2026-02-04T05:59:24Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04240v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04231v1",
          "title": "GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning",
          "summary": "Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings.",
          "authors": [
            "Rui Tang",
            "Guankun Wang",
            "Long Bai",
            "Huxin Gao",
            "Jiewen Lai",
            "Chi Kit Ng",
            "Jiazheng Wang",
            "Fan Zhang",
            "Hongliang Ren"
          ],
          "published": "2026-02-04T05:42:55Z",
          "updated": "2026-02-04T05:42:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04231v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04228v1",
          "title": "Reshaping Action Error Distributions for Reliable Vision-Language-Action Models",
          "summary": "In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: https://cognition2actionlab.github.io/VLA-TMEE.github.io/",
          "authors": [
            "Shuanghao Bai",
            "Dakai Wang",
            "Cheng Chi",
            "Wanqi Zhou",
            "Jing Lyu",
            "Xiaoguang Zhao",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Lei Xing",
            "Shanghang Zhang",
            "Badong Chen"
          ],
          "published": "2026-02-04T05:37:09Z",
          "updated": "2026-02-04T05:37:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04228v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13267v1",
          "title": "Beyond Ground: Map-Free LiDAR Relocalization for UAVs",
          "summary": "Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.",
          "authors": [
            "Hengyu Mu",
            "Jianshi Wu",
            "Yuxin Guo",
            "XianLian Lin",
            "Qingyong Hu",
            "Chenglu Wen",
            "Cheng Wang"
          ],
          "published": "2026-02-04T05:36:14Z",
          "updated": "2026-02-04T05:36:14Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO",
            "eess.IV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13267v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04215v2",
          "title": "OAT: Ordered Action Tokenization",
          "summary": "Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approaches either rely on analytical discretization methods that produce prohibitively long token sequences, or learned latent tokenizers that lack structure, limiting their compatibility with next-token prediction. In this work, we identify three desiderata for action tokenization - high compression, total decodability, and a left-to-right causally ordered token space - and introduce Ordered Action Tokenization (OAT), a learned action tokenizer that satisfies all three. OAT discretizes action chunks into an ordered sequence of tokens using transformer with registers, finite scalar quantization, and ordering-inducing training mechanisms. The resulting token space aligns naturally with autoregressive generation and enables prefix-based detokenization, yielding an anytime trade-off between inference cost and action fidelity. Across more than 20 tasks spanning four simulation benchmarks and real-world settings, autoregressive policies equipped with OAT consistently outperform prior tokenization schemes and diffusion-based baselines, while offering significantly greater flexibility at inference time.",
          "authors": [
            "Chaoqi Liu",
            "Xiaoshen Han",
            "Jiawei Gao",
            "Yue Zhao",
            "Haonan Chen",
            "Yilun Du"
          ],
          "published": "2026-02-04T05:01:04Z",
          "updated": "2026-02-11T08:23:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04215v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04214v1",
          "title": "ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator",
          "summary": "Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at https://zhihaibi.github.io/Alore/.",
          "authors": [
            "Zhihai Bi",
            "Yushan Zhang",
            "Kai Chen",
            "Guoyang Zhao",
            "Yulin Li",
            "Jun Ma"
          ],
          "published": "2026-02-04T04:57:36Z",
          "updated": "2026-02-04T04:57:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04214v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04208v1",
          "title": "SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models",
          "summary": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.",
          "authors": [
            "Hyeonbeom Choi",
            "Daechul Ahn",
            "Youhan Lee",
            "Taewook Kang",
            "Seongwon Cho",
            "Jonghyun Choi"
          ],
          "published": "2026-02-04T04:48:16Z",
          "updated": "2026-02-04T04:48:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04208v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04184v1",
          "title": "Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models",
          "summary": "Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a \"good\" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning",
          "authors": [
            "Angel Martinez-Sanchez",
            "Parthib Roy",
            "Ross Greer"
          ],
          "published": "2026-02-04T03:44:56Z",
          "updated": "2026-02-04T03:44:56Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04184v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04174v1",
          "title": "GenMRP: A Generative Multi-Route Planning Framework for Efficient and Personalized Real-Time Industrial Navigation",
          "summary": "Existing industrial-scale navigation applications contend with massive road networks, typically employing two main categories of approaches for route planning. The first relies on precomputed road costs for optimal routing and heuristic algorithms for generating alternatives, while the second, generative methods, has recently gained significant attention. However, the former struggles with personalization and route diversity, while the latter fails to meet the efficiency requirements of large-scale real-time scenarios. To address these limitations, we propose GenMRP, a generative framework for multi-route planning. To ensure generation efficiency, GenMRP first introduces a skeleton-to-capillary approach that dynamically constructs a relevant sub-network significantly smaller than the full road network. Within this sub-network, routes are generated iteratively. The first iteration identifies the optimal route, while the subsequent ones generate alternatives that balance quality and diversity using the newly proposed correctional boosting approach. Each iteration incorporates road features, user historical sequences, and previously generated routes into a Link Cost Model to update road costs, followed by route generation using the Dijkstra algorithm. Extensive experiments show that GenMRP achieves state-of-the-art performance with high efficiency in both offline and online environments. To facilitate further research, we have publicly released the training and evaluation dataset. GenMRP has been fully deployed in a real-world navigation app, demonstrating its effectiveness and benefits.",
          "authors": [
            "Chengzhang Wang",
            "Chao Chen",
            "Jun Tao",
            "Tengfei Liu",
            "He Bai",
            "Song Wang",
            "Longfei Xu",
            "Kaikui Liu",
            "Xiangxiang Chu"
          ],
          "published": "2026-02-04T03:21:21Z",
          "updated": "2026-02-04T03:21:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.GR",
            "cs.IR"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04174v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04157v1",
          "title": "A Modern System Recipe for Situated Embodied Human-Robot Conversation with Real-Time Multimodal LLMs and Tool-Calling",
          "summary": "Situated embodied conversation requires robots to interleave real-time dialogue with active perception: deciding what to look at, when to look, and what to say under tight latency constraints. We present a simple, minimal system recipe that pairs a real-time multimodal language model with a small set of tool interfaces for attention and active perception. We study six home-style scenarios that require frequent attention shifts and increasing perceptual scope. Across four system variants, we evaluate turn-level tool-decision correctness against human annotations and collect subjective ratings of interaction quality. Results indicate that real-time multimodal large language models and tool use for active perception is a promising direction for practical situated embodied conversation.",
          "authors": [
            "Dong Won Lee",
            "Sarah Gillet",
            "Louis-Philippe Morency",
            "Cynthia Breazeal",
            "Hae Won Park"
          ],
          "published": "2026-02-04T02:48:16Z",
          "updated": "2026-02-04T02:48:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04157v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04152v1",
          "title": "MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments",
          "summary": "Current 3D scene graph generation (3DSGG) approaches heavily rely on a single-agent assumption and small-scale environments, exhibiting limited scalability to real-world scenarios. In this work, we introduce Multi-Agent 3D Scene Graph Generation (MA3DSG) model, the first framework designed to tackle this scalability challenge using multiple agents. We develop a training-free graph alignment algorithm that efficiently merges partial query graphs from individual agents into a unified global scene graph. Leveraging extensive analysis and empirical insights, our approach enables conventional single-agent systems to operate collaboratively without requiring any learnable parameters. To rigorously evaluate 3DSGG performance, we propose MA3DSG-Bench-a benchmark that supports diverse agent configurations, domain sizes, and environmental conditions-providing a more general and extensible evaluation framework. This work lays a solid foundation for scalable, multi-agent 3DSGG research.",
          "authors": [
            "Yirum Kim",
            "Jaewoo Kim",
            "Ue-Hwan Kim"
          ],
          "published": "2026-02-04T02:39:57Z",
          "updated": "2026-02-04T02:39:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04152v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04137v1",
          "title": "Shaping Expressiveness in Robotics: The Role of Design Tools in Crafting Embodied Robot Movements",
          "summary": "As robots increasingly become part of shared human spaces, their movements must transcend basic functionality by incorporating expressive qualities to enhance engagement and communication. This paper introduces a movement-centered design pedagogy designed to support engineers in creating expressive robotic arm movements. Through a hands-on interactive workshop informed by interdisciplinary methodologies, participants explored various creative possibilities, generating valuable insights into expressive motion design. The iterative approach proposed integrates analytical frameworks from dance, enabling designers to examine motion through dynamic and embodied dimensions. A custom manual remote controller facilitates interactive, real-time manipulation of the robotic arm, while dedicated animation software supports visualization, detailed motion sequencing, and precise parameter control. Qualitative analysis of this interactive design process reveals that the proposed \"toolbox\" effectively bridges the gap between human intent and robotic expressiveness resulting in more intuitive and engaging expressive robotic arm movements.",
          "authors": [
            "Elisabetta Zibetti",
            "Alexandra Mercader",
            "Hélène Duval",
            "Florent Levillain",
            "Audrey Rochette",
            "David St-Onge"
          ],
          "published": "2026-02-04T02:08:16Z",
          "updated": "2026-02-04T02:08:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04137v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04132v2",
          "title": "Lyapunov Constrained Soft Actor-Critic (LC-SAC) using Koopman Operator Theory for Quadrotor Trajectory Tracking",
          "summary": "Reinforcement Learning (RL) has achieved remarkable success in solving complex sequential decision-making problems. However, its application to safety-critical physical systems remains constrained by the lack of stability guarantees. Standard RL algorithms prioritize reward maximization, often yielding policies that may induce oscillations or unbounded state divergence. There has been significant work in incorporating Lyapunov-based stability guarantees in RL algorithms with key challenges being selecting a candidate Lyapunov function, computational complexity by using excessive function approximators and conservative policies by incorporating stability criterion in the learning process. In this work we propose a novel Lyapunov-constrained Soft Actor-Critic (LC-SAC) algorithm using Koopman operator theory. We propose use of extended dynamic mode decomposition (EDMD) to produce a linear approximation of the system and use this approximation to derive a closed form solution for candidate Lyapunov function. This derived Lyapunov function is incorporated in the SAC algorithm to further provide guarantees for a policy that stabilizes the nonlinear system. The results are evaluated trajectory tracking of a 2D Quadrotor environment based on safe-control-gym. The proposed algorithm shows training convergence and decaying violations for Lyapunov stability criterion compared to baseline vanilla SAC algorithm. GitHub Repository: https://github.com/DhruvKushwaha/LC-SAC-Quadrotor-Trajectory-Tracking",
          "authors": [
            "Dhruv S. Kushwaha",
            "Zoleikha A. Biron"
          ],
          "published": "2026-02-04T01:51:05Z",
          "updated": "2026-02-06T19:54:40Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04132v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04130v1",
          "title": "Multi-threaded Recast-Based A* Pathfinding for Scalable Navigation in Dynamic Game Environments",
          "summary": "While the A* algorithm remains the industry standard for game pathfinding, its integration into dynamic 3D environments faces trade-offs between computational performance and visual realism. This paper proposes a multi-threaded framework that enhances standard A* through Recast-based mesh generation, Bezier-curve trajectory smoothing, and density analysis for crowd coordination. We evaluate our system across ten incremental phases, from 2D mazes to complex multi-level dynamic worlds. Experimental results demonstrate that the framework maintains 350+ FPS with 1000 simultaneous agents and achieves collision-free crowd navigation through density-aware path coordination.",
          "authors": [
            "Tiroshan Madushanka",
            "Sakuna Madushanka"
          ],
          "published": "2026-02-04T01:47:18Z",
          "updated": "2026-02-04T01:47:18Z",
          "primary_category": "cs.GR",
          "categories": [
            "cs.GR",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04130v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04129v1",
          "title": "KGLAMP: Knowledge Graph-guided Language model for Adaptive Multi-robot Planning and Replanning",
          "summary": "Heterogeneous multi-robot systems are increasingly deployed in long-horizon missions that require coordination among robots with diverse capabilities. However, existing planning approaches struggle to construct accurate symbolic representations and maintain plan consistency in dynamic environments. Classical PDDL planners require manually crafted symbolic models, while LLM-based planners often ignore agent heterogeneity and environmental uncertainty. We introduce KGLAMP, a knowledge-graph-guided LLM planning framework for heterogeneous multi-robot teams. The framework maintains a structured knowledge graph encoding object relations, spatial reachability, and robot capabilities, which guides the LLM in generating accurate PDDL problem specifications. The knowledge graph serves as a persistent, dynamically updated memory that incorporates new observations and triggers replanning upon detecting inconsistencies, enabling symbolic plans to adapt to evolving world states. Experiments on the MAT-THOR benchmark show that KGLAMP improves performance by at least 25.5% over both LLM-only and PDDL-based variants.",
          "authors": [
            "Chak Lam Shek",
            "Faizan M. Tariq",
            "Sangjae Bae",
            "David Isele",
            "Piyush Gupta"
          ],
          "published": "2026-02-04T01:46:02Z",
          "updated": "2026-02-04T01:46:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.ET",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04129v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04076v1",
          "title": "Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study",
          "summary": "In this paper, we introduce an autonomous Ultrasonic Sacral Osteotomy (USO) robotic system that integrates an ultrasonic osteotome with a seven-degree-of-freedom (DoF) robotic manipulator guided by an optical tracking system. To assess multi-directional control along both the surface trajectory and cutting depth of this system, we conducted quantitative comparisons between manual USO (MUSO) and robotic USO (RUSO) in Sawbones phantoms under identical osteotomy conditions. The RUSO system achieved sub-millimeter trajectory accuracy (0.11 mm RMSE), an order of magnitude improvement over MUSO (1.10 mm RMSE). Moreover, MUSO trials showed substantial over-penetration (16.0 mm achieved vs. 8.0 mm target), whereas the RUSO system maintained precise depth control (8.1 mm). These results demonstrate that robotic procedures can effectively overcome the critical limitations of manual osteotomy, establishing a foundation for safer and more precise sacral resections.",
          "authors": [
            "Daniyal Maroufi",
            "Yash Kulkarni",
            "Justin E. Bird",
            "Jeffrey H. Siewerdsen",
            "Farshid Alambeigi"
          ],
          "published": "2026-02-03T23:25:58Z",
          "updated": "2026-02-03T23:25:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04076v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04057v1",
          "title": "Control and State Estimation of Vehicle-Mounted Aerial Systems in GPS-Denied, Non-Inertial Environments",
          "summary": "We present a robust control and estimation framework for quadrotors operating in Global Navigation Satellite System(GNSS)-denied, non-inertial environments where inertial sensors such as Inertial Measurement Units (IMUs) become unreliable due to platform-induced accelerations. In such settings, conventional estimators fail to distinguish whether the measured accelerations arise from the quadrotor itself or from the non-inertial platform, leading to drift and control degradation. Unlike conventional approaches that depend heavily on IMU and GNSS, our method relies exclusively on external position measurements combined with a Extended Kalman Filter with Unknown Inputs (EKF-UI) to account for platform motion. The estimator is paired with a cascaded PID controller for full 3D tracking. To isolate estimator performance from localization errors, all tests are conducted using high-precision motion capture systems. Experimental results in a moving-cart testbed validate our approach under both translational in X-axis and Y-axis dissonance. Compared to standard EKF, the proposed method significantly improves stability and trajectory tracking without requiring inertial feedback, enabling practical deployment on moving platforms such as trucks or elevators.",
          "authors": [
            "Riming Xu",
            "Obadah Wali",
            "Yasmine Marani",
            "Eric Feron"
          ],
          "published": "2026-02-03T22:45:34Z",
          "updated": "2026-02-03T22:45:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04057v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04056v1",
          "title": "Modular Safety Guardrails Are Necessary for Foundation-Model-Enabled Robots in the Real World",
          "summary": "The integration of foundation models (FMs) into robotics has accelerated real-world deployment, while introducing new safety challenges arising from open-ended semantic reasoning and embodied physical action. These challenges require safety notions beyond physical constraint satisfaction. In this paper, we characterize FM-enabled robot safety along three dimensions: action safety (physical feasibility and constraint compliance), decision safety (semantic and contextual appropriateness), and human-centered safety (conformance to human intent, norms, and expectations). We argue that existing approaches, including static verification, monolithic controllers, and end-to-end learned policies, are insufficient in settings where tasks, environments, and human expectations are open-ended, long-tailed, and subject to adaptation over time. To address this gap, we propose modular safety guardrails, consisting of monitoring (evaluation) and intervention layers, as an architectural foundation for comprehensive safety across the autonomy stack. Beyond modularity, we highlight possible cross-layer co-design opportunities through representation alignment and conservatism allocation to enable faster, less conservative, and more effective safety enforcement. We call on the community to explore richer guardrail modules and principled co-design strategies to advance safe real-world physical AI deployment.",
          "authors": [
            "Joonkyung Kim",
            "Wenxi Chen",
            "Davood Soleymanzadeh",
            "Yi Ding",
            "Xiangbo Gao",
            "Zhengzhong Tu",
            "Ruqi Zhang",
            "Fan Fei",
            "Sushant Veer",
            "Yiwei Lyu",
            "Minghui Zheng",
            "Yan Gu"
          ],
          "published": "2026-02-03T22:41:51Z",
          "updated": "2026-02-03T22:41:51Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04056v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04050v1",
          "title": "An Anatomy-specific Guidewire Shaping Robot for Improved Vascular Navigation",
          "summary": "Neuroendovascular access often relies on passive microwires that are hand-shaped at the back table and then used to track a microcatheter to the target. Neuroendovascular surgeons determine the shape of the wire by examining the patient pre-operative images and using their experience to identify anatomy specific shapes of the wire that would facilitate reaching the target. This procedure is particularly complex in convoluted anatomical structures and is heavily dependent on the level of expertise of the surgeon. Towards enabling standardized autonomous shaping, we present a bench-top guidewire shaping robot capable of producing navigation-specific desired wire configurations. We present a model that can map the desired wire shape into robot actions, calibrated using experimental data. We show that the robot can produce clinically common tip geometries (C, S, Angled, Hook) and validate them with respect to the model-predicted shapes in 2D. Our model predicts the shape with a Root Mean Square (RMS) error of 0.56mm across all shapes when compared to the experimental results. We also demonstrate 3D tip shaping capabilities and the ability to traverse complex endoluminal navigation from the petrous Internal Carotid Artery (ICA) to the Posterior Communicating Artery (PComm).",
          "authors": [
            "Aabha Tamhankar",
            "Jay Patil",
            "Giovanni Pittiglio"
          ],
          "published": "2026-02-03T22:36:16Z",
          "updated": "2026-02-03T22:36:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04050v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04037v1",
          "title": "DADP: Domain Adaptive Diffusion Policy",
          "summary": "Learning domain adaptive policies that can generalize to unseen transition dynamics, remains a fundamental challenge in learning-based control. Substantial progress has been made through domain representation learning to capture domain-specific information, thus enabling domain-aware decision making. We analyze the process of learning domain representations through dynamical prediction and find that selecting contexts adjacent to the current step causes the learned representations to entangle static domain information with varying dynamical properties. Such mixture can confuse the conditioned policy, thereby constraining zero-shot adaptation. To tackle the challenge, we propose DADP (Domain Adaptive Diffusion Policy), which achieves robust adaptation through unsupervised disentanglement and domain-aware diffusion injection. First, we introduce Lagged Context Dynamical Prediction, a strategy that conditions future state estimation on a historical offset context; by increasing this temporal gap, we unsupervisedly disentangle static domain representations by filtering out transient properties. Second, we integrate the learned domain representations directly into the generative process by biasing the prior distribution and reformulating the diffusion target. Extensive experiments on challenging benchmarks across locomotion and manipulation demonstrate the superior performance, and the generalizability of DADP over prior methods. More visualization results are available on the https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.",
          "authors": [
            "Pengcheng Wang",
            "Qinghang Liu",
            "Haotian Lin",
            "Yiheng Li",
            "Guojian Zhan",
            "Masayoshi Tomizuka",
            "Yixiao Wang"
          ],
          "published": "2026-02-03T22:04:46Z",
          "updated": "2026-02-03T22:04:46Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04037v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.04012v1",
          "title": "FDA Flocking: Future Direction-Aware Flocking via Velocity Prediction",
          "summary": "Understanding self-organization in natural collectives such as bird flocks inspires swarm robotics, yet most flocking models remain reactive, overlooking anticipatory cues that enhance coordination. Motivated by avian postural and wingbeat signals, as well as multirotor attitude tilts that precede directional changes, this work introduces a principled, bio-inspired anticipatory augmentation of reactive flocking termed Future Direction-Aware (FDA) flocking. In the proposed framework, agents blend reactive alignment with a predictive term based on short-term estimates of neighbors' future velocities, regulated by a tunable blending parameter that interpolates between reactive and anticipatory behaviors. This predictive structure enhances velocity consensus and cohesion-separation balance while mitigating the adverse effects of sensing and communication delays and measurement noise that destabilize reactive baselines. Simulation results demonstrate that FDA achieves faster and higher alignment, enhanced translational displacement of the flock, and improved robustness to delays and noise compared to a purely reactive model. Future work will investigate adaptive blending strategies, weighted prediction schemes, and experimental validation on multirotor drone swarms.",
          "authors": [
            "Hossein B. Jond",
            "Martin Saska"
          ],
          "published": "2026-02-03T21:00:33Z",
          "updated": "2026-02-03T21:00:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.04012v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03987v1",
          "title": "Towards X-embodiment safety: A control theory perspective on transferring safety certificates across dynamical systems",
          "summary": "Control barrier functions (CBFs) provide a powerful tool for enforcing safety constraints in control systems, but their direct application to complex, high-dimensional dynamics is often challenging. In many settings, safety certificates are more naturally designed for simplified or alternative system models that do not exactly match the dynamics of interest. This paper addresses the problem of transferring safety guarantees between dynamical systems with mismatched dynamics. We propose a transferred control barrier function (tCBF) framework that enables safety constraints defined on one system to be systematically enforced on another system using a simulation function and an explicit margin term. The resulting transferred barrier accounts for model mismatch and induces a safety condition that can be enforced on the target system via a quadratic-program-based safety filter. The proposed approach is general and does not require the two systems to share the same state dimension or dynamics. We demonstrate the effectiveness of the framework on a quadrotor navigation task with the transferred barrier ensuring collision avoidance for the target system, while remaining minimally invasive to a nominal controller. These results highlight the potential of transferred control barrier functions as a general mechanism for enforcing safety across heterogeneous dynamical systems.",
          "authors": [
            "Nikolaos Bousias",
            "George Pappas"
          ],
          "published": "2026-02-03T20:20:08Z",
          "updated": "2026-02-03T20:20:08Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03987v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03986v1",
          "title": "eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models",
          "summary": "We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.",
          "authors": [
            "Nikolaos Bousias",
            "Lars Lindemann",
            "George Pappas"
          ],
          "published": "2026-02-03T20:18:59Z",
          "updated": "2026-02-03T20:18:59Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03986v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03983v2",
          "title": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement",
          "summary": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.",
          "authors": [
            "Weikang Qiu",
            "Tinglin Huang",
            "Rex Ying"
          ],
          "published": "2026-02-03T20:17:47Z",
          "updated": "2026-02-14T03:09:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03983v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03973v1",
          "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
          "summary": "Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/",
          "authors": [
            "Shuo Liu",
            "Ishneet Sukhvinder Singh",
            "Yiqing Xu",
            "Jiafei Duan",
            "Ranjay Krishna"
          ],
          "published": "2026-02-03T19:50:16Z",
          "updated": "2026-02-03T19:50:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03973v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03920v1",
          "title": "How Users Understand Robot Foundation Model Performance through Task Success Rates and Beyond",
          "summary": "Robot Foundation Models (RFMs) represent a promising approach to developing general-purpose home robots. Given the broad capabilities of RFMs, users will inevitably ask an RFM-based robot to perform tasks that the RFM was not trained or evaluated on. In these cases, it is crucial that users understand the risks associated with attempting novel tasks due to the relatively high cost of failure. Furthermore, an informed user who understands an RFM's capabilities will know what situations and tasks the robot can handle. In this paper, we study how non-roboticists interpret performance information from RFM evaluations. These evaluations typically report task success rate (TSR) as the primary performance metric. While TSR is intuitive to experts, it is necessary to validate whether novices also use this information as intended. Toward this end, we conducted a study in which users saw real evaluation data, including TSR, failure case descriptions, and videos from multiple published RFM research projects. The results highlight that non-experts not only use TSR in a manner consistent with expert expectations but also highly value other information types, such as failure cases that are not often reported in RFM evaluations. Furthermore, we find that users want access to both real data from previous evaluations of the RFM and estimates from the robot about how well it will do on a novel task.",
          "authors": [
            "Isaac Sheidlower",
            "Jindan Huang",
            "James Staley",
            "Bingyu Wu",
            "Qicong Chen",
            "Reuben Aronson",
            "Elaine Short"
          ],
          "published": "2026-02-03T18:36:45Z",
          "updated": "2026-02-03T18:36:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03920v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03799v1",
          "title": "Conformal Reachability for Safe Control in Unknown Environments",
          "summary": "Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.",
          "authors": [
            "Xinhang Ma",
            "Junlin Wu",
            "Yiannis Kantaros",
            "Yevgeniy Vorobeychik"
          ],
          "published": "2026-02-03T18:01:38Z",
          "updated": "2026-02-03T18:01:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03799v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03793v1",
          "title": "BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks",
          "summary": "Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .",
          "authors": [
            "Yixiang Chen",
            "Peiyan Li",
            "Jiabing Yang",
            "Keji He",
            "Xiangnan Wu",
            "Yuan Xu",
            "Kai Wang",
            "Jing Liu",
            "Nianfeng Liu",
            "Yan Huang",
            "Liang Wang"
          ],
          "published": "2026-02-03T17:56:28Z",
          "updated": "2026-02-03T17:56:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03793v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03782v1",
          "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
          "summary": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.",
          "authors": [
            "Yuhao Xu",
            "Yantai Yang",
            "Zhenyang Fan",
            "Yufan Liu",
            "Yuming Li",
            "Bing Li",
            "Zhipeng Zhang"
          ],
          "published": "2026-02-03T17:43:45Z",
          "updated": "2026-02-03T17:43:45Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03782v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03781v1",
          "title": "A Scene Graph Backed Approach to Open Set Semantic Mapping",
          "summary": "While Open Set Semantic Mapping and 3D Semantic Scene Graphs (3DSSGs) are established paradigms in robotic perception, deploying them effectively to support high-level reasoning in large-scale, real-world environments remains a significant challenge. Most existing approaches decouple perception from representation, treating the scene graph as a derivative layer generated post hoc. This limits both consistency and scalability. In contrast, we propose a mapping architecture where the 3DSSG serves as the foundational backend, acting as the primary knowledge representation for the entire mapping process. Our approach leverages prior work on incremental scene graph prediction to infer and update the graph structure in real-time as the environment is explored. This ensures that the map remains topologically consistent and computationally efficient, even during extended operations in large-scale settings. By maintaining an explicit, spatially grounded representation that supports both flat and hierarchical topologies, we bridge the gap between sub-symbolic raw sensor data and high-level symbolic reasoning. Consequently, this provides a stable, verifiable structure that knowledge-driven frameworks, ranging from knowledge graphs and ontologies to Large Language Models (LLMs), can directly exploit, enabling agents to operate with enhanced interpretability, trustworthiness, and alignment to human concepts.",
          "authors": [
            "Martin Günther",
            "Felix Igelbrink",
            "Oscar Lima",
            "Lennart Niecksch",
            "Marian Renz",
            "Martin Atzmueller"
          ],
          "published": "2026-02-03T17:41:51Z",
          "updated": "2026-02-03T17:41:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03781v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03691v1",
          "title": "Input-to-State Safe Backstepping: Robust Safety-Critical Control with Unmatched Uncertainties",
          "summary": "Guaranteeing safety in the presence of unmatched disturbances -- uncertainties that cannot be directly canceled by the control input -- remains a key challenge in nonlinear control. This paper presents a constructive approach to safety-critical control of nonlinear systems with unmatched disturbances. We first present a generalization of the input-to-state safety (ISSf) framework for systems with these uncertainties using the recently developed notion of an Optimal Decay CBF, which provides more flexibility for satisfying the associated Lyapunov-like conditions for safety. From there, we outline a procedure for constructing ISSf-CBFs for two relevant classes of systems with unmatched uncertainties: i) strict-feedback systems; ii) dual-relative-degree systems, which are similar to differentially flat systems. Our theoretical results are illustrated via numerical simulations of an inverted pendulum and planar quadrotor.",
          "authors": [
            "Max H. Cohen",
            "Pio Ong",
            "Aaron D. Ames"
          ],
          "published": "2026-02-03T16:09:36Z",
          "updated": "2026-02-03T16:09:36Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03691v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03674v1",
          "title": "When Should Agents Coordinate in Differentiable Sequential Decision Problems?",
          "summary": "Multi-robot teams must coordinate to operate effectively. When a team operates in an uncoordinated manner, and agents choose actions that are only individually optimal, the team's outcome can suffer. However, in many domains, coordination requires costly communication. We explore the value of coordination in a broad class of differentiable motion-planning problems. In particular, we model coordinated behavior as a spectrum: at one extreme, agents jointly optimize a common team objective, and at the other, agents make unilaterally optimal decisions given their individual decision variables, i.e., they operate at Nash equilibria. We then demonstrate that reasoning about coordination in differentiable motion-planning problems reduces to reasoning about the second-order properties of agents' objectives, and we provide algorithms that use this second-order reasoning to determine at which times a team of agents should coordinate.",
          "authors": [
            "Caleb Probine",
            "Su Ann Low",
            "David Fridovich-Keil",
            "Ufuk Topcu"
          ],
          "published": "2026-02-03T15:55:16Z",
          "updated": "2026-02-03T15:55:16Z",
          "primary_category": "cs.MA",
          "categories": [
            "cs.MA",
            "cs.GT",
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03674v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03668v1",
          "title": "MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction",
          "summary": "Learning \\emph{latent actions} from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining. To make VLA pretraining effective, latent actions should contain information about the underlying agent's actions despite the absence of ground-truth labels. We propose \\textbf{M}ulti-\\textbf{V}iew\\textbf{P}oint \\textbf{L}atent \\textbf{A}ction \\textbf{M}odel (\\textbf{MVP-LAM}), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos. MVP-LAM trains latent actions with a \\emph{cross-viewpoint reconstruction} objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues. On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation. Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.",
          "authors": [
            "Jung Min Lee",
            "Dohyeok Lee",
            "Seokhun Ju",
            "Taehyun Cho",
            "Jin Woo Koo",
            "Li Zhao",
            "Sangwoo Hong",
            "Jungwoo Lee"
          ],
          "published": "2026-02-03T15:51:25Z",
          "updated": "2026-02-03T15:51:25Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03668v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03639v1",
          "title": "Variance-Reduced Model Predictive Path Integral via Quadratic Model Approximation",
          "summary": "Sampling-based controllers, such as Model Predictive Path Integral (MPPI) methods, offer substantial flexibility but often suffer from high variance and low sample efficiency. To address these challenges, we introduce a hybrid variance-reduced MPPI framework that integrates a prior model into the sampling process. Our key insight is to decompose the objective function into a known approximate model and a residual term. Since the residual captures only the discrepancy between the model and the objective, it typically exhibits a smaller magnitude and lower variance than the original objective. Although this principle applies to general modeling choices, we demonstrate that adopting a quadratic approximation enables the derivation of a closed-form, model-guided prior that effectively concentrates samples in informative regions. Crucially, the framework is agnostic to the source of geometric information, allowing the quadratic model to be constructed from exact derivatives, structural approximations (e.g., Gauss- or Quasi-Newton), or gradient-free randomized smoothing. We validate the approach on standard optimization benchmarks, a nonlinear, underactuated cart-pole control task, and a contact-rich manipulation problem with non-smooth dynamics. Across these domains, we achieve faster convergence and superior performance in low-sample regimes compared to standard MPPI. These results suggest that the method can make sample-based control strategies more practical in scenarios where obtaining samples is expensive or limited.",
          "authors": [
            "Fabian Schramm",
            "Franki Nguimatsia Tiofack",
            "Nicolas Perrin-Gilbert",
            "Marc Toussaint",
            "Justin Carpentier"
          ],
          "published": "2026-02-03T15:23:17Z",
          "updated": "2026-02-03T15:23:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03639v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03623v1",
          "title": "Self-supervised Physics-Informed Manipulation of Deformable Linear Objects with Non-negligible Dynamics",
          "summary": "We address dynamic manipulation of deformable linear objects by presenting SPiD, a physics-informed self-supervised learning framework that couples an accurate deformable object model with an augmented self-supervised training strategy. On the modeling side, we extend a mass-spring model to more accurately capture object dynamics while remaining lightweight enough for high-throughput rollouts during self-supervised learning. On the learning side, we train a neural controller using a task-oriented cost, enabling end-to-end optimization through interaction with the differentiable object model. In addition, we propose a self-supervised DAgger variant that detects distribution shift during deployment and performs offline self-correction to further enhance robustness without expert supervision. We evaluate our method primarily on the rope stabilization task, where a robot must bring a swinging rope to rest as quickly and smoothly as possible. Extensive experiments in both simulation and the real world demonstrate that the proposed controller achieves fast and smooth rope stabilization, generalizing across unseen initial states, rope lengths, masses, non-uniform mass distributions, and external disturbances. Additionally, we develop an affordable markerless rope perception method and demonstrate that our controller maintains performance with noisy and low-frequency state updates. Furthermore, we demonstrate the generality of the framework by extending it to the rope trajectory tracking task. Overall, SPiD offers a data-efficient, robust, and physically grounded framework for dynamic manipulation of deformable linear objects, featuring strong sim-to-real generalization.",
          "authors": [
            "Youyuan Long",
            "Gokhan Solak",
            "Sara Zeynalpour",
            "Heng Zhang",
            "Arash Ajoudani"
          ],
          "published": "2026-02-03T15:14:09Z",
          "updated": "2026-02-03T15:14:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03623v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03603v1",
          "title": "Human-in-the-Loop Failure Recovery with Adaptive Task Allocation",
          "summary": "Since the recent Covid-19 pandemic, mobile manipulators and humanoid assistive robots with higher levels of autonomy have increasingly been adopted for patient care and living assistance. Despite advancements in autonomy, these robots often struggle to perform reliably in dynamic and unstructured environments and require human intervention to recover from failures. Effective human-robot collaboration is essential to enable robots to receive assistance from the most competent operator, in order to reduce their workload and minimize disruptions in task execution. In this paper, we propose an adaptive method for allocating robotic failures to human operators (ARFA). Our proposed approach models the capabilities of human operators, and continuously updates these beliefs based on their actual performance for failure recovery. For every failure to be resolved, a reward function calculates expected outcomes based on operator capabilities and historical data, task urgency, and current workload distribution. The failure is then assigned to the operator with the highest expected reward. Our simulations and user studies show that ARFA outperforms random allocation, significantly reducing robot idle time, improving overall system performance, and leading to a more distributed workload among operators.",
          "authors": [
            "Lorena Maria Genua",
            "Nikita Boguslavskii",
            "Zhi Li"
          ],
          "published": "2026-02-03T14:55:48Z",
          "updated": "2026-02-03T14:55:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03603v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03908v1",
          "title": "Beyond the Vehicle: Cooperative Localization by Fusing Point Clouds for GPS-Challenged Urban Scenarios",
          "summary": "Accurate vehicle localization is a critical challenge in urban environments where GPS signals are often unreliable. This paper presents a cooperative multi-sensor and multi-modal localization approach to address this issue by fusing data from vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Our approach integrates cooperative data with a point cloud registration-based simultaneous localization and mapping (SLAM) algorithm. The system processes point clouds generated from diverse sensor modalities, including vehicle-mounted LiDAR and stereo cameras, as well as sensors deployed at intersections. By leveraging shared data from infrastructure, our method significantly improves localization accuracy and robustness in complex, GPS-noisy urban scenarios.",
          "authors": [
            "Kuo-Yi Chao",
            "Ralph Rasshofer",
            "Alois Christian Knoll"
          ],
          "published": "2026-02-03T14:47:31Z",
          "updated": "2026-02-03T14:47:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03908v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03571v2",
          "title": "Multi-Player, Multi-Strategy Quantum Game Model for Interaction-Aware Decision-Making in Automated Driving",
          "summary": "Although significant progress has been made in decision-making for automated driving, challenges remain for deployment in the real world. One challenge lies in addressing interaction-awareness. Most existing approaches oversimplify interactions between the ego vehicle and surrounding agents, and often neglect interactions among the agents themselves. A common solution is to model these interactions using classical game theory. However, its formulation assumes rational players, whereas human behavior is frequently uncertain or irrational. To address these challenges, we propose the Quantum Game Decision-Making (QGDM) model, a novel framework that combines classical game theory with quantum mechanics principles (such as superposition, entanglement, and interference) to tackle multi-player, multi-strategy decision-making problems. To the best of our knowledge, this is one of the first studies to apply quantum game theory to decision-making for automated driving. QGDM runs in real time on a standard computer, without requiring quantum hardware. We evaluate QGDM in simulation across various scenarios, including roundabouts, merging, and highways, and compare its performance with multiple baseline methods. Results show that QGDM significantly improves success rates and reduces collision rates compared to classical approaches, particularly in scenarios with high interaction.",
          "authors": [
            "Karim Essalmi",
            "Fernando Garrido",
            "Fawzi Nashashibi"
          ],
          "published": "2026-02-03T14:14:11Z",
          "updated": "2026-02-09T16:17:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03571v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03550v1",
          "title": "Formal Evidence Generation for Assurance Cases for Robotic Software Models",
          "summary": "Robotics and Autonomous Systems are increasingly deployed in safety-critical domains, so that demonstrating their safety is essential. Assurance Cases (ACs) provide structured arguments supported by evidence, but generating and maintaining this evidence is labour-intensive, error-prone, and difficult to keep consistent as systems evolve. We present a model-based approach to systematically generating AC evidence by embedding formal verification into the assurance workflow. The approach addresses three challenges: systematically deriving formal assertions from natural language requirements using templates, orchestrating multiple formal verification tools to handle diverse property types, and integrating formal evidence production into the workflow. Leveraging RoboChart, a domain-specific modelling language with formal semantics, we combine model checking and theorem proving in our approach. Structured requirements are automatically transformed into formal assertions using predefined templates, and verification results are automatically integrated as evidence. Case studies demonstrate the effectiveness of our approach.",
          "authors": [
            "Fang Yan",
            "Simon Foster",
            "Ana Cavalcanti",
            "Ibrahim Habli",
            "James Baxter"
          ],
          "published": "2026-02-03T14:01:30Z",
          "updated": "2026-02-03T14:01:30Z",
          "primary_category": "cs.SE",
          "categories": [
            "cs.SE",
            "cs.FL",
            "cs.LO",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03550v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03547v1",
          "title": "AffordanceGrasp-R1:Leveraging Reasoning-Based Affordance Segmentation with Reinforcement Learning for Robotic Grasping",
          "summary": "We introduce AffordanceGrasp-R1, a reasoning-driven affordance segmentation framework for robotic grasping that combines a chain-of-thought (CoT) cold-start strategy with reinforcement learning to enhance deduction and spatial grounding. In addition, we redesign the grasping pipeline to be more context-aware by generating grasp candidates from the global scene point cloud and subsequently filtering them using instruction-conditioned affordance masks. Extensive experiments demonstrate that AffordanceGrasp-R1 consistently outperforms state-of-the-art (SOTA) methods on benchmark datasets, and real-world robotic grasping evaluations further validate its robustness and generalization under complex language-conditioned manipulation scenarios.",
          "authors": [
            "Dingyi Zhou",
            "Mu He",
            "Zhuowei Fang",
            "Xiangtong Yao",
            "Yinlong Liu",
            "Alois Knoll",
            "Hu Cao"
          ],
          "published": "2026-02-03T14:00:56Z",
          "updated": "2026-02-03T14:00:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03547v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03544v1",
          "title": "Investigating the Influence of Spatial Ability in Augmented Reality-assisted Robot Programming",
          "summary": "Augmented Reality (AR) offers promising opportunities to enhance learning, but its mechanisms and effects are not yet fully understood. As learning becomes increasingly personalized, considering individual learner characteristics becomes more important. This study investigates the moderating effect of spatial ability on learning experience with AR in the context of robot programming. A between-subjects experiment ($N=71$) compared conventional robot programming to an AR-assisted approach using a head-mounted display. Participants' spatial ability was assessed using the Mental Rotation Test. The learning experience was measured through the System Usability Scale (SUS) and cognitive load. The results indicate that AR support does not significantly improve the learning experience compared to the conventional approach. However, AR appears to have a compensatory effect on the influence of spatial ability. In the control group, spatial ability was significantly positively associated with SUS scores and negatively associated with extraneous cognitive load, indicating that higher spatial ability predicts a better learning experience. In the AR condition, these relationships were not observable, suggesting that AR mitigated the disadvantage typically experienced by learners with lower spatial abilities. These findings suggest that AR can serve a compensatory function by reducing the influence of learner characteristics. Future research should further explore this compensatory role of AR to guide the design of personalized learning environments that address diverse learner needs and reduce barriers for learners with varying cognitive profiles.",
          "authors": [
            "Nicolas Leins",
            "Jana Gonnermann-Müller",
            "Malte Teichmann",
            "Sebastian Pokutta"
          ],
          "published": "2026-02-03T13:58:54Z",
          "updated": "2026-02-03T13:58:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03544v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03511v1",
          "title": "CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains",
          "summary": "Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.",
          "authors": [
            "Qixin Zeng",
            "Hongyin Zhang",
            "Shangke Lyu",
            "Junxi Jin",
            "Donglin Wang",
            "Chao Huang"
          ],
          "published": "2026-02-03T13:30:18Z",
          "updated": "2026-02-03T13:30:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03511v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03447v1",
          "title": "HetroD: A High-Fidelity Drone Dataset and Benchmark for Autonomous Driving in Heterogeneous Traffic",
          "summary": "We present HetroD, a dataset and benchmark for developing autonomous driving systems in heterogeneous environments. HetroD targets the critical challenge of navi- gating real-world heterogeneous traffic dominated by vulner- able road users (VRUs), including pedestrians, cyclists, and motorcyclists that interact with vehicles. These mixed agent types exhibit complex behaviors such as hook turns, lane splitting, and informal right-of-way negotiation. Such behaviors pose significant challenges for autonomous vehicles but remain underrepresented in existing datasets focused on structured, lane-disciplined traffic. To bridge the gap, we collect a large- scale drone-based dataset to provide a holistic observation of traffic scenes with centimeter-accurate annotations, HD maps, and traffic signal states. We further develop a modular toolkit for extracting per-agent scenarios to support downstream task development. In total, the dataset comprises over 65.4k high- fidelity agent trajectories, 70% of which are from VRUs. HetroD supports modeling of VRU behaviors in dense, het- erogeneous traffic and provides standardized benchmarks for forecasting, planning, and simulation tasks. Evaluation results reveal that state-of-the-art prediction and planning models struggle with the challenges presented by our dataset: they fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and exhibit limited performance in dense and multi-agent scenarios, highlighting the need for more robust approaches to heterogeneous traffic. See our project page for more examples: https://hetroddata.github.io/HetroD/",
          "authors": [
            "Yu-Hsiang Chen",
            "Wei-Jer Chang",
            "Christian Kotulla",
            "Thomas Keutgens",
            "Steffen Runde",
            "Tobias Moers",
            "Christoph Klas",
            "Wei Zhan",
            "Masayoshi Tomizuka",
            "Yi-Ting Chen"
          ],
          "published": "2026-02-03T12:12:47Z",
          "updated": "2026-02-03T12:12:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03447v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03445v1",
          "title": "CRL-VLA: Continual Vision-Language-Action Learning",
          "summary": "Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.",
          "authors": [
            "Qixin Zeng",
            "Shuo Zhang",
            "Hongyin Zhang",
            "Renjie Wang",
            "Han Zhao",
            "Libang Zhao",
            "Runze Li",
            "Donglin Wang",
            "Chao Huang"
          ],
          "published": "2026-02-03T12:09:53Z",
          "updated": "2026-02-03T12:09:53Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03445v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03435v1",
          "title": "Model-based Optimal Control for Rigid-Soft Underactuated Systems",
          "summary": "Continuum soft robots are inherently underactuated and subject to intrinsic input constraints, making dynamic control particularly challenging, especially in hybrid rigid-soft robots. While most existing methods focus on quasi-static behaviors, dynamic tasks such as swing-up require accurate exploitation of continuum dynamics. This has led to studies on simple low-order template systems that often fail to capture the complexity of real continuum deformations. Model-based optimal control offers a systematic solution; however, its application to rigid-soft robots is often limited by the computational cost and inaccuracy of numerical differentiation for high-dimensional models. Building on recent advances in the Geometric Variable Strain model that enable analytical derivatives, this work investigates three optimal control strategies for underactuated soft systems-Direct Collocation, Differential Dynamic Programming, and Nonlinear Model Predictive Control-to perform dynamic swing-up tasks. To address stiff continuum dynamics and constrained actuation, implicit integration schemes and warm-start strategies are employed to improve numerical robustness and computational efficiency. The methods are evaluated in simulation on three Rigid-Soft and high-order soft benchmark systems-the Soft Cart-Pole, the Soft Pendubot, and the Soft Furuta Pendulum- highlighting their performance and computational trade-offs.",
          "authors": [
            "Daniele Caradonna",
            "Nikhil Nair",
            "Anup Teejo Mathew",
            "Daniel Feliu Talegón",
            "Imran Afgan",
            "Egidio Falotico",
            "Cosimo Della Santina",
            "Federico Renda"
          ],
          "published": "2026-02-03T11:59:36Z",
          "updated": "2026-02-03T11:59:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03435v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03430v2",
          "title": "ProAct: A Benchmark and Multimodal Framework for Structure-Aware Proactive Response",
          "summary": "While passive agents merely follow instructions, proactive agents align with higher-level objectives, such as assistance and safety by continuously monitoring the environment to determine when and how to act. However, developing proactive agents is hindered by the lack of specialized resources. To address this, we introduce ProAct-75, a benchmark designed to train and evaluate proactive agents across diverse domains, including assistance, maintenance, and safety monitoring. Spanning 75 tasks, our dataset features 91,581 step-level annotations enriched with explicit task graphs. These graphs encode step dependencies and parallel execution possibilities, providing the structural grounding necessary for complex decision-making. Building on this benchmark, we propose ProAct-Helper, a reference baseline powered by a Multimodal Large Language Model (MLLM) that grounds decision-making in state detection, and leveraging task graphs to enable entropy-driven heuristic search for action selection, allowing agents to execute parallel threads independently rather than mirroring the human's next step. Extensive experiments demonstrate that ProAct-Helper outperforms strong closed-source models, improving trigger detection mF1 by 6.21%, saving 0.25 more steps in online one-step decision, and increasing the rate of parallel actions by 15.58%.",
          "authors": [
            "Xiaomeng Zhu",
            "Fengming Zhu",
            "Weijie Zhou",
            "Ye Tian",
            "Zhenlin Hu",
            "Yufei Huang",
            "Yuchun Guo",
            "Xinyu Wu",
            "Zhengyou Zhang",
            "Fangzhen Lin",
            "Xuantang Xiong"
          ],
          "published": "2026-02-03T11:52:19Z",
          "updated": "2026-02-04T03:41:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03430v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03418v2",
          "title": "Learning-based Initialization of Trajectory Optimization for Path-following Problems of Redundant Manipulators",
          "summary": "Trajectory optimization (TO) is an efficient tool to generate a redundant manipulator's joint trajectory following a 6-dimensional Cartesian path. The optimization performance largely depends on the quality of initial trajectories. However, the selection of a high-quality initial trajectory is non-trivial and requires a considerable time budget due to the extremely large space of the solution trajectories and the lack of prior knowledge about task constraints in configuration space. To alleviate the issue, we present a learning-based initial trajectory generation method that generates high-quality initial trajectories in a short time budget by adopting example-guided reinforcement learning. In addition, we suggest a null-space projected imitation reward to consider null-space constraints by efficiently learning kinematically feasible motion captured in expert demonstrations. Our statistical evaluation in simulation shows the improved optimality, efficiency, and applicability of TO when we plug in our method's output, compared with three other baselines. We also show the performance improvement and feasibility via real-world experiments with a seven-degree-of-freedom manipulator.",
          "authors": [
            "Minsung Yoon",
            "Mincheul Kang",
            "Daehyung Park",
            "Sung-Eui Yoon"
          ],
          "published": "2026-02-03T11:44:20Z",
          "updated": "2026-02-09T00:29:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03418v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03406v1",
          "title": "Deep-Learning-Based Control of a Decoupled Two-Segment Continuum Robot for Endoscopic Submucosal Dissection",
          "summary": "Manual endoscopic submucosal dissection (ESD) is technically demanding, and existing single-segment robotic tools offer limited dexterity. These limitations motivate the development of more advanced solutions. To address this, DESectBot, a novel dual segment continuum robot with a decoupled structure and integrated surgical forceps, enabling 6 degrees of freedom (DoFs) tip dexterity for improved lesion targeting in ESD, was developed in this work. Deep learning controllers based on gated recurrent units (GRUs) for simultaneous tip position and orientation control, effectively handling the nonlinear coupling between continuum segments, were proposed. The GRU controller was benchmarked against Jacobian based inverse kinematics, model predictive control (MPC), a feedforward neural network (FNN), and a long short-term memory (LSTM) network. In nested-rectangle and Lissajous trajectory tracking tasks, the GRU achieved the lowest position/orientation RMSEs: 1.11 mm/ 4.62° and 0.81 mm/ 2.59°, respectively. For orientation control at a fixed position (four target poses), the GRU attained a mean RMSE of 0.14 mm and 0.72°, outperforming all alternatives. In a peg transfer task, the GRU achieved a 100% success rate (120 success/120 attempts) with an average transfer time of 11.8s, the STD significantly outperforms novice-controlled systems. Additionally, an ex vivo ESD demonstration grasping, elevating, and resecting tissue as the scalpel completed the cut confirmed that DESectBot provides sufficient stiffness to divide thick gastric mucosa and an operative workspace adequate for large lesions.These results confirm that GRU-based control significantly enhances precision, reliability, and usability in ESD surgical training scenarios.",
          "authors": [
            "Yuancheng Shao",
            "Yao Zhang",
            "Jia Gu",
            "Zixi Chen",
            "Di Wu",
            "Yuqiao Chen",
            "Bo Lu",
            "Wenjie Liu",
            "Cesare Stefanini",
            "Peng Qi"
          ],
          "published": "2026-02-03T11:35:02Z",
          "updated": "2026-02-03T11:35:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03406v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03397v2",
          "title": "Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms",
          "summary": "Quadruped robots face limitations in long-range navigation efficiency due to their reliance on legs. To ameliorate the limitations, we introduce a Reinforcement Learning-based Active Transporter Riding method (\\textit{RL-ATR}), inspired by humans' utilization of personal transporters, including Segways. The \\textit{RL-ATR} features a transporter riding policy and two state estimators. The policy devises adequate maneuvering strategies according to transporter-specific control dynamics, while the estimators resolve sensor ambiguities in non-inertial frames by inferring unobservable robot and transporter states. Comprehensive evaluations in simulation validate proficient command tracking abilities across various transporter-robot models and reduced energy consumption compared to legged locomotion. Moreover, we conduct ablation studies to quantify individual component contributions within the \\textit{RL-ATR}. This riding ability could broaden the locomotion modalities of quadruped robots, potentially expanding the operational range and efficiency.",
          "authors": [
            "Minsung Yoon",
            "Sung-Eui Yoon"
          ],
          "published": "2026-02-03T11:17:42Z",
          "updated": "2026-02-09T00:32:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03397v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03376v1",
          "title": "PlanTRansformer: Unified Prediction and Planning with Goal-conditioned Transformer",
          "summary": "Trajectory prediction and planning are fundamental yet disconnected components in autonomous driving. Prediction models forecast surrounding agent motion under unknown intentions, producing multimodal distributions, while planning assumes known ego objectives and generates deterministic trajectories. This mismatch creates a critical bottleneck: prediction lacks supervision for agent intentions, while planning requires this information. Existing prediction models, despite strong benchmarking performance, often remain disconnected from planning constraints such as collision avoidance and dynamic feasibility. We introduce Plan TRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. A teacher-student training strategy progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable. PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to the baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models. Project Website: https://github.com/SelzerConst/PlanTRansformer",
          "authors": [
            "Constantin Selzer",
            "Fabina B. Flohr"
          ],
          "published": "2026-02-03T10:55:05Z",
          "updated": "2026-02-03T10:55:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03376v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03367v2",
          "title": "Learning-based Adaptive Control of Quadruped Robots for Active Stabilization on Moving Platforms",
          "summary": "A quadruped robot faces balancing challenges on a six-degrees-of-freedom moving platform, like subways, buses, airplanes, and yachts, due to independent platform motions and resultant diverse inertia forces on the robot. To alleviate these challenges, we present the Learning-based Active Stabilization on Moving Platforms (\\textit{LAS-MP}), featuring a self-balancing policy and system state estimators. The policy adaptively adjusts the robot's posture in response to the platform's motion. The estimators infer robot and platform states based on proprioceptive sensor data. For a systematic training scheme across various platform motions, we introduce platform trajectory generation and scheduling methods. Our evaluation demonstrates superior balancing performance across multiple metrics compared to three baselines. Furthermore, we conduct a detailed analysis of the \\textit{LAS-MP}, including ablation studies and evaluation of the estimators, to validate the effectiveness of each component.",
          "authors": [
            "Minsung Yoon",
            "Heechan Shin",
            "Jeil Jeong",
            "Sung-Eui Yoon"
          ],
          "published": "2026-02-03T10:37:40Z",
          "updated": "2026-02-09T00:36:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03367v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03350v1",
          "title": "Manipulation via Force Distribution at Contact",
          "summary": "Efficient and robust trajectories play a crucial role in contact-rich manipulation, which demands accurate mod- eling of object-robot interactions. Many existing approaches rely on point contact models due to their computational effi- ciency. Simple contact models are computationally efficient but inherently limited for achieving human-like, contact-rich ma- nipulation, as they fail to capture key frictional dynamics and torque generation observed in human manipulation. This study introduces a Force-Distributed Line Contact (FDLC) model in contact-rich manipulation and compares it against conventional point contact models. A bi-level optimization framework is constructed, in which the lower-level solves an optimization problem for contact force computation, and the upper-level optimization applies iLQR for trajectory optimization. Through this framework, the limitations of point contact are demon- strated, and the benefits of the FDLC in generating efficient and robust trajectories are established. The effectiveness of the proposed approach is validated by a box rotating task, demonstrating that FDLC enables trajectories generated via non-uniform force distributions along the contact line, while requiring lower control effort and less motion of the robot.",
          "authors": [
            "Haegu Lee",
            "Yitaek Kim",
            "Casper Hewson Rask",
            "Christoffer Sloth"
          ],
          "published": "2026-02-03T10:22:36Z",
          "updated": "2026-02-03T10:22:36Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03350v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03310v1",
          "title": "RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization",
          "summary": "Vision-Language-Action (VLA) models hold promise for generalist robotics but currently struggle with data scarcity, architectural inefficiencies, and the inability to generalize across different hardware platforms. We introduce RDT2, a robotic foundation model built upon a 7B parameter VLM designed to enable zero-shot deployment on novel embodiments for open-vocabulary tasks. To achieve this, we collected one of the largest open-source robotic datasets--over 10,000 hours of demonstrations in diverse families--using an enhanced, embodiment-agnostic Universal Manipulation Interface (UMI). Our approach employs a novel three-stage training recipe that aligns discrete linguistic knowledge with continuous control via Residual Vector Quantization (RVQ), flow-matching, and distillation for real-time inference. Consequently, RDT2 becomes one of the first models that simultaneously zero-shot generalizes to unseen objects, scenes, instructions, and even robotic platforms. Besides, it outperforms state-of-the-art baselines in dexterous, long-horizon, and dynamic downstream tasks like playing table tennis. See https://rdt-robotics.github.io/rdt2/ for more information.",
          "authors": [
            "Songming Liu",
            "Bangguo Li",
            "Kai Ma",
            "Lingxuan Wu",
            "Hengkai Tan",
            "Xiao Ouyang",
            "Hang Su",
            "Jun Zhu"
          ],
          "published": "2026-02-03T09:38:23Z",
          "updated": "2026-02-03T09:38:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03310v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03294v1",
          "title": "LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices",
          "summary": "Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source.",
          "authors": [
            "Jonas Kühne",
            "Christian Vogt",
            "Michele Magno",
            "Luca Benini"
          ],
          "published": "2026-02-03T09:20:57Z",
          "updated": "2026-02-03T09:20:57Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO",
            "eess.IV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03294v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03250v1",
          "title": "Collision Detection with Analytical Derivatives of Contact Kinematics",
          "summary": "Differentiable contact kinematics are essential for gradient-based methods in robotics, yet the mapping from robot state to contact distance, location, and normal becomes non-smooth in degenerate configurations of shapes with zero or undefined curvature. We address this inherent limitation by selectively regularizing such geometries into strictly convex implicit representations, restoring uniqueness and smoothness of the contact map. Leveraging this geometric regularization, we develop iDCOL, an implicit differentiable collision detection and contact kinematics framework. iDCOL represents colliding bodies using strictly convex implicit surfaces and computes collision detection and contact kinematics by solving a fixed-size nonlinear system derived from a geometric scaling-based convex optimization formulation. By applying the Implicit Function Theorem to the resulting system residual, we derive analytical derivatives of the contact kinematic quantities. We develop a fast Newton-based solver for iDCOL and provide an open-source C++ implementation of the framework. The robustness of the approach is evaluated through extensive collision simulations and benchmarking, and applicability is demonstrated in gradient-based kinematic path planning and differentiable contact physics, including multi-body rigid collisions and a soft-robot interaction example.",
          "authors": [
            "Anup Teejo Mathew",
            "Anees Peringal",
            "Daniele Caradonna",
            "Frederic Boyer",
            "Federico Renda"
          ],
          "published": "2026-02-03T08:34:31Z",
          "updated": "2026-02-03T08:34:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03250v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03248v1",
          "title": "A thin and soft optical tactile sensor for highly sensitive object perception",
          "summary": "Tactile sensing is crucial in robotics and wearable devices for safe perception and interaction with the environment. Optical tactile sensors have emerged as promising solutions, as they are immune to electromagnetic interference and have high spatial resolution. However, existing optical approaches, particularly vision-based tactile sensors, rely on complex optical assemblies that involve lenses and cameras, resulting in bulky, rigid, and alignment-sensitive designs. In this study, we present a thin, compact, and soft optical tactile sensor featuring an alignment-free configuration. The soft optical sensor operates by capturing deformation-induced changes in speckle patterns generated within a soft silicone material, thereby enabling precise force measurements and texture recognition via machine learning. The experimental results show a root-mean-square error of 40 mN in the force measurement and a classification accuracy of 93.33% over nine classes of textured surfaces, including Mahjong tiles. The proposed speckle-based approach provides a compact, easily fabricated, and mechanically compliant platform that bridges optical sensing with flexible shape-adaptive architectures, thereby demonstrating its potential as a novel tactile-sensing paradigm for soft robotics and wearable haptic interfaces.",
          "authors": [
            "Yanchen Shen",
            "Kohei Tsuji",
            "Haruto Koizumi",
            "Jiseon Hong",
            "Tomoaki Niiyama",
            "Hiroyuki Kuwabara",
            "Hayato Ishida",
            "Jun Hiramitsu",
            "Mitsuhito Mase",
            "Satoshi Sunada"
          ],
          "published": "2026-02-03T08:28:14Z",
          "updated": "2026-02-03T08:28:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "physics.app-ph",
            "physics.optics"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03248v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03229v1",
          "title": "Omnidirectional Solid-State mmWave Radar Perception for UAV Power Line Collision Avoidance",
          "summary": "Detecting and estimating distances to power lines is a challenge for both human UAV pilots and autonomous systems, which increases the risk of unintended collisions. We present a mmWave radar-based perception system that provides spherical sensing coverage around a small UAV for robust power line detection and avoidance. The system integrates multiple compact solid-state mmWave radar modules to synthesize an omnidirectional field of view while remaining lightweight. We characterize the sensing behavior of this omnidirectional radar arrangement in power line environments and develop a robust detection-and-avoidance algorithm tailored to that behavior. Field experiments on real power lines demonstrate reliable detection at ranges up to 10 m, successful avoidance maneuvers at flight speeds upwards of 10 m/s, and detection of wires as thin as 1.2 mm in diameter. These results indicate the approach's suitability as an additional safety layer for both autonomous and manual UAV flight.",
          "authors": [
            "Nicolaj Haarhøj Malle",
            "Emad Ebeid"
          ],
          "published": "2026-02-03T08:02:32Z",
          "updated": "2026-02-03T08:02:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03229v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03209v1",
          "title": "Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements",
          "summary": "Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations. Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets. To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments. A synthetic dataset generation pipeline tailored to field robotics enables the creation of multiple realistic datasets for training purposes. This dataset generation approach utilizes textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis to simulate diverse field robotics scenarios. Our approach achieves an end-to-end latency of 53 ms per frame on a Nvidia Jetson AGX Orin, enabling real-time deployment on embedded platforms. Extensive evaluation demonstrates competitive performance across diverse real-world field robotics scenarios.",
          "authors": [
            "Marco Job",
            "Thomas Stastny",
            "Eleni Kelasidi",
            "Roland Siegwart",
            "Michael Pantic"
          ],
          "published": "2026-02-03T07:24:05Z",
          "updated": "2026-02-03T07:24:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03209v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03205v1",
          "title": "HUSKY: Humanoid Skateboarding System via Physics-Aware Whole-Body Control",
          "summary": "While current humanoid whole-body control frameworks predominantly rely on the static environment assumptions, addressing tasks characterized by high dynamism and complex interactions presents a formidable challenge. In this paper, we address humanoid skateboarding, a highly challenging task requiring stable dynamic maneuvering on an underactuated wheeled platform. This integrated system is governed by non-holonomic constraints and tightly coupled human-object interactions. Successfully executing this task requires simultaneous mastery of hybrid contact dynamics and robust balance control on a mechanically coupled, dynamically unstable skateboard. To overcome the aforementioned challenges, we propose HUSKY, a learning-based framework that integrates humanoid-skateboard system modeling and physics-aware whole-body control. We first model the coupling relationship between board tilt and truck steering angles, enabling a principled analysis of system dynamics. Building upon this, HUSKY leverages Adversarial Motion Priors (AMP) to learn human-like pushing motions and employs a physics-guided, heading-oriented strategy for lean-to-steer behaviors. Moreover, a trajectory-guided mechanism ensures smooth and stable transitions between pushing and steering. Experimental results on the Unitree G1 humanoid platform demonstrate that our framework enables stable and agile maneuvering on skateboards in real-world scenarios. The project page is available on https://husky-humanoid.github.io/.",
          "authors": [
            "Jinrui Han",
            "Dewei Wang",
            "Chenyun Zhang",
            "Xinzhe Liu",
            "Ping Luo",
            "Chenjia Bai",
            "Xuelong Li"
          ],
          "published": "2026-02-03T07:18:01Z",
          "updated": "2026-02-03T07:18:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03205v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03188v1",
          "title": "Hierarchical Proportion Models for Motion Generation via Integration of Motion Primitives",
          "summary": "Imitation learning (IL) enables robots to acquire human-like motion skills from demonstrations, but it still requires extensive high-quality data and retraining to handle complex or long-horizon tasks. To improve data efficiency and adaptability, this study proposes a hierarchical IL framework that integrates motion primitives with proportion-based motion synthesis. The proposed method employs a two-layer architecture, where the upper layer performs long-term planning, while a set of lower-layer models learn individual motion primitives, which are combined according to specific proportions. Three model variants are introduced to explore different trade-offs between learning flexibility, computational cost, and adaptability: a learning-based proportion model, a sampling-based proportion model, and a playback-based proportion model, which differ in how the proportions are determined and whether the upper layer is trainable. Through real-robot pick-and-place experiments, the proposed models successfully generated complex motions not included in the primitive set. The sampling-based and playback-based proportion models achieved more stable and adaptable motion generation than the standard hierarchical model, demonstrating the effectiveness of proportion-based motion integration for practical robot learning.",
          "authors": [
            "Yu-Han Shu",
            "Toshiaki Tsuji",
            "Sho Sakaino"
          ],
          "published": "2026-02-03T06:57:06Z",
          "updated": "2026-02-03T06:57:06Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03188v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03177v1",
          "title": "Estimation of Ground Reaction Forces from Kinematic Data during Locomotion",
          "summary": "Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis.",
          "authors": [
            "Gautami Golani",
            "Dong Anh Khoa To",
            "Ananda Sidarta",
            "Arun-Kumar Kaliya-Perumal",
            "Oliver Roberts",
            "Lek Syn Lim",
            "Jim Patton",
            "Domenico Campolo"
          ],
          "published": "2026-02-03T06:45:14Z",
          "updated": "2026-02-03T06:45:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03177v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03153v1",
          "title": "When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens",
          "summary": "Downstream fine-tuning of vision-language-action (VLA) models enhances robotics, yet exposes the pipeline to backdoor risks. Attackers can pretrain VLAs on poisoned data to implant backdoors that remain stealthy but can trigger harmful behavior during inference. However, existing defenses either lack mechanistic insight into multimodal backdoors or impose prohibitive computational costs via full-model retraining. To this end, we uncover a deep-layer attention grabbing mechanism: backdoors redirect late-stage attention and form compact embedding clusters near the clean manifold. Leveraging this insight, we introduce Bera, a test-time backdoor erasure framework that detects tokens with anomalous attention via latent-space localization, masks suspicious regions using deep-layer cues, and reconstructs a trigger-free image to break the trigger-unsafe-action mapping while restoring correct behavior. Unlike prior defenses, Bera requires neither retraining of VLAs nor any changes to the training pipeline. Extensive experiments across multiple embodied platforms and tasks show that Bera effectively maintains nominal performance, significantly reduces attack success rates, and consistently restores benign behavior from backdoored outputs, thereby offering a robust and practical defense mechanism for securing robotic systems.",
          "authors": [
            "Xuetao Li",
            "Pinhan Fu",
            "Wenke Huang",
            "Nengyuan Pan",
            "Songhua Yang",
            "Kaiyan Zhao",
            "Guancheng Wan",
            "Mengde Li",
            "Jifeng Xuan",
            "Miao Li"
          ],
          "published": "2026-02-03T06:09:43Z",
          "updated": "2026-02-03T06:09:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03153v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03147v1",
          "title": "Multi-function Robotized Surgical Dissector for Endoscopic Pulmonary Thromboendarterectomy: Preclinical Study and Evaluation",
          "summary": "Patients suffering chronic severe pulmonary thromboembolism need Pulmonary Thromboendarterectomy (PTE) to remove the thromb and intima located inside pulmonary artery (PA). During the surgery, a surgeon holds tweezers and a dissector to delicately strip the blockage, but available tools for this surgery are rigid and straight, lacking distal dexterity to access into thin branches of PA. Therefore, this work presents a novel robotized dissector based on concentric push/pull robot (CPPR) structure, enabling entering deep thin branch of tortuous PA. Compared with conventional rigid dissectors, our design characterizes slenderness and dual-segment-bending dexterity. Owing to the hollow and thin-walled structure of the CPPR-based dissector as it has a slender body of 3.5mm in diameter, the central lumen accommodates two channels for irrigation and tip tool, and space for endoscopic camera's signal wire. To provide accurate surgical manipulation, optimization-based kinematics model was established, realizing a 2mm accuracy in positioning the tip tool (60mm length) under open-loop control strategy. As such, with the endoscopic camera, traditional PTE is possible to be upgraded as endoscopic PTE. Basic physic performance of the robotized dissector including stiffness, motion accuracy and maneuverability was evaluated through experiments. Surgery simulation on ex vivo porcine lung also demonstrates its dexterity and notable advantages in PTE.",
          "authors": [
            "Runfeng Zhu",
            "Xin Zhong",
            "Qingxiang Zhao",
            "Jing Lin",
            "Zhong Wu",
            "Kang Li"
          ],
          "published": "2026-02-03T06:04:11Z",
          "updated": "2026-02-03T06:04:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03147v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03112v2",
          "title": "A Unified Candidate Set with Scene-Adaptive Refinement via Diffusion for End-to-End Autonomous Driving",
          "summary": "End-to-end autonomous driving is increasingly adopting a multimodal planning paradigm that generates multiple trajectory candidates and selects the final plan, making candidate-set design critical. A fixed trajectory vocabulary provides stable coverage in routine driving but often misses optimal solutions in complex interactions, while scene-adaptive refinement can cause over-correction in simple scenarios by unnecessarily perturbing already strong vocabulary trajectories.We propose CdDrive, which preserves the original vocabulary candidates and augments them with scene-adaptive candidates generated by vocabulary-conditioned diffusion denoising. Both candidate types are jointly scored by a shared selection module, enabling reliable performance across routine and highly interactive scenarios. We further introduce HATNA (Horizon-Aware Trajectory Noise Adapter) to improve the smoothness and geometric continuity of diffusion candidates via temporal smoothing and horizon-aware noise modulation. Experiments on NAVSIM v1 and NAVSIM v2 demonstrate leading performance, and ablations verify the contribution of each component. Code: https://github.com/WWW-TJ/CdDrive.",
          "authors": [
            "Zhengfei Wu",
            "Shuaixi Pan",
            "Shuohan Chen",
            "Shuo Yang",
            "Yanjun Huang"
          ],
          "published": "2026-02-03T05:14:08Z",
          "updated": "2026-02-04T04:42:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03112v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03087v1",
          "title": "Training and Simulation of Quadrupedal Robot in Adaptive Stair Climbing for Indoor Firefighting: An End-to-End Reinforcement Learning Approach",
          "summary": "Quadruped robots are used for primary searches during the early stages of indoor fires. A typical primary search involves quickly and thoroughly looking for victims under hazardous conditions and monitoring flammable materials. However, situational awareness in complex indoor environments and rapid stair climbing across different staircases remain the main challenges for robot-assisted primary searches. In this project, we designed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize both navigation and locomotion. In the first stage, the quadrupeds, Unitree Go2, were trained to climb stairs in Isaac Lab's pyramid-stair terrain. In the second stage, the quadrupeds were trained to climb various realistic indoor staircases in the Isaac Lab engine, with the learned policy transferred from the previous stage. These indoor staircases are straight, L-shaped, and spiral, to support climbing tasks in complex environments. This project explores how to balance navigation and locomotion and how end-to-end RL methods can enable quadrupeds to adapt to different stair shapes. Our main contributions are: (1) A two-stage end-to-end RL framework that transfers stair-climbing skills from abstract pyramid terrain to realistic indoor stair topologies. (2) A centerline-based navigation formulation that enables unified learning of navigation and locomotion without hierarchical planning. (3) Demonstration of policy generalization across diverse staircases using only local height-map perception. (4) An empirical analysis of success, efficiency, and failure modes under increasing stair difficulty.",
          "authors": [
            "Baixiao Huang",
            "Baiyu Huang",
            "Yu Hou"
          ],
          "published": "2026-02-03T04:23:50Z",
          "updated": "2026-02-03T04:23:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03087v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03054v1",
          "title": "Towards Considerate Embodied AI: Co-Designing Situated Multi-Site Healthcare Robots from Abstract Concepts to High-Fidelity Prototypes",
          "summary": "Co-design is essential for grounding embodied artificial intelligence (AI) systems in real-world contexts, especially high-stakes domains such as healthcare. While prior work has explored multidisciplinary collaboration, iterative prototyping, and support for non-technical participants, few have interwoven these into a sustained co-design process. Such efforts often target one context and low-fidelity stages, limiting the generalizability of findings and obscuring how participants' ideas evolve. To address these limitations, we conducted a 14-week workshop with a multidisciplinary team of 22 participants, centered around how embodied AI can reduce non-value-added task burdens in three healthcare settings: emergency departments, long-term rehabilitation facilities, and sleep disorder clinics. We found that the iterative progression from abstract brainstorming to high-fidelity prototypes, supported by educational scaffolds, enabled participants to understand real-world trade-offs and generate more deployable solutions. We propose eight guidelines for co-designing more considerate embodied AI: attuned to context, responsive to social dynamics, mindful of expectations, and grounded in deployment. Project Page: https://byc-sophie.github.io/Towards-Considerate-Embodied-AI/",
          "authors": [
            "Yuanchen Bai",
            "Ruixiang Han",
            "Niti Parikh",
            "Wendy Ju",
            "Angelique Taylor"
          ],
          "published": "2026-02-03T03:30:41Z",
          "updated": "2026-02-03T03:30:41Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03054v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.03002v1",
          "title": "RPL: Learning Robust Humanoid Perceptive Locomotion on Challenging Terrains",
          "summary": "Humanoid perceptive locomotion has made significant progress and shows great promise, yet achieving robust multi-directional locomotion on complex terrains remains underexplored. To tackle this challenge, we propose RPL, a two-stage training framework that enables multi-directional locomotion on challenging terrains, and remains robust with payloads. RPL first trains terrain-specific expert policies with privileged height map observations to master decoupled locomotion and manipulation skills across different terrains, and then distills them into a transformer policy that leverages multiple depth cameras to cover a wide range of views. During distillation, we introduce two techniques to robustify multi-directional locomotion, depth feature scaling based on velocity commands and random side masking, which are critical for asymmetric depth observations and unseen widths of terrains. For scalable depth distillation, we develop an efficient multi-depth system that ray-casts against both dynamic robot meshes and static terrain meshes in massively parallel environments, achieving a 5-times speedup over the depth rendering pipelines in existing simulators while modeling realistic sensor latency, noise, and dropout. Extensive real-world experiments demonstrate robust multi-directional locomotion with payloads (2kg) across challenging terrains, including 20° slopes, staircases with different step lengths (22 cm, 25 cm, 30 cm), and 25 cm by 25 cm stepping stones separated by 60 cm gaps.",
          "authors": [
            "Yuanhang Zhang",
            "Younggyo Seo",
            "Juyue Chen",
            "Yifu Yuan",
            "Koushil Sreenath",
            "Pieter Abbeel",
            "Carmelo Sferrazza",
            "Karen Liu",
            "Rocky Duan",
            "Guanya Shi"
          ],
          "published": "2026-02-03T02:17:08Z",
          "updated": "2026-02-03T02:17:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.03002v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02960v1",
          "title": "Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control",
          "summary": "Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at https://eagle-wbc.github.io/",
          "authors": [
            "Quanquan Peng",
            "Yunfeng Lin",
            "Yufei Xue",
            "Jiangmiao Pang",
            "Weinan Zhang"
          ],
          "published": "2026-02-03T00:58:29Z",
          "updated": "2026-02-03T00:58:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02960v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02915v1",
          "title": "Modular Isoperimetric Soft Robotic Truss for Lunar Applications",
          "summary": "We introduce a large-scale robotic system designed as a lightweight, modular, and reconfigurable structure for lunar applications. The system consists of truss-like robotic triangles formed by continuous inflated fabric tubes routed through two robotic roller units and a connecting unit. A newly developed spherical joint enables up to three triangles to connect at a vertex, allowing construction of truss assemblies beyond a single octahedron. When deflated, the triangles compact to approximately the volume of the roller units, achieving a stowed-to-deployed volume ratio of 1:18.3. Upon inflation, the roller units pinch the tubes, locally reducing bending stiffness to form effective joints. Electric motors then translate the roller units along the tube, shifting the pinch point by lengthening one edge while shortening another at the same rate, thereby preserving a constant perimeter (isoperimetric). This shape-changing process requires no additional compressed air, enabling untethered operation after initial inflation. We demonstrate the system as a 12-degree-of-freedom solar array capable of tilting up to 60 degrees and sweeping 360 degrees, and as a 14-degree-of-freedom locomotion device using a step-and-slide gait. This modular, shape-adaptive system addresses key challenges for sustainable lunar operations and future space missions.",
          "authors": [
            "Mihai Stanciu",
            "Isaac Weaver",
            "Adam Rose",
            "James Wade",
            "Kaden Paxton",
            "Chris Paul",
            "Spencer Stowell",
            "Nathan Usevitch"
          ],
          "published": "2026-02-02T23:41:38Z",
          "updated": "2026-02-02T23:41:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02915v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02895v1",
          "title": "Moving On, Even When You're Broken: Fail-Active Trajectory Generation via Diffusion Policies Conditioned on Embodiment and Task",
          "summary": "Robot failure is detrimental and disruptive, often requiring human intervention to recover. Maintaining safe operation under impairment to achieve task completion, i.e. fail-active operation, is our target. Focusing on actuation failures, we introduce DEFT, a diffusion-based trajectory generator conditioned on the robot's current embodiment and task constraints. DEFT generalizes across failure types, supports constrained and unconstrained motions, and enables task completion under arbitrary failure. We evaluated DEFT in both simulation and real-world scenarios using a 7-DoF robotic arm. In simulation over thousands of joint-failure cases across multiple tasks, DEFT outperformed the baseline by up to 2 times. On failures unseen during training, it continued to outperform the baseline, indicating robust generalization in simulation. Further, we performed real-world evaluations on two multi-step tasks, drawer manipulation and whiteboard erasing. These experiments demonstrated DEFT succeeding on tasks where classical methods failed. Our results show that DEFT achieves fail-active manipulation across arbitrary failure configurations and real-world deployments.",
          "authors": [
            "Gilberto G. Briscoe-Martinez",
            "Yaashia Gautam",
            "Rahul Shetty",
            "Anuj Pasricha",
            "Marco M. Nicotra",
            "Alessandro Roncone"
          ],
          "published": "2026-02-02T23:02:48Z",
          "updated": "2026-02-02T23:02:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02895v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02864v1",
          "title": "Accelerating Structured Chain-of-Thought in Autonomous Vehicles",
          "summary": "Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning.",
          "authors": [
            "Yi Gu",
            "Yan Wang",
            "Yuxiao Chen",
            "Yurong You",
            "Wenjie Luo",
            "Yue Wang",
            "Wenhao Ding",
            "Boyi Li",
            "Heng Yang",
            "Boris Ivanovic",
            "Marco Pavone"
          ],
          "published": "2026-02-02T22:14:26Z",
          "updated": "2026-02-02T22:14:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02864v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02858v1",
          "title": "IMAGINE: Intelligent Multi-Agent Godot-based Indoor Networked Exploration",
          "summary": "The exploration of unknown, Global Navigation Satellite System (GNSS) denied environments by an autonomous communication-aware and collaborative group of Unmanned Aerial Vehicles (UAVs) presents significant challenges in coordination, perception, and decentralized decision-making. This paper implements Multi-Agent Reinforcement Learning (MARL) to address these challenges in a 2D indoor environment, using high-fidelity game-engine simulations (Godot) and continuous action spaces. Policy training aims to achieve emergent collaborative behaviours and decision-making under uncertainty using Network-Distributed Partially Observable Markov Decision Processes (ND-POMDPs). Each UAV is equipped with a Light Detection and Ranging (LiDAR) sensor and can share data (sensor measurements and a local occupancy map) with neighbouring agents. Inter-agent communication constraints include limited range, bandwidth and latency. Extensive ablation studies evaluated MARL training paradigms, reward function, communication system, neural network (NN) architecture, memory mechanisms, and POMDP formulations. This work jointly addresses several key limitations in prior research, namely reliance on discrete actions, single-agent or centralized formulations, assumptions of a priori knowledge and permanent connectivity, inability to handle dynamic obstacles, short planning horizons and architectural complexity in Recurrent NNs/Transformers. Results show that the scalable training paradigm, combined with a simplified architecture, enables rapid autonomous exploration of an indoor area. The implementation of Curriculum-Learning (five increasingly complex levels) also enabled faster, more robust training. This combination of high-fidelity simulation, MARL formulation, and computational efficiency establishes a strong foundation for deploying learned cooperative strategies in physical robotic systems.",
          "authors": [
            "Tiago Leite",
            "Maria Conceição",
            "António Grilo"
          ],
          "published": "2026-02-02T22:08:41Z",
          "updated": "2026-02-02T22:08:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG",
            "cs.MA",
            "cs.NI",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02858v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02857v1",
          "title": "Latent Perspective-Taking via a Schrödinger Bridge in Influence-Augmented Local Models",
          "summary": "Operating in environments alongside humans requires robots to make decisions under uncertainty. In addition to exogenous dynamics, they must reason over others' hidden mental-models and mental-states. While Interactive POMDPs and Bayesian Theory of Mind formulations are principled, exact nested-belief inference is intractable, and hand-specified models are brittle in open-world settings. We address both by learning structured mental-models and an estimator of others' mental-states. Building on the Influence-Based Abstraction, we instantiate an Influence-Augmented Local Model to decompose socially-aware robot tasks into local dynamics, social influences, and exogenous factors. We propose (a) a neuro-symbolic world model instantiating a factored, discrete Dynamic Bayesian Network, and (b) a perspective-shift operator modeled as an amortized Schrödinger Bridge over the learned local dynamics that transports factored egocentric beliefs into other-centric beliefs. We show that this architecture enables agents to synthesize socially-aware policies in model-based reinforcement learning, via decision-time mental-state planning (a Schrödinger Bridge in belief space), with preliminary results in a MiniGrid social navigation task.",
          "authors": [
            "Kevin Alcedo",
            "Pedro U. Lima",
            "Rachid Alami"
          ],
          "published": "2026-02-02T22:05:23Z",
          "updated": "2026-02-02T22:05:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02857v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02847v1",
          "title": "Causal Flow Q-Learning for Robust Offline Reinforcement Learning",
          "summary": "Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator's and the learner's sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies' worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\\% that of confounding-unaware, state-of-the-art offline RL methods.",
          "authors": [
            "Mingxuan Li",
            "Junzhe Zhang",
            "Elias Bareinboim"
          ],
          "published": "2026-02-02T21:50:52Z",
          "updated": "2026-02-02T21:50:52Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02847v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02846v1",
          "title": "Kino-PAX$^+$: Near-Optimal Massively Parallel Kinodynamic Sampling-based Motion Planner",
          "summary": "Sampling-based motion planners (SBMPs) are widely used for robot motion planning with complex kinodynamic constraints in high-dimensional spaces, yet they struggle to achieve \\emph{real-time} performance due to their serial computation design. Recent efforts to parallelize SBMPs have achieved significant speedups in finding feasible solutions; however, they provide no guarantees of optimizing an objective function. We introduce Kino-PAX$^{+}$, a massively parallel kinodynamic SBMP with asymptotic near-optimal guarantees. Kino-PAX$^{+}$ builds a sparse tree of dynamically feasible trajectories by decomposing traditionally serial operations into three massively parallel subroutines. The algorithm focuses computation on the most promising nodes within local neighborhoods for propagation and refinement, enabling rapid improvement of solution cost. We prove that, while maintaining probabilistic $δ$-robust completeness, this focus on promising nodes ensures asymptotic $δ$-robust near-optimality. Our results show that Kino-PAX$^{+}$ finds solutions up to three orders of magnitude faster than existing serial methods and achieves lower solution costs than a state-of-the-art GPU-based planner.",
          "authors": [
            "Nicolas Perrault",
            "Qi Heng Ho",
            "Morteza Lahijanian"
          ],
          "published": "2026-02-02T21:50:18Z",
          "updated": "2026-02-02T21:50:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.DC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02846v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02839v2",
          "title": "Language Movement Primitives: Grounding Language Models in Robot Motion",
          "summary": "Enabling robots to perform novel manipulation tasks from natural language instructions remains a fundamental challenge in robotics, despite significant progress in generalized problem solving with foundational models. Large vision and language models (VLMs) are capable of processing high-dimensional input data for visual scene and language understanding, as well as decomposing tasks into a sequence of logical steps; however, they struggle to ground those steps in embodied robot motion. On the other hand, robotics foundation models output action commands, but require in-domain fine-tuning or experience before they are able to perform novel tasks successfully. At its core, there still remains the fundamental challenge of connecting abstract task reasoning with low-level motion control. To address this disconnect, we propose Language Movement Primitives (LMPs), a framework that grounds VLM reasoning in Dynamic Movement Primitive (DMP) parameterization. Our key insight is that DMPs provide a small number of interpretable parameters, and VLMs can set these parameters to specify diverse, continuous, and stable trajectories. Put another way: VLMs can reason over free-form natural language task descriptions, and semantically ground their desired motions into DMPs -- bridging the gap between high-level task reasoning and low-level position and velocity control. Building on this combination of VLMs and DMPs, we formulate our LMP pipeline for zero-shot robot manipulation that effectively completes tabletop manipulation problems by generating a sequence of DMP motions. Across 20 real-world manipulation tasks, we show that LMP achieves 80% task success as compared to 31% for the best-performing baseline. See videos at our website: https://collab.me.vt.edu/lmp",
          "authors": [
            "Yinlong Dai",
            "Benjamin A. Christie",
            "Daniel J. Evans",
            "Dylan P. Losey",
            "Simon Stepputtis"
          ],
          "published": "2026-02-02T21:41:08Z",
          "updated": "2026-02-16T16:41:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02839v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02831v1",
          "title": "Adaptive Linear Path Model-Based Diffusion",
          "summary": "The interest in combining model-based control approaches with diffusion models has been growing. Although we have seen many impressive robotic control results in difficult tasks, the performance of diffusion models is highly sensitive to the choice of scheduling parameters, making parameter tuning one of the most critical challenges. We introduce Linear Path Model-Based Diffusion (LP-MBD), which replaces the variance-preserving schedule with a flow-matching-inspired linear probability path. This yields a geometrically interpretable and decoupled parameterization that reduces tuning complexity and provides a stable foundation for adaptation. Building on this, we propose Adaptive LP-MBD (ALP-MBD), which leverages reinforcement learning to adjust diffusion steps and noise levels according to task complexity and environmental conditions. Across numerical studies, Brax benchmarks, and mobile-robot trajectory tracking, LP-MBD simplifies scheduling while maintaining strong performance, and ALP-MBD further improves robustness, adaptability, and real-time efficiency.",
          "authors": [
            "Yutaka Shimizu",
            "Masayoshi Tomizuka"
          ],
          "published": "2026-02-02T21:33:03Z",
          "updated": "2026-02-02T21:33:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02831v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02826v1",
          "title": "Fast Near Time-Optimal Motion Planning for Holonomic Vehicles in Structured Environments",
          "summary": "This paper proposes a novel and efficient optimization-based method for generating near time-optimal trajectories for holonomic vehicles navigating through complex but structured environments. The approach aims to solve the problem of motion planning for planar motion systems using magnetic levitation that can be used in assembly lines, automated laboratories or clean-rooms. In these applications, time-optimal trajectories that can be computed in real-time are required to increase productivity and allow the vehicles to be reactive if needed. The presented approach encodes the environment representation using free-space corridors and represents the motion of the vehicle through such a corridor using a motion primitive. These primitives are selected heuristically and define the trajectory with a limited number of degrees of freedom, which are determined in an optimization problem. As a result, the method achieves significantly lower computation times compared to the state-of-the-art, most notably solving a full Optimal Control Problem (OCP), OMG-tools or VP-STO without significantly compromising optimality within a fixed corridor sequence. The approach is benchmarked extensively in simulation and is validated on a real-world Beckhoff XPlanar system",
          "authors": [
            "Louis Callens",
            "Bastiaan Vandewal",
            "Ibrahim Ibrahim",
            "Jan Swevers",
            "Wilm Decré"
          ],
          "published": "2026-02-02T21:26:30Z",
          "updated": "2026-02-02T21:26:30Z",
          "primary_category": "math.OC",
          "categories": [
            "math.OC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02826v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02814v1",
          "title": "Sub-optimality bounds for certainty equivalent policies in partially observed systems",
          "summary": "In this paper, we present a generalization of the certainty equivalence principle of stochastic control. One interpretation of the classical certainty equivalence principle for linear systems with output feedback and quadratic costs is as follows: the optimal action at each time is obtained by evaluating the optimal state-feedback policy of the stochastic linear system at the minimum mean square error (MMSE) estimate of the state. Motivated by this interpretation, we consider certainty equivalent policies for general (non-linear) partially observed stochastic systems that allow for any state estimate rather than restricting to MMSE estimates. In such settings, the certainty equivalent policy is not optimal. For models where the cost and the dynamics are smooth in an appropriate sense, we derive upper bounds on the sub-optimality of certainty equivalent policies. We present several examples to illustrate the results.",
          "authors": [
            "Berk Bozkurt",
            "Aditya Mahajan",
            "Ashutosh Nayyar",
            "Yi Ouyang"
          ],
          "published": "2026-02-02T21:12:13Z",
          "updated": "2026-02-02T21:12:13Z",
          "primary_category": "math.OC",
          "categories": [
            "math.OC",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02814v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02790v1",
          "title": "Simulating Human Audiovisual Search Behavior",
          "summary": "Locating a target based on auditory and visual cues$\\unicode{x2013}$such as finding a car in a crowded parking lot or identifying a speaker in a virtual meeting$\\unicode{x2013}$requires balancing effort, time, and accuracy under uncertainty. Existing models of audiovisual search often treat perception and action in isolation, overlooking how people adaptively coordinate movement and sensory strategies. We present Sensonaut, a computational model of embodied audiovisual search. The core assumption is that people deploy their body and sensory systems in ways they believe will most efficiently improve their chances of locating a target, trading off time and effort under perceptual constraints. Our model formulates this as a resource-rational decision-making problem under partial observability. We validate the model against newly collected human data, showing that it reproduces both adaptive scaling of search time and effort under task complexity, occlusion, and distraction, and characteristic human errors. Our simulation of human-like resource-rational search informs the design of audiovisual interfaces that minimize search cost and cognitive load.",
          "authors": [
            "Hyunsung Cho",
            "Xuejing Luo",
            "Byungjoo Lee",
            "David Lindlbauer",
            "Antti Oulasvirta"
          ],
          "published": "2026-02-02T20:47:05Z",
          "updated": "2026-02-02T20:47:05Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02790v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02773v1",
          "title": "Bimanual High-Density EMG Control for In-Home Mobile Manipulation by a User with Quadriplegia",
          "summary": "Mobile manipulators in the home can enable people with cervical spinal cord injury (cSCI) to perform daily physical household tasks that they could not otherwise do themselves. However, paralysis in these users often limits access to traditional robot control interfaces such as joysticks or keyboards. In this work, we introduce and deploy the first system that enables a user with quadriplegia to control a mobile manipulator in their own home using bimanual high-density electromyography (HDEMG). We develop a pair of custom, fabric-integrated HDEMG forearm sleeves, worn on both arms, that capture residual neuromotor activity from clinically paralyzed degrees of freedom and support real-time gesture-based robot control. Second, by integrating vision, language, and motion planning modules, we introduce a shared autonomy framework that supports robust and user-driven teleoperation, with particular benefits for navigation-intensive tasks in home environments. Finally, to demonstrate the system in the wild, we present a twelve-day in-home user study evaluating real-time use of the wearable EMG interface for daily robot control. Together, these system components enable effective robot control for performing activities of daily living and other household tasks in a real home environment.",
          "authors": [
            "Jehan Yang",
            "Eleanor Hodgson",
            "Cindy Sun",
            "Zackory Erickson",
            "Doug Weber"
          ],
          "published": "2026-02-02T20:28:18Z",
          "updated": "2026-02-02T20:28:18Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02773v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02745v1",
          "title": "Ethical Asymmetry in Human-Robot Interaction - An Empirical Test of Sparrow's Hypothesis",
          "summary": "The ethics of human-robot interaction (HRI) have been discussed extensively based on three traditional frameworks: deontology, consequentialism, and virtue ethics. We conducted a mixed within/between experiment to investigate Sparrow's proposed ethical asymmetry hypothesis in human treatment of robots. The moral permissibility of action (MPA) was manipulated as a subject grouping variable, and virtue type (prudence, justice, courage, and temperance) was controlled as a within-subjects factor. We tested moral stimuli using an online questionnaire with Perceived Moral Permissibility of Action (PMPA) and Perceived Virtue Scores (PVS) as response measures. The PVS measure was based on an adaptation of the established Questionnaire on Cardinal Virtues (QCV), while the PMPA was based on Malle et al. [39] work. We found that the MPA significantly influenced the PMPA and perceived virtue scores. The best-fitting model to describe the relationship between PMPA and PVS was cubic, which is symmetrical in nature. Our study did not confirm Sparrow's asymmetry hypothesis. The adaptation of the QCV is expected to have utility for future studies, pending additional psychometric property assessments.",
          "authors": [
            "Minyi Wang",
            "Christoph Bartneck",
            "Michael-John Turp",
            "David Kaber"
          ],
          "published": "2026-02-02T19:59:02Z",
          "updated": "2026-02-02T19:59:02Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02745v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02741v1",
          "title": "PokeNet: Learning Kinematic Models of Articulated Objects from Human Observations",
          "summary": "Articulation modeling enables robots to learn joint parameters of articulated objects for effective manipulation which can then be used downstream for skill learning or planning. Existing approaches often rely on prior knowledge about the objects, such as the number or type of joints. Some of these approaches also fail to recover occluded joints that are only revealed during interaction. Others require large numbers of multi-view images for every object, which is impractical in real-world settings. Furthermore, prior works neglect the order of manipulations, which is essential for many multi-DoF objects where one joint must be operated before another, such as a dishwasher. We introduce PokeNet, an end-to-end framework that estimates articulation models from a single human demonstration without prior object knowledge. Given a sequence of point cloud observations of a human manipulating an unknown object, PokeNet predicts joint parameters, infers manipulation order, and tracks joint states over time. PokeNet outperforms existing state-of-the-art methods, improving joint axis and state estimation accuracy by an average of over 27% across diverse objects, including novel and unseen categories. We demonstrate these gains in both simulation and real-world environments.",
          "authors": [
            "Anmol Gupta",
            "Weiwei Gu",
            "Omkar Patil",
            "Jun Ki Lee",
            "Nakul Gopalan"
          ],
          "published": "2026-02-02T19:55:02Z",
          "updated": "2026-02-02T19:55:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02741v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02730v1",
          "title": "AROLA: A Modular Layered Architecture for Scaled Autonomous Racing",
          "summary": "Autonomous racing has advanced rapidly, particularly on scaled platforms, and software stacks must evolve accordingly. In this work, AROLA is introduced as a modular, layered software architecture in which fragmented and monolithic designs are reorganized into interchangeable layers and components connected through standardized ROS 2 interfaces. The autonomous-driving pipeline is decomposed into sensing, pre-processing, perception, localization and mapping, planning, behavior, control, and actuation, enabling rapid module replacement and objective benchmarking without reliance on custom message definitions. To support consistent performance evaluation, a Race Monitor framework is introduced as a lightweight system through which lap timing, trajectory quality, and computational load are logged in real time and standardized post-race analyses are generated. AROLA is validated in simulation and on hardware using the RoboRacer platform, including deployment at the 2025 RoboRacer IV25 competition. Together, AROLA and Race Monitor demonstrate that modularity, transparent interfaces, and systematic evaluation can accelerate development and improve reproducibility in scaled autonomous racing.",
          "authors": [
            "Fam Shihata",
            "Mohammed Abdelazim",
            "Ahmed Hussein"
          ],
          "published": "2026-02-02T19:46:34Z",
          "updated": "2026-02-02T19:46:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.SE"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02730v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02722v1",
          "title": "Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion",
          "summary": "We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: https://sites.google.com/view/hecrl",
          "authors": [
            "Dan Haramati",
            "Carl Qi",
            "Tal Daniel",
            "Amy Zhang",
            "Aviv Tamar",
            "George Konidaris"
          ],
          "published": "2026-02-02T19:40:54Z",
          "updated": "2026-02-02T19:40:54Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02722v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02481v1",
          "title": "Flow Policy Gradients for Robot Control",
          "summary": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.",
          "authors": [
            "Brent Yi",
            "Hongsuk Choi",
            "Himanshu Gaurav Singh",
            "Xiaoyu Huang",
            "Takara E. Truong",
            "Carmelo Sferrazza",
            "Yi Ma",
            "Rocky Duan",
            "Pieter Abbeel",
            "Guanya Shi",
            "Karen Liu",
            "Angjoo Kanazawa"
          ],
          "published": "2026-02-02T18:56:49Z",
          "updated": "2026-02-02T18:56:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02481v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02473v1",
          "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
          "summary": "Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.",
          "authors": [
            "Yinhuai Wang",
            "Qihan Zhao",
            "Yuen Fui Lau",
            "Runyi Yu",
            "Hok Wai Tsui",
            "Qifeng Chen",
            "Jingbo Wang",
            "Jiangmiao Pang",
            "Ping Tan"
          ],
          "published": "2026-02-02T18:53:01Z",
          "updated": "2026-02-02T18:53:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02473v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02459v1",
          "title": "TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments",
          "summary": "Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/",
          "authors": [
            "Zhiyu Huang",
            "Yun Zhang",
            "Johnson Liu",
            "Rui Song",
            "Chen Tang",
            "Jiaqi Ma"
          ],
          "published": "2026-02-02T18:47:49Z",
          "updated": "2026-02-02T18:47:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02459v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02456v1",
          "title": "Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning",
          "summary": "Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.",
          "authors": [
            "Albert Gassol Puigjaner",
            "Angelos Zacharia",
            "Kostas Alexis"
          ],
          "published": "2026-02-02T18:47:02Z",
          "updated": "2026-02-02T18:47:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02456v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02454v1",
          "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
          "summary": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.",
          "authors": [
            "Ansh Kumar Sharma",
            "Yixiang Sun",
            "Ninghao Lu",
            "Yunzhe Zhang",
            "Jiarao Liu",
            "Sherry Yang"
          ],
          "published": "2026-02-02T18:44:45Z",
          "updated": "2026-02-02T18:44:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02454v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02430v1",
          "title": "3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM",
          "summary": "Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.",
          "authors": [
            "Pierre-Yves Lajoie",
            "Benjamin Ramtoula",
            "Daniele De Martini",
            "Giovanni Beltrame"
          ],
          "published": "2026-02-02T18:30:32Z",
          "updated": "2026-02-02T18:30:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02430v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02411v1",
          "title": "Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces",
          "summary": "Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.",
          "authors": [
            "Hanwen Ren",
            "Junyong Kim",
            "Aathman Tharmasanthiran",
            "Ahmed H. Qureshi"
          ],
          "published": "2026-02-02T18:10:45Z",
          "updated": "2026-02-02T18:10:45Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02411v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02402v1",
          "title": "SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation",
          "summary": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.",
          "authors": [
            "Mu Huang",
            "Hui Wang",
            "Kerui Ren",
            "Linning Xu",
            "Yunsong Zhou",
            "Mulin Yu",
            "Bo Dai",
            "Jiangmiao Pang"
          ],
          "published": "2026-02-02T17:59:31Z",
          "updated": "2026-02-02T17:59:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "physics.app-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02402v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02396v1",
          "title": "PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning",
          "summary": "Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.",
          "authors": [
            "Amisha Bhaskar",
            "Pratap Tokekar",
            "Stefano Di Cairano",
            "Alexander Schperberg"
          ],
          "published": "2026-02-02T17:57:37Z",
          "updated": "2026-02-02T17:57:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02396v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02389v1",
          "title": "Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures",
          "summary": "Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.",
          "authors": [
            "Marina Ruediger",
            "Ashis G. Banerjee"
          ],
          "published": "2026-02-02T17:51:32Z",
          "updated": "2026-02-02T17:51:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02389v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02331v1",
          "title": "TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour",
          "summary": "Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.",
          "authors": [
            "Shaoting Zhu",
            "Baijun Ye",
            "Jiaxuan Wang",
            "Jiakang Chen",
            "Ziwen Zhuang",
            "Linzhan Mou",
            "Runhan Huang",
            "Hang Zhao"
          ],
          "published": "2026-02-02T16:55:10Z",
          "updated": "2026-02-02T16:55:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02331v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02293v1",
          "title": "Before Autonomy Takes Control: Software Testing in Robotics",
          "summary": "Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.",
          "authors": [
            "Nils Chur",
            "Thiago Santos de Moura",
            "Argentina Ortega",
            "Sven Peldszus",
            "Thorsten Berger",
            "Nico Hochgeschwender",
            "Yannic Noller"
          ],
          "published": "2026-02-02T16:30:23Z",
          "updated": "2026-02-02T16:30:23Z",
          "primary_category": "cs.SE",
          "categories": [
            "cs.SE",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02293v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02269v1",
          "title": "Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems",
          "summary": "We present $multipanda\\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.",
          "authors": [
            "Jon Škerlj",
            "Seongjin Bien",
            "Abdeldjallil Naceri",
            "Sami Haddadin"
          ],
          "published": "2026-02-02T16:11:12Z",
          "updated": "2026-02-02T16:11:12Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.SE",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02269v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02236v3",
          "title": "Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL",
          "summary": "Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.",
          "authors": [
            "Julian Lemmel",
            "Felix Resch",
            "Mónika Farsang",
            "Ramin Hasani",
            "Daniela Rus",
            "Radu Grosu"
          ],
          "published": "2026-02-02T15:41:53Z",
          "updated": "2026-02-17T14:36:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG",
            "cs.NE",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02236v3",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02220v1",
          "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation",
          "summary": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap",
          "authors": [
            "Bo Miao",
            "Weijia Liu",
            "Jun Luo",
            "Lachlan Shinnick",
            "Jian Liu",
            "Thomas Hamilton-Smith",
            "Yuhe Yang",
            "Zijie Wu",
            "Vanja Videnovic",
            "Feras Dayoub",
            "Anton van den Hengel"
          ],
          "published": "2026-02-02T15:26:19Z",
          "updated": "2026-02-02T15:26:19Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02220v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02181v1",
          "title": "Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls",
          "summary": "Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.",
          "authors": [
            "Elad Siman Tov",
            "Nili E. Krausz"
          ],
          "published": "2026-02-02T14:49:35Z",
          "updated": "2026-02-02T14:49:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02181v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02167v1",
          "title": "Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding",
          "summary": "Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.",
          "authors": [
            "Soheil Behnam Roudsari",
            "Alexandre S. Brandão",
            "Felipe N. Martins"
          ],
          "published": "2026-02-02T14:44:27Z",
          "updated": "2026-02-02T14:44:27Z",
          "primary_category": "eess.SP",
          "categories": [
            "eess.SP",
            "cs.CV",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02167v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02142v1",
          "title": "FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation",
          "summary": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.",
          "authors": [
            "Ruiteng Zhao",
            "Wenshuo Wang",
            "Yicheng Ma",
            "Xiaocong Li",
            "Francis E. H. Tay",
            "Marcelo H. Ang",
            "Haiyue Zhu"
          ],
          "published": "2026-02-02T14:19:46Z",
          "updated": "2026-02-02T14:19:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02142v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13252v1",
          "title": "DORA: Dataflow Oriented Robotic Architecture",
          "summary": "Robotic middleware serves as the foundational infrastructure, enabling complex robotic systems to operate in a coordinated and modular manner. In data-intensive robotic applications, especially in industrial scenarios, communication efficiency directly impact system responsiveness, stability, and overall productivity. However, existing robotic middleware exhibit several limitations: (1) they rely heavily on (de)serialization mechanisms, introducing significant overhead for large-sized data; (2) they lack efficient and flexible support for heterogeneous data sizes, particularly in intra-robot communication and Python-based execution environments. To address these challenges, we propose Dataflow-Oriented Robotic Architecture (DORA) that enables explicit data dependency specification and efficient zero-copy data transmission. We implement the proposed framework as an open-source system and evaluate it through extensive experiments in both simulation and real-world robotic environments. Experimental results demonstrate substantial reductions in latency and CPU overhead compared to state-of-the-art middleware.",
          "authors": [
            "Xiaodong Zhang",
            "Baorui Lv",
            "Xavier Tao",
            "Xiong Wang",
            "Jie Bao",
            "Yong He",
            "Yue Chen",
            "Zijiang Yang"
          ],
          "published": "2026-02-02T13:28:39Z",
          "updated": "2026-02-02T13:28:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.NI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13252v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02038v1",
          "title": "Frictional Contact Solving for Material Point Method",
          "summary": "Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.",
          "authors": [
            "Etienne Ménager",
            "Justin Carpentier"
          ],
          "published": "2026-02-02T12:34:32Z",
          "updated": "2026-02-02T12:34:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02038v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02035v1",
          "title": "Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization",
          "summary": "Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.",
          "authors": [
            "Ahmad Farooq",
            "Kamran Iqbal"
          ],
          "published": "2026-02-02T12:32:28Z",
          "updated": "2026-02-02T12:32:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.IT",
            "cs.LG",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02035v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02026v1",
          "title": "Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp",
          "summary": "We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.",
          "authors": [
            "Zhenwei Niu",
            "Xiaoyi Chen",
            "Jiayu Hu",
            "Zhaoyang Liu",
            "Xiaozu Ju"
          ],
          "published": "2026-02-02T12:21:27Z",
          "updated": "2026-02-02T12:21:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02026v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02006v1",
          "title": "Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements",
          "summary": "Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities. This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.",
          "authors": [
            "Thomas Jantos",
            "Giulio Delama",
            "Stephan Weiss",
            "Jan Steinbrener"
          ],
          "published": "2026-02-02T12:04:34Z",
          "updated": "2026-02-02T12:04:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02006v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01948v1",
          "title": "A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications",
          "summary": "Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.",
          "authors": [
            "Patrick Frank",
            "Christian Friedrich"
          ],
          "published": "2026-02-02T10:58:53Z",
          "updated": "2026-02-02T10:58:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01948v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01939v1",
          "title": "Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy",
          "summary": "Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.",
          "authors": [
            "Yuxin He",
            "Ruihao Zhang",
            "Tianao Shen",
            "Cheng Liu",
            "Qiang Nie"
          ],
          "published": "2026-02-02T10:43:46Z",
          "updated": "2026-02-02T10:43:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01939v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01930v1",
          "title": "LIEREx: Language-Image Embeddings for Robotic Exploration",
          "summary": "Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.",
          "authors": [
            "Felix Igelbrink",
            "Lennart Niecksch",
            "Marian Renz",
            "Martin Günther",
            "Martin Atzmueller"
          ],
          "published": "2026-02-02T10:30:50Z",
          "updated": "2026-02-02T10:30:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01930v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01916v1",
          "title": "ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning",
          "summary": "As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/",
          "authors": [
            "Keyu Chen",
            "Wenchao Sun",
            "Hao Cheng",
            "Zheng Fu",
            "Sifa Zheng"
          ],
          "published": "2026-02-02T10:20:11Z",
          "updated": "2026-02-02T10:20:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01916v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07024v1",
          "title": "A Distributed Multi-Modal Sensing Approach for Human Activity Recognition in Real-Time Human-Robot Collaboration",
          "summary": "Human activity recognition (HAR) is fundamental in human-robot collaboration (HRC), enabling robots to respond to and dynamically adapt to human intentions. This paper introduces a HAR system combining a modular data glove equipped with Inertial Measurement Units and a vision-based tactile sensor to capture hand activities in contact with a robot. We tested our activity recognition approach under different conditions, including offline classification of segmented sequences, real-time classification under static conditions, and a realistic HRC scenario. The experimental results show a high accuracy for all the tasks, suggesting that multiple collaborative settings could benefit from this multi-modal approach.",
          "authors": [
            "Valerio Belcamino",
            "Nhat Minh Dinh Le",
            "Quan Khanh Luu",
            "Alessandro Carfì",
            "Van Anh Ho",
            "Fulvio Mastrogiovanni"
          ],
          "published": "2026-02-02T10:14:19Z",
          "updated": "2026-02-02T10:14:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07024v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01899v1",
          "title": "Multi-Task Learning for Robot Perception with Imbalanced Data",
          "summary": "Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.",
          "authors": [
            "Ozgur Erkent"
          ],
          "published": "2026-02-02T10:05:59Z",
          "updated": "2026-02-02T10:05:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01899v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01892v1",
          "title": "Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study",
          "summary": "This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.",
          "authors": [
            "Alexandre Lombard",
            "Florent Perronnet",
            "Nicolas Gaud",
            "Abdeljalil Abbas-Turki"
          ],
          "published": "2026-02-02T10:03:37Z",
          "updated": "2026-02-02T10:03:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01892v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01880v1",
          "title": "Multimodal Large Language Models for Real-Time Situated Reasoning",
          "summary": "In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.",
          "authors": [
            "Giulio Antonio Abbo",
            "Senne Lenaerts",
            "Tony Belpaeme"
          ],
          "published": "2026-02-02T09:52:11Z",
          "updated": "2026-02-02T09:52:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01880v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01870v1",
          "title": "BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models",
          "summary": "Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.",
          "authors": [
            "Riccardo Andrea Izzo",
            "Gianluca Bardaro",
            "Matteo Matteucci"
          ],
          "published": "2026-02-02T09:43:17Z",
          "updated": "2026-02-02T09:43:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01870v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01860v1",
          "title": "Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach",
          "summary": "Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.",
          "authors": [
            "Filip Novák",
            "Matěj Petrlík",
            "Matej Novosad",
            "Parakh M. Gupta",
            "Robert Pěnička",
            "Martin Saska"
          ],
          "published": "2026-02-02T09:32:19Z",
          "updated": "2026-02-02T09:32:19Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01860v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15875v1",
          "title": "Fly0: Decoupling Semantic Grounding from Geometric Planning for Zero-Shot Aerial Navigation",
          "summary": "Current Visual-Language Navigation (VLN) methodologies face a trade-off between semantic understanding and control precision. While Multimodal Large Language Models (MLLMs) offer superior reasoning, deploying them as low-level controllers leads to high latency, trajectory oscillations, and poor generalization due to weak geometric grounding. To address these limitations, we propose Fly0, a framework that decouples semantic reasoning from geometric planning. The proposed method operates through a three-stage pipeline: (1) an MLLM-driven module for grounding natural language instructions into 2D pixel coordinates; (2) a geometric projection module that utilizes depth data to localize targets in 3D space; and (3) a geometric planner that generates collision-free trajectories. This mechanism enables robust navigation even when visual contact is lost. By eliminating the need for continuous inference, Fly0 reduces computational overhead and improves system stability. Extensive experiments in simulation and real-world environments demonstrate that Fly0 outperforms state-of-the-art baselines, improving the Success Rate by over 20\\% and reducing Navigation Error (NE) by approximately 50\\% in unstructured environments. Our code is available at https://github.com/xuzhenxing1/Fly0.",
          "authors": [
            "Zhenxing Xu",
            "Brikit Lu",
            "Weidong Bao",
            "Zhengqiu Zhu",
            "Junsong Zhang",
            "Hui Yan",
            "Wenhao Lu",
            "Ji Wang"
          ],
          "published": "2026-02-02T09:06:50Z",
          "updated": "2026-02-02T09:06:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15875v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01834v1",
          "title": "Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models",
          "summary": "Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.",
          "authors": [
            "Siqi Wen",
            "Shu Yang",
            "Shaopeng Fu",
            "Jingfeng Zhang",
            "Lijie Hu",
            "Di Wang"
          ],
          "published": "2026-02-02T09:06:43Z",
          "updated": "2026-02-02T09:06:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01834v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01811v1",
          "title": "From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models",
          "summary": "While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.",
          "authors": [
            "Wentao Zhang",
            "Aolan Sun",
            "Wentao Mo",
            "Xiaoyang Qu",
            "Yuxin Zheng",
            "Jianzong Wang"
          ],
          "published": "2026-02-02T08:44:40Z",
          "updated": "2026-02-02T08:44:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01811v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01789v3",
          "title": "RFS: Reinforcement Learning with Residual Flow Steering for Dexterous Manipulation",
          "summary": "Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors. We propose Residual Flow Steering(RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy. We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning in both simulation and real-world settings when adapting pretrained base policies. Project website:https://weirdlabuw.github.io/rfs.",
          "authors": [
            "Entong Su",
            "Tyler Westenbroek",
            "Anusha Nagabandi",
            "Abhishek Gupta"
          ],
          "published": "2026-02-02T08:11:57Z",
          "updated": "2026-02-05T09:22:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01789v3",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01780v2",
          "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models",
          "summary": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLab-SYSU/DDP-WM.",
          "authors": [
            "Shicheng Yin",
            "Kaixuan Yin",
            "Weixing Chen",
            "Yang Liu",
            "Guanbin Li",
            "Liang Lin"
          ],
          "published": "2026-02-02T08:04:25Z",
          "updated": "2026-02-03T02:48:57Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01780v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.13248v1",
          "title": "X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles",
          "summary": "Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon. At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification. At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts. The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.",
          "authors": [
            "Ashkan Y. Zadeh",
            "Xiaomeng Li",
            "Andry Rakotonirainy",
            "Ronald Schroeter",
            "Sebastien Glaser",
            "Zishuo Zhu"
          ],
          "published": "2026-02-02T07:18:25Z",
          "updated": "2026-02-02T07:18:25Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.CL",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.13248v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01731v1",
          "title": "Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion",
          "summary": "Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.",
          "authors": [
            "Jiwoo Hwang",
            "Taegeun Yang",
            "Jeil Jeong",
            "Minsung Yoon",
            "Sung-Eui Yoon"
          ],
          "published": "2026-02-02T07:12:27Z",
          "updated": "2026-02-02T07:12:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01731v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01700v1",
          "title": "Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels",
          "summary": "In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.",
          "authors": [
            "Ruoyu Wang",
            "Xuchen Liu",
            "Zongzhou Wu",
            "Zixuan Guo",
            "Wendi Ding",
            "Ben M. Chen"
          ],
          "published": "2026-02-02T06:15:33Z",
          "updated": "2026-02-02T06:15:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01700v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01693v1",
          "title": "GSR: Learning Structured Reasoning for Embodied Manipulation",
          "summary": "Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.",
          "authors": [
            "Kewei Hu",
            "Michael Zhang",
            "Wei Ying",
            "Tianhao Liu",
            "Guoqiang Hao",
            "Zimeng Li",
            "Wanchan Yu",
            "Jiajian Jing",
            "Fangwen Chen",
            "Hanwen Kang"
          ],
          "published": "2026-02-02T06:07:42Z",
          "updated": "2026-02-02T06:07:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01693v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01679v1",
          "title": "Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications",
          "summary": "The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.",
          "authors": [
            "Raghavasimhan Sankaranarayanan",
            "Paul Stuart",
            "Nicholas Ahn",
            "Arno Sungarian",
            "Yash Chitalia"
          ],
          "published": "2026-02-02T05:46:31Z",
          "updated": "2026-02-02T05:46:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01679v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01673v1",
          "title": "Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss",
          "summary": "Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.",
          "authors": [
            "Enguang Fan"
          ],
          "published": "2026-02-02T05:41:42Z",
          "updated": "2026-02-02T05:41:42Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01673v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01662v2",
          "title": "AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act",
          "summary": "Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.",
          "authors": [
            "Pengyuan Guo",
            "Zhonghao Mai",
            "Zhengtong Xu",
            "Kaidi Zhang",
            "Heng Zhang",
            "Zichen Miao",
            "Arash Ajoudani",
            "Zachary Kingston",
            "Qiang Qiu",
            "Yu She"
          ],
          "published": "2026-02-02T05:30:14Z",
          "updated": "2026-02-09T03:21:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01662v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01644v1",
          "title": "From Perception to Action: Spatial AI Agents and World Models",
          "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.",
          "authors": [
            "Gloria Felicia",
            "Nolan Bryant",
            "Handi Putra",
            "Ayaan Gazali",
            "Eliel Lobo",
            "Esteban Rojas"
          ],
          "published": "2026-02-02T05:00:55Z",
          "updated": "2026-02-02T05:00:55Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.MA",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01644v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01632v1",
          "title": "A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation",
          "summary": "Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.",
          "authors": [
            "Chuizheng Kong",
            "Yunho Cho",
            "Wonsuhk Jung",
            "Idris Wibowo",
            "Parth Shinde",
            "Sundhar Vinodh-Sangeetha",
            "Long Kiu Chung",
            "Zhenyang Chen",
            "Andrew Mattei",
            "Advaith Nidumukkala",
            "Alexander Elias",
            "Danfei Xu",
            "Taylor Higgins",
            "Shreyas Kousik"
          ],
          "published": "2026-02-02T04:44:55Z",
          "updated": "2026-02-02T04:44:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01632v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01629v1",
          "title": "AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments",
          "summary": "Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \\textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.",
          "authors": [
            "Renukanandan Tumu",
            "Aditya Singh",
            "Rahul Mangharam"
          ],
          "published": "2026-02-02T04:41:35Z",
          "updated": "2026-02-02T04:41:35Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01629v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01568v1",
          "title": "Efficiently Solving Mixed-Hierarchy Games with Quasi-Policy Approximations",
          "summary": "Multi-robot coordination often exhibits hierarchical structure, with some robots' decisions depending on the planned behaviors of others. While game theory provides a principled framework for such interactions, existing solvers struggle to handle mixed information structures that combine simultaneous (Nash) and hierarchical (Stackelberg) decision-making. We study N-robot forest-structured mixed-hierarchy games, in which each robot acts as a Stackelberg leader over its subtree while robots in different branches interact via Nash equilibria. We derive the Karush-Kuhn-Tucker (KKT) first-order optimality conditions for this class of games and show that they involve increasingly high-order derivatives of robots' best-response policies as the hierarchy depth grows, rendering a direct solution intractable. To overcome this challenge, we introduce a quasi-policy approximation that removes higher-order policy derivatives and develop an inexact Newton method for efficiently solving the resulting approximated KKT systems. We prove local exponential convergence of the proposed algorithm for games with non-quadratic objectives and nonlinear constraints. The approach is implemented in a highly optimized Julia library (MixedHierarchyGames.jl) and evaluated in simulated experiments, demonstrating real-time convergence for complex mixed-hierarchy information structures.",
          "authors": [
            "Hamzah Khan",
            "Dong Ho Lee",
            "Jingqi Li",
            "Tianyu Qiu",
            "Christian Ellis",
            "Jesse Milzman",
            "Wesley Suttle",
            "David Fridovich-Keil"
          ],
          "published": "2026-02-02T03:03:29Z",
          "updated": "2026-02-02T03:03:29Z",
          "primary_category": "cs.GT",
          "categories": [
            "cs.GT",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01568v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01536v1",
          "title": "UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning",
          "summary": "Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.",
          "authors": [
            "Shuai Liu",
            "Siheng Ren",
            "Xiaoyao Zhu",
            "Quanmin Liang",
            "Zefeng Li",
            "Qiang Li",
            "Xin Hu",
            "Kai Huang"
          ],
          "published": "2026-02-02T02:10:51Z",
          "updated": "2026-02-02T02:10:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01536v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01535v1",
          "title": "Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations",
          "summary": "While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.",
          "authors": [
            "Huzaifa Mustafa Unjhawala",
            "Khizar Shaikh",
            "Luning Bakke",
            "Radu Serban",
            "Dan Negrut"
          ],
          "published": "2026-02-02T02:10:34Z",
          "updated": "2026-02-02T02:10:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01535v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01515v1",
          "title": "RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots",
          "summary": "Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.",
          "authors": [
            "Humphrey Munn",
            "Brendan Tidd",
            "Peter Bohm",
            "Marcus Gallagher",
            "David Howard"
          ],
          "published": "2026-02-02T01:04:55Z",
          "updated": "2026-02-02T01:04:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01515v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01501v3",
          "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching",
          "summary": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.",
          "authors": [
            "Minwoo Jung",
            "Nived Chebrolu",
            "Lucas Carvalho de Lima",
            "Haedam Oh",
            "Maurice Fallon",
            "Ayoung Kim"
          ],
          "published": "2026-02-02T00:32:07Z",
          "updated": "2026-02-12T13:22:17Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01501v3",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01448v1",
          "title": "Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression",
          "summary": "This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable \"ring mechanism\", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.",
          "authors": [
            "Harshith Jella",
            "Pejman Kheradmand",
            "Joseph Klein",
            "Behnam Moradkhani",
            "Yash Chitalia"
          ],
          "published": "2026-02-01T21:33:47Z",
          "updated": "2026-02-01T21:33:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01448v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01429v1",
          "title": "Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors",
          "summary": "This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.",
          "authors": [
            "Gonzalo Olguin",
            "Javier Ruiz-del-Solar"
          ],
          "published": "2026-02-01T20:32:31Z",
          "updated": "2026-02-01T20:32:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01429v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01389v2",
          "title": "Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation",
          "summary": "Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.",
          "authors": [
            "Michele Antonazzi",
            "Lorenzo Signorelli",
            "Matteo Luperto",
            "Nicola Basilico"
          ],
          "published": "2026-02-01T18:49:03Z",
          "updated": "2026-02-15T00:08:05Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01389v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01385v1",
          "title": "TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design",
          "summary": "Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.",
          "authors": [
            "Xiangyu Li",
            "Mingwei Lai",
            "Mengke Zhang",
            "Junxiao Lin",
            "Tiancheng Lai",
            "Junping Zhi",
            "Chao Xu",
            "Fei Gao",
            "Yanjun Cao"
          ],
          "published": "2026-02-01T18:38:22Z",
          "updated": "2026-02-01T18:38:22Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01385v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01268v1",
          "title": "OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth",
          "summary": "Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.",
          "authors": [
            "Jaehyeon Cho",
            "Jhonghyun An"
          ],
          "published": "2026-02-01T14:57:33Z",
          "updated": "2026-02-01T14:57:33Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01268v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01266v1",
          "title": "Reinforcement Learning for Active Perception in Autonomous Navigation",
          "summary": "This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.",
          "authors": [
            "Grzegorz Malczyk",
            "Mihir Kulkarni",
            "Kostas Alexis"
          ],
          "published": "2026-02-01T14:54:40Z",
          "updated": "2026-02-01T14:54:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01266v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01226v1",
          "title": "SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models",
          "summary": "Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., \"Form a circle\") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.",
          "authors": [
            "Aditya Shibu",
            "Marah Saleh",
            "Mohamed Al-Musleh",
            "Nidhal Abdulaziz"
          ],
          "published": "2026-02-01T13:34:34Z",
          "updated": "2026-02-01T13:34:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01226v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01189v2",
          "title": "SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment",
          "summary": "We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.",
          "authors": [
            "Astik Srivastava",
            "Thomas J Chackenkulam",
            "Bitla Bhanu Teja",
            "Antony Thomas",
            "Madhava Krishna"
          ],
          "published": "2026-02-01T12:24:12Z",
          "updated": "2026-02-08T06:31:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01189v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01166v1",
          "title": "Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models",
          "summary": "Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \\href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.",
          "authors": [
            "Shuanghao Bai",
            "Jing Lyu",
            "Wanqi Zhou",
            "Zhe Li",
            "Dakai Wang",
            "Lei Xing",
            "Xiaoguang Zhao",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Cheng Chi",
            "Badong Chen",
            "Shanghang Zhang"
          ],
          "published": "2026-02-01T11:34:37Z",
          "updated": "2026-02-01T11:34:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01166v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01158v1",
          "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs",
          "summary": "Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\\% success rates to as low as 2\\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.",
          "authors": [
            "Daniel Yezid Guarnizo Orjuela",
            "Leonardo Scappatura",
            "Veronica Di Gennaro",
            "Riccardo Andrea Izzo",
            "Gianluca Bardaro",
            "Matteo Matteucci"
          ],
          "published": "2026-02-01T11:09:08Z",
          "updated": "2026-02-01T11:09:08Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01158v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01156v1",
          "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning",
          "summary": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.",
          "authors": [
            "Shunpeng Yang",
            "Ben Liu",
            "Hua Chen"
          ],
          "published": "2026-02-01T11:08:09Z",
          "updated": "2026-02-01T11:08:09Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01156v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01153v1",
          "title": "UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors",
          "summary": "Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.",
          "authors": [
            "Zhuo Chen",
            "Fei Ni",
            "Kaiyao Luo",
            "Zhiyuan Wu",
            "Xuyang Zhang",
            "Emmanouil Spyrakos-Papastavridis",
            "Lorenzo Jamone",
            "Nathan F. Lepora",
            "Jiankang Deng",
            "Shan Luo"
          ],
          "published": "2026-02-01T11:03:01Z",
          "updated": "2026-02-01T11:03:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01153v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01115v2",
          "title": "KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV",
          "summary": "Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \\href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\\textcolor{red}{link}}",
          "authors": [
            "Zhihao Chen",
            "Yiyuan Ge",
            "Ziyang Wang"
          ],
          "published": "2026-02-01T09:27:56Z",
          "updated": "2026-02-13T05:20:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01115v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01100v2",
          "title": "StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating",
          "summary": "Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a \"Lock-and-Gated\" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.",
          "authors": [
            "Tongqing Chen",
            "Hang Wu",
            "Jiasen Wang",
            "Xiaotao Li",
            "Lu Fang"
          ],
          "published": "2026-02-01T08:51:17Z",
          "updated": "2026-02-07T09:55:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01100v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01092v1",
          "title": "Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance",
          "summary": "Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE",
          "authors": [
            "Peng Zhou",
            "Zhongxuan Li",
            "Jinsong Wu",
            "Jiaming Qi",
            "Jun Hu",
            "David Navarro-Alarcon",
            "Jia Pan",
            "Lihua Xie",
            "Shiyao Zhang",
            "Zeqing Zhang"
          ],
          "published": "2026-02-01T08:16:48Z",
          "updated": "2026-02-01T08:16:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01092v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01085v1",
          "title": "Estimating Force Interactions of Deformable Linear Objects from their Shapes",
          "summary": "This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.",
          "authors": [
            "Qi Jing Chen",
            "Shilin Shan",
            "Timothy Bretl",
            "Quang-Cuong Pham"
          ],
          "published": "2026-02-01T08:02:30Z",
          "updated": "2026-02-01T08:02:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01085v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01067v1",
          "title": "A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation",
          "summary": "Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.",
          "authors": [
            "Fanqi Lin",
            "Kushal Arora",
            "Jean Mercat",
            "Haruki Nishimura",
            "Paarth Shah",
            "Chen Xu",
            "Mengchao Zhang",
            "Mark Zolotas",
            "Maya Angeles",
            "Owen Pfannenstiehl",
            "Andrew Beaulieu",
            "Jose Barreiros"
          ],
          "published": "2026-02-01T07:22:55Z",
          "updated": "2026-02-01T07:22:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01067v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02590v1",
          "title": "StepNav: Structured Trajectory Priors for Efficient and Multimodal Visual Navigation",
          "summary": "Visual navigation is fundamental to autonomous systems, yet generating reliable trajectories in cluttered and uncertain environments remains a core challenge. Recent generative models promise end-to-end synthesis, but their reliance on unstructured noise priors often yields unsafe, inefficient, or unimodal plans that cannot meet real-time requirements. We propose StepNav, a novel framework that bridges this gap by introducing structured, multimodal trajectory priors derived from variational principles. StepNav first learns a geometry-aware success probability field to identify all feasible navigation corridors. These corridors are then used to construct an explicit, multi-modal mixture prior that initializes a conditional flow-matching process. This refinement is formulated as an optimal control problem with explicit smoothness and safety regularization. By replacing unstructured noise with physically-grounded candidates, StepNav generates safer and more efficient plans in significantly fewer steps. Experiments in both simulation and real-world benchmarks demonstrate consistent improvements in robustness, efficiency, and safety over state-of-the-art generative planners, advancing reliable trajectory generation for practical autonomous navigation. The code has been released at https://github.com/LuoXubo/StepNav.",
          "authors": [
            "Xubo Luo",
            "Aodi Wu",
            "Haodong Han",
            "Xue Wan",
            "Wei Zhang",
            "Leizheng Shu",
            "Ruisuo Wang"
          ],
          "published": "2026-02-01T06:45:42Z",
          "updated": "2026-02-01T06:45:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02590v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01041v1",
          "title": "LLM-Based Behavior Tree Generation for Construction Machinery",
          "summary": "Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.",
          "authors": [
            "Akinosuke Tsutsumi",
            "Tomoya Itsuka",
            "Yuichiro Kasahara",
            "Tomoya Kouno",
            "Kota Akinari",
            "Genki Yamauchi",
            "Daisuke Endo",
            "Taro Abe",
            "Takeshi Hashimoto",
            "Keiji Nagatani",
            "Ryo Kurazume"
          ],
          "published": "2026-02-01T06:03:16Z",
          "updated": "2026-02-01T06:03:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01041v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01040v1",
          "title": "Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration",
          "summary": "Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.",
          "authors": [
            "Yuhang Zhang",
            "Chao Yan",
            "Jiaxi Yu",
            "Jiaping Xiao",
            "Mir Feroskhan"
          ],
          "published": "2026-02-01T06:01:15Z",
          "updated": "2026-02-01T06:01:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01040v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.01018v1",
          "title": "Offline Discovery of Interpretable Skills from Multi-Task Trajectories",
          "summary": "Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.",
          "authors": [
            "Chongyu Zhu",
            "Mithun Vanniasinghe",
            "Jiayu Chen",
            "Chi-Guhn Lee"
          ],
          "published": "2026-02-01T05:03:58Z",
          "updated": "2026-02-01T05:03:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.01018v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00993v1",
          "title": "HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving",
          "summary": "End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.",
          "authors": [
            "Weizhe Tang",
            "Junwei You",
            "Jiaxi Liu",
            "Zhaoyi Wang",
            "Rui Gan",
            "Zilin Huang",
            "Feng Wei",
            "Bin Ran"
          ],
          "published": "2026-02-01T03:15:08Z",
          "updated": "2026-02-01T03:15:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00993v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00992v1",
          "title": "Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds",
          "summary": "In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.",
          "authors": [
            "Phone Thiha Kyaw",
            "Jonathan Kelly"
          ],
          "published": "2026-02-01T03:14:46Z",
          "updated": "2026-02-01T03:14:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00992v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00982v1",
          "title": "Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025",
          "summary": "Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.",
          "authors": [
            "Phu-Hoa Pham",
            "Chi-Nguyen Tran",
            "Dao Sy Duy Minh",
            "Nguyen Lam Phu Quy",
            "Huynh Trung Kiet"
          ],
          "published": "2026-02-01T02:44:52Z",
          "updated": "2026-02-01T02:44:52Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.NE",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00982v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00980v1",
          "title": "Meanshift Shape Formation Control Using Discrete Mass Distribution",
          "summary": "The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.",
          "authors": [
            "Yichen Cai",
            "Yuan Gao",
            "Pengpeng Li",
            "Wei Wang",
            "Guibin Sun",
            "Jinhu Lü"
          ],
          "published": "2026-02-01T02:44:00Z",
          "updated": "2026-02-01T02:44:00Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00980v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00937v1",
          "title": "CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining",
          "summary": "Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.",
          "authors": [
            "I-Chun Arthur Liu",
            "Krzysztof Choromanski",
            "Sandy Huang",
            "Connor Schenck"
          ],
          "published": "2026-01-31T23:32:54Z",
          "updated": "2026-01-31T23:32:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00937v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00935v1",
          "title": "Minimal Footprint Grasping Inspired by Ants",
          "summary": "Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.",
          "authors": [
            "Mohamed Sorour",
            "Barbara Webb"
          ],
          "published": "2026-01-31T23:24:54Z",
          "updated": "2026-01-31T23:24:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00935v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00923v1",
          "title": "SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation",
          "summary": "The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\\%$ in simulated cluttered environments and $72.0\\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.",
          "authors": [
            "Jincheng Wang",
            "Lingfan Bao",
            "Tong Yang",
            "Diego Martinez Plasencia",
            "Jianhao Jiao",
            "Dimitrios Kanoulas"
          ],
          "published": "2026-01-31T22:37:27Z",
          "updated": "2026-01-31T22:37:27Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00923v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00919v1",
          "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
          "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.",
          "authors": [
            "I. Apanasevich",
            "M. Artemyev",
            "R. Babakyan",
            "P. Fedotova",
            "D. Grankin",
            "E. Kupryashin",
            "A. Misailidi",
            "D. Nerus",
            "A. Nutalapati",
            "G. Sidorov",
            "I. Efremov",
            "M. Gerasyov",
            "D. Pikurov",
            "Y. Senchenko",
            "S. Davidenko",
            "D. Kulikov",
            "M. Sultankin",
            "K. Askarbek",
            "O. Shamanin",
            "D. Statovoy",
            "E. Zalyaev",
            "I. Zorin",
            "A. Letkin",
            "E. Rusakov",
            "A. Silchenko",
            "V. Vorobyov",
            "S. Sobolnikov",
            "A. Postnikov"
          ],
          "published": "2026-01-31T22:13:23Z",
          "updated": "2026-01-31T22:13:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00919v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00915v1",
          "title": "UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation",
          "summary": "Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \\textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.",
          "authors": [
            "Zhiyuan Wu",
            "Xiangyu Zhang",
            "Zhuo Chen",
            "Jiankang Deng",
            "Rolandos Alexandros Potamias",
            "Shan Luo"
          ],
          "published": "2026-01-31T21:56:16Z",
          "updated": "2026-01-31T21:56:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00915v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00886v1",
          "title": "RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback",
          "summary": "Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.",
          "authors": [
            "Amitesh Vatsa",
            "Zhixian Xie",
            "Wanxin Jin"
          ],
          "published": "2026-01-31T20:17:15Z",
          "updated": "2026-01-31T20:17:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00886v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00877v1",
          "title": "Learning When to Jump for Off-road Navigation",
          "summary": "Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.",
          "authors": [
            "Zhipeng Zhao",
            "Taimeng Fu",
            "Shaoshu Su",
            "Qiwei Du",
            "Ehsan Tarkesh Esfahani",
            "Karthik Dantu",
            "Souma Chowdhury",
            "Chen Wang"
          ],
          "published": "2026-01-31T19:41:09Z",
          "updated": "2026-01-31T19:41:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00877v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00868v1",
          "title": "Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects",
          "summary": "Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.",
          "authors": [
            "Nikhil Uday Shinde",
            "Dylan Hirsch",
            "Michael C. Yip",
            "Sylvia Herbert"
          ],
          "published": "2026-01-31T19:07:10Z",
          "updated": "2026-01-31T19:07:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00868v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00823v1",
          "title": "Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation",
          "summary": "Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the \"helpfulness\" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative \"gliding\". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.",
          "authors": [
            "Spyridon Syntakas",
            "Kostas Vlachos"
          ],
          "published": "2026-01-31T17:26:08Z",
          "updated": "2026-01-31T17:26:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00823v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00814v2",
          "title": "SyNeT: Synthetic Negatives for Traversability Learning",
          "summary": "Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos will be publicly available.",
          "authors": [
            "Bomena Kim",
            "Hojun Lee",
            "Younsoo Park",
            "Yaoyu Hu",
            "Sebastian Scherer",
            "Inwook Shim"
          ],
          "published": "2026-01-31T16:47:26Z",
          "updated": "2026-02-03T03:56:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00814v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00810v1",
          "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization",
          "summary": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.",
          "authors": [
            "Ze Huang",
            "Zhongyang Xiao",
            "Mingliang Song",
            "Longan Yang",
            "Hongyuan Yuan",
            "Li Sun"
          ],
          "published": "2026-01-31T16:37:30Z",
          "updated": "2026-01-31T16:37:30Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00810v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00808v1",
          "title": "Physics-informed Diffusion Mamba Transformer for Real-world Driving",
          "summary": "Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning.",
          "authors": [
            "Hang Zhou",
            "Qiang Zhang",
            "Peiran Liu",
            "Yihao Qin",
            "Zhaoxu Yan",
            "Yiding Ji"
          ],
          "published": "2026-01-31T16:35:54Z",
          "updated": "2026-01-31T16:35:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00808v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00807v1",
          "title": "Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds",
          "summary": "Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.",
          "authors": [
            "Xianzhe Fan",
            "Shengliang Deng",
            "Xiaoyang Wu",
            "Yuxiang Lu",
            "Zhuoling Li",
            "Mi Yan",
            "Yujia Zhang",
            "Zhizheng Zhang",
            "He Wang",
            "Hengshuang Zhao"
          ],
          "published": "2026-01-31T16:34:52Z",
          "updated": "2026-01-31T16:34:52Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00807v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00743v1",
          "title": "SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning",
          "summary": "Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \\textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \\textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.",
          "authors": [
            "Xu Pan",
            "Zhenglin Wan",
            "Xingrui Yu",
            "Xianwei Zheng",
            "Youkai Ke",
            "Ming Sun",
            "Rui Wang",
            "Ziwei Wang",
            "Ivor Tsang"
          ],
          "published": "2026-01-31T14:16:24Z",
          "updated": "2026-01-31T14:16:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00743v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00708v2",
          "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
          "summary": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.",
          "authors": [
            "Weiqi Gai",
            "Yuman Gao",
            "Yuan Zhou",
            "Yufan Xie",
            "Zhiyang Liu",
            "Yuze Wu",
            "Xin Zhou",
            "Fei Gao",
            "Zhijun Meng"
          ],
          "published": "2026-01-31T13:10:02Z",
          "updated": "2026-02-03T05:20:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00708v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00686v1",
          "title": "Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching",
          "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.",
          "authors": [
            "Yujie Wei",
            "Jiahan Fan",
            "Jiyu Guo",
            "Ruichen Zhen",
            "Rui Shao",
            "Xiu Su",
            "Zeke Xie",
            "Shuo Yang"
          ],
          "published": "2026-01-31T12:12:51Z",
          "updated": "2026-01-31T12:12:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00686v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00678v2",
          "title": "Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion",
          "summary": "Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.",
          "authors": [
            "Tianyang Wu",
            "Hanwei Guo",
            "Yuhang Wang",
            "Junshu Yang",
            "Xinyang Sui",
            "Jiayi Xie",
            "Xingyu Chen",
            "Zeyang Liu",
            "Xuguang Lan"
          ],
          "published": "2026-01-31T11:50:16Z",
          "updated": "2026-02-10T13:18:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00678v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00675v1",
          "title": "Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction",
          "summary": "Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.",
          "authors": [
            "Valerio Belcamino",
            "Mariya Kilina",
            "Alessandro Carfì",
            "Valeria Seidita",
            "Fulvio Mastrogiovanni",
            "Antonio Chella"
          ],
          "published": "2026-01-31T11:42:33Z",
          "updated": "2026-01-31T11:42:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00675v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00575v1",
          "title": "Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction",
          "summary": "Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically \"easy to verify but hard to solve\", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.",
          "authors": [
            "Chaoqun Cui",
            "Jing Huang",
            "Shijing Wang",
            "Liming Zheng",
            "Qingchao Kong",
            "Zhixiong Zeng"
          ],
          "published": "2026-01-31T07:36:54Z",
          "updated": "2026-01-31T07:36:54Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00575v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00566v1",
          "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
          "summary": "Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.",
          "authors": [
            "Nan Song",
            "Junzhe Jiang",
            "Jingyu Li",
            "Xiatian Zhu",
            "Li Zhang"
          ],
          "published": "2026-01-31T07:12:26Z",
          "updated": "2026-01-31T07:12:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00566v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00557v1",
          "title": "ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation",
          "summary": "Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.",
          "authors": [
            "Weisheng Dai",
            "Kai Lan",
            "Jianyi Zhou",
            "Bo Zhao",
            "Xiu Su",
            "Junwen Tong",
            "Weili Guan",
            "Shuo Yang"
          ],
          "published": "2026-01-31T06:40:57Z",
          "updated": "2026-01-31T06:40:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00557v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00551v1",
          "title": "APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation",
          "summary": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{https://github.com/4amGodvzx/apex}{GitHub}",
          "authors": [
            "Daoxuan Zhang",
            "Ping Chen",
            "Xiaobo Xia",
            "Xiu Su",
            "Ruichen Zhen",
            "Jianqiang Xiao",
            "Shuo Yang"
          ],
          "published": "2026-01-31T06:27:57Z",
          "updated": "2026-01-31T06:27:57Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00551v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00514v2",
          "title": "A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation",
          "summary": "Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.",
          "authors": [
            "Yaohua Liu",
            "Binkai Ou",
            "Zicheng Qiu",
            "Ce Hao",
            "Hengjun Zhang"
          ],
          "published": "2026-01-31T05:09:21Z",
          "updated": "2026-02-03T08:06:28Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00514v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00500v1",
          "title": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
          "summary": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.",
          "authors": [
            "Jianyi Zhou",
            "Yujie Wei",
            "Ruichen Zhen",
            "Bo Zhao",
            "Xiaobo Xia",
            "Rui Shao",
            "Xiu Su",
            "Shuo Yang"
          ],
          "published": "2026-01-31T03:59:07Z",
          "updated": "2026-01-31T03:59:07Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00500v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00480v1",
          "title": "FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control",
          "summary": "Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm \"flows\" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.",
          "authors": [
            "Mohini Priya Kolluri",
            "Ammar Waheed",
            "Zohaib Hasnain"
          ],
          "published": "2026-01-31T03:04:38Z",
          "updated": "2026-01-31T03:04:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00480v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00475v1",
          "title": "Parallel Stochastic Gradient-Based Planning for World Models",
          "summary": "World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables (\"virtual states\") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.",
          "authors": [
            "Michael Psenka",
            "Michael Rabbat",
            "Aditi Krishnapriyan",
            "Yann LeCun",
            "Amir Bar"
          ],
          "published": "2026-01-31T02:57:47Z",
          "updated": "2026-01-31T02:57:47Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00475v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00466v1",
          "title": "Stealthy Coverage Control for Human-enabled Real-Time 3D Reconstruction",
          "summary": "In this paper, we propose a novel semi-autonomous image sampling strategy, called stealthy coverage control, for human-enabled 3D structure reconstruction. The present mission involves a fundamental problem: while the number of images required to accurately reconstruct a 3D model depends on the structural complexity of the target scene to be reconstructed, it is not realistic to assume prior knowledge of the spatially non-uniform structural complexity. We approach this issue by leveraging human flexible reasoning and situational recognition capabilities. Specifically, we design a semi-autonomous system that leaves identification of regions that need more images and navigation of the drones to such regions to a human operator. To this end, we first present a way to reflect the human intention in autonomous coverage control. Subsequently, in order to avoid operational conflicts between manual control and autonomous coverage control, we develop the stealthy coverage control that decouples the drone motion for efficient image sampling from navigation by the human. Simulation studies on a Unity/ROS2-based simulator demonstrate that the present semi-autonomous system outperforms the one without human interventions in the sense of the reconstructed model quality.",
          "authors": [
            "Reiji Terunuma",
            "Yuta Nakamura",
            "Takuma Abe",
            "Takeshi Hatanaka"
          ],
          "published": "2026-01-31T02:41:13Z",
          "updated": "2026-01-31T02:41:13Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00466v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15873v1",
          "title": "Test-Time Adaptation for Tactile-Vision-Language Models",
          "summary": "Tactile-vision-language (TVL) models are increasingly deployed in real-world robotic and multimodal perception tasks, where test-time distribution shifts are unavoidable. Existing test-time adaptation (TTA) methods provide filtering in unimodal settings but lack explicit treatment of modality-wise reliability under asynchronous cross-modal shifts, leaving them brittle when some modalities become unreliable. We study TTA for TVL models under such shifts and propose a reliability-aware framework that estimates per-modality reliability from prediction uncertainty and perturbation-based responses. This shared reliability signal is used to (i) filter unreliable test samples, (ii) adaptively fuse tactile, visual, and language features, and (iii) regularize test-time optimization with a reliability-guided objective. On the TAG-C benchmark and additional TVL scenarios, our approach consistently outperforms strong TTA baselines, achieving accuracy gains of up to 49.9\\% under severe modality corruptions, underscoring the importance of explicit modality-wise reliability modeling for robust test-time adaptation.",
          "authors": [
            "Chuyang Ye",
            "Haoxian Jing",
            "Qinting Jiang",
            "Yixi Lin",
            "Qiang Li",
            "Xing Tang",
            "Jingyan Jiang"
          ],
          "published": "2026-01-31T02:26:01Z",
          "updated": "2026-01-31T02:26:01Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15873v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00458v1",
          "title": "LatentTrack: Sequential Weight Generation via Latent Filtering",
          "summary": "We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates. At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.",
          "authors": [
            "Omer Haq"
          ],
          "published": "2026-01-31T02:22:59Z",
          "updated": "2026-01-31T02:22:59Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO",
            "stat.ML"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00458v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00440v1",
          "title": "DISK: Dynamic Inference SKipping for World Models",
          "summary": "We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.",
          "authors": [
            "Anugunj Naman",
            "Gaibo Zhang",
            "Ayushman Singh",
            "Yaguang Zhang"
          ],
          "published": "2026-01-31T01:28:27Z",
          "updated": "2026-01-31T01:28:27Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00440v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00401v1",
          "title": "ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control",
          "summary": "Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.",
          "authors": [
            "Jean Pierre Sleiman",
            "He Li",
            "Alphonsus Adu-Bredu",
            "Robin Deits",
            "Arun Kumar",
            "Kevin Bergamin",
            "Mohak Bhardwaj",
            "Scott Biddlestone",
            "Nicola Burger",
            "Matthew A. Estrada",
            "Francesco Iacobelli",
            "Twan Koolen",
            "Alexander Lambert",
            "Erica Lin",
            "M. Eva Mungai",
            "Zach Nobles",
            "Shane Rozen-Levy",
            "Yuyao Shi",
            "Jiashun Wang",
            "Jakob Welner",
            "Fangzhou Yu",
            "Mike Zhang",
            "Alfred Rizzi",
            "Jessica Hodgins",
            "Sylvain Bertrand",
            "Yeuhi Abe",
            "Scott Kuindersma",
            "Farbod Farshidian"
          ],
          "published": "2026-01-30T23:35:02Z",
          "updated": "2026-01-30T23:35:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00401v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00325v1",
          "title": "Motion Planning with Metric Temporal Logic Using Reachability Analysis and Hybrid Zonotopes",
          "summary": "Metric temporal logic (MTL) provides a formal framework for defining time-dependent mission requirements on autonomous vehicles. However, optimizing control decisions subject to these constraints is often computationally expensive. This article presents a method that uses reachability analysis to implicitly express the set of states satisfying an MTL specification and then optimizes to find a motion plan. The hybrid zonotope set representation is used to efficiently and conveniently encode MTL specifications into reachable sets. A numerical benchmark highlights the proposed method's computational advantages as compared to existing methods in the literature. Further numerical examples and an experimental application demonstrate the ability to address time-varying environments, region-dependent disturbances, and multi-agent coordination.",
          "authors": [
            "Andrew F. Thompson",
            "Joshua A. Robbins",
            "Jonah J. Glunt",
            "Sean B. Brennan",
            "Herschel C. Pangborn"
          ],
          "published": "2026-01-30T21:25:26Z",
          "updated": "2026-01-30T21:25:26Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00325v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00324v1",
          "title": "Dual Quaternion SE(3) Synchronization with Recovery Guarantees",
          "summary": "Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods.",
          "authors": [
            "Jianing Zhao",
            "Linglingzhi Zhu",
            "Anthony Man-Cho So"
          ],
          "published": "2026-01-30T21:24:20Z",
          "updated": "2026-01-30T21:24:20Z",
          "primary_category": "math.OC",
          "categories": [
            "math.OC",
            "cs.CV",
            "cs.RO",
            "eess.SP"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00324v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00270v1",
          "title": "RVDebloater: Mode-based Adaptive Firmware Debloating for Robotic Vehicles",
          "summary": "As the number of embedded devices grows and their functional requirements increase, embedded firmware is becoming increasingly larger, thereby expanding its attack surface. Despite the increase in firmware size, many embedded devices, such as robotic vehicles (RVs), operate in distinct modes, each requiring only a small subset of the firmware code at runtime. We refer to such devices as mode-based embedded devices. Debloating is an approach to reduce attack surfaces by removing or restricting unneeded code, but existing techniques suffer from significant limitations, such as coarse granularity and irreversible code removal, limiting their applicability. To address these limitations, we propose RVDebloater, a novel adaptive debloating technique for mode-based embedded devices that automatically identifies unneeded firmware code for each mode using either static or dynamic analysis, and dynamically debloats the firmware for each mode at the function level at runtime. RVDebloater introduces a new software-based enforcement approach that supports diverse mode-based embedded devices. We implemented RVDebloater using the LLVM compiler and evaluated its efficiency and effectiveness on six different RVs, including both simulated and real ones, with different real-world missions. We find that device requirements change throughout its lifetime for each mode, and that many critical firmware functions can be restricted in other modes, with an average of 85% of functions not being required. The results showed that none of the missions failed after debloating with RVDebloater, indicating that it neither incurred false positives nor false negatives. Further, RVDebloater prunes the firmware call graph by an average of 45% across different firmware. Finally, RVDebloater incurred an average performance overhead of 3.9% and memory overhead of 4% (approximately 0.25 MB) on real RVs.",
          "authors": [
            "Mohsen Salehi",
            "Karthik Pattabiraman"
          ],
          "published": "2026-01-30T19:46:08Z",
          "updated": "2026-01-30T19:46:08Z",
          "primary_category": "cs.CR",
          "categories": [
            "cs.CR",
            "cs.RO",
            "cs.SE"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00270v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.23285v1",
          "title": "End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms",
          "summary": "Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline conditions collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Our results confirmed that integrated optimization is most beneficial in complex, goal-ambiguous scenarios, and is generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.",
          "authors": [
            "MH Farhadi",
            "Ali Rabiee",
            "Sima Ghafoori",
            "Anna Cetera",
            "Andrew Fisher",
            "Reza Abiri"
          ],
          "published": "2026-01-30T18:59:16Z",
          "updated": "2026-01-30T18:59:16Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.23285v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.23266v1",
          "title": "IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models",
          "summary": "This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96\\% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available.",
          "authors": [
            "Seyed Ahmad Hosseini Miangoleh",
            "Amin Jalal Aghdasian",
            "Farzaneh Abdollahi"
          ],
          "published": "2026-01-30T18:34:10Z",
          "updated": "2026-01-30T18:34:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.23266v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00222v2",
          "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
          "summary": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.",
          "authors": [
            "Guoxin Lian",
            "Shuo Wang",
            "Yucheng Wang",
            "Yongcai Wang",
            "Maiyue Chen",
            "Kaihui Wang",
            "Bo Zhang",
            "Zhizhong Su",
            "Deying Li",
            "Zhaoxin Fan"
          ],
          "published": "2026-01-30T17:33:16Z",
          "updated": "2026-02-03T09:27:42Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00222v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.23107v1",
          "title": "FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows",
          "summary": "Accurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor-to-sensor errors without considering the miscalibration of individual sensors that cause these errors in the first place. We introduce FlowCalib, the first framework that detects LiDAR-to-vehicle miscalibration using motion cues from the scene flow of static objects. Our approach leverages the systematic bias induced by rotational misalignment in the flow field generated from sequential 3D point clouds, eliminating the need for additional sensors. The architecture integrates a neural scene flow prior for flow estimation and incorporates a dual-branch detection network that fuses learned global flow features with handcrafted geometric descriptors. These combined representations allow the system to perform two complementary binary classification tasks: a global binary decision indicating whether misalignment is present and separate, axis-specific binary decisions indicating whether each rotational axis is misaligned. Experiments on the nuScenes dataset demonstrate FlowCalib's ability to robustly detect miscalibration, establishing a benchmark for sensor-to-vehicle miscalibration detection.",
          "authors": [
            "Ilir Tahiraj",
            "Peter Wittal",
            "Markus Lienkamp"
          ],
          "published": "2026-01-30T15:53:16Z",
          "updated": "2026-01-30T15:53:16Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.23107v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.23087v2",
          "title": "CoLA-Flow Policy: Temporally Coherent Imitation Learning via Continuous Latent Action Flow Matching for Robotic Manipulation",
          "summary": "Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference latency, while flow matching enables fast one-step generation yet often leads to unstable execution when applied directly in the raw action space. We propose LG-Flow Policy, a trajectory-level imitation learning framework that performs flow matching in a continuous latent action space. By encoding action sequences into temporally regularized latent trajectories and learning an explicit latent-space flow, the proposed approach decouples global motion structure from low-level control noise, resulting in smooth and reliable long-horizon execution. LG-Flow Policy further incorporates geometry-aware point cloud conditioning and execution-time multimodal modulation, with visual cues evaluated as a representative modality in real-world settings. Experimental results in simulation and on physical robot platforms demonstrate that LG-Flow Policy achieves near single-step inference, substantially improves trajectory smoothness and task success over flow-based baselines operating in the raw action space, and remains significantly more efficient than diffusion-based policies.",
          "authors": [
            "Wu Songwei",
            "Jiang Zhiduo",
            "Xie Guanghu",
            "Sun Wandong",
            "Liu Hong",
            "Liu Yang"
          ],
          "published": "2026-01-30T15:36:43Z",
          "updated": "2026-02-10T10:16:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.23087v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.23080v1",
          "title": "Robust and Generalized Humanoid Motion Tracking",
          "summary": "Learning a general humanoid whole-body controller is challenging because practical reference motions can exhibit noise and inconsistencies after being transferred to the robot domain, and local defects may be amplified by closed-loop execution, causing drift or failure in highly dynamic and contact-rich behaviors. We propose a dynamics-conditioned command aggregation framework that uses a causal temporal encoder to summarize recent proprioception and a multi-head cross-attention command encoder to selectively aggregate a context window based on the current dynamics. We further integrate a fall recovery curriculum with random unstable initialization and an annealed upward assistance force to improve robustness and disturbance rejection. The resulting policy requires only about 3.5 hours of motion data and supports single-stage end-to-end training without distillation. The proposed method is evaluated under diverse reference inputs and challenging motion regimes, demonstrating zero-shot transfer to unseen motions as well as robust sim-to-real transfer on a physical humanoid robot.",
          "authors": [
            "Yubiao Ma",
            "Han Yu",
            "Jiayin Xie",
            "Changtai Lv",
            "Qiang Luo",
            "Chi Zhang",
            "Yunpeng Yin",
            "Boyang Xing",
            "Xuemei Ren",
            "Dongdong Zheng"
          ],
          "published": "2026-01-30T15:27:43Z",
          "updated": "2026-01-30T15:27:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.23080v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.23075v1",
          "title": "RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning",
          "summary": "On-policy deep reinforcement learning remains a dominant paradigm for continuous control, yet standard implementations rely on Gaussian actors and relatively shallow MLP policies, often leading to brittle optimization when gradients are noisy and policy updates must be conservative. In this paper, we revisit policy representation as a first-class design choice for on-policy optimization. We study discretized categorical actors that represent each action dimension with a distribution over bins, yielding a policy objective that resembles a cross-entropy loss. Building on architectural advances from supervised learning, we further propose regularized actor networks, while keeping critic design fixed. Our results show that simply replacing the standard actor network with our discretized regularized actor yields consistent gains and achieve the state-of-the-art performance across diverse continuous-control benchmarks.",
          "authors": [
            "Yuexin Bian",
            "Jie Feng",
            "Tao Wang",
            "Yijiang Li",
            "Sicun Gao",
            "Yuanyuan Shi"
          ],
          "published": "2026-01-30T15:24:34Z",
          "updated": "2026-01-30T15:24:34Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.23075v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.23038v1",
          "title": "MOSAIC: Modular Scalable Autonomy for Intelligent Coordination of Heterogeneous Robotic Teams",
          "summary": "Mobile robots have become indispensable for exploring hostile environments, such as in space or disaster relief scenarios, but often remain limited to teleoperation by a human operator. This restricts the deployment scale and requires near-continuous low-latency communication between the operator and the robot. We present MOSAIC: a scalable autonomy framework for multi-robot scientific exploration using a unified mission abstraction based on Points of Interest (POIs) and multiple layers of autonomy, enabling supervision by a single operator. The framework dynamically allocates exploration and measurement tasks based on each robot's capabilities, leveraging team-level redundancy and specialization to enable continuous operation. We validated the framework in a space-analog field experiment emulating a lunar prospecting scenario, involving a heterogeneous team of five robots and a single operator. Despite the complete failure of one robot during the mission, the team completed 82.3% of assigned tasks at an Autonomy Ratio of 86%, while the operator workload remained at only 78.2%. These results demonstrate that the proposed framework enables robust, scalable multi-robot scientific exploration with limited operator intervention. We further derive practical lessons learned in robot interoperability, networking architecture, team composition, and operator workload management to inform future multi-robot exploration missions.",
          "authors": [
            "David Oberacker",
            "Julia Richer",
            "Philip Arm",
            "Marvin Grosse Besselmann",
            "Lennart Puck",
            "William Talbot",
            "Maximilian Schik",
            "Sabine Bellmann",
            "Tristan Schnell",
            "Hendrik Kolvenbach",
            "Rüdiger Dillmann",
            "Marco Hutter",
            "Arne Roennau"
          ],
          "published": "2026-01-30T14:46:15Z",
          "updated": "2026-01-30T14:46:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.23038v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22988v1",
          "title": "Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation",
          "summary": "Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including reliance on multi-view observations during inference which is impractical in single-view restricted scenarios, incomplete scene modeling that fails to capture holistic and fine-grained geometric structures essential for precise manipulation, and lack of effective policy training strategies to retain and exploit the acquired 3D knowledge. To address these challenges, we present MethodName, a unified representation-policy learning framework for view-generalizable robotic manipulation. MethodName introduces a single-view 3D pretraining paradigm that leverages point cloud reconstruction and feed-forward gaussian splatting under multi-view supervision to learn holistic geometric representations. During policy learning, MethodName performs multi-step distillation to preserve the pretrained geometric understanding and effectively transfer it to manipulation skills. We conduct experiments on 12 RLBench tasks, where our approach outperforms the previous state-of-the-art method by 12.7% in average success rate. Further evaluation on six representative tasks demonstrates strong zero-shot view generalization, with success rate drops of only 22.0% and 29.7% under moderate and large viewpoint shifts respectively, whereas the state-of-the-art method suffers larger decreases of 41.6% and 51.5%.",
          "authors": [
            "Di Zhang",
            "Weicheng Duan",
            "Dasen Gu",
            "Hongye Lu",
            "Hai Zhang",
            "Hang Yu",
            "Junqiao Zhao",
            "Guang Chen"
          ],
          "published": "2026-01-30T13:53:53Z",
          "updated": "2026-01-30T13:53:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22988v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22982v1",
          "title": "About an Automating Annotation Method for Robot Markers",
          "summary": "Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.",
          "authors": [
            "Wataru Uemura",
            "Takeru Nagashima"
          ],
          "published": "2026-01-30T13:44:56Z",
          "updated": "2026-01-30T13:44:56Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22982v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22965v1",
          "title": "Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation",
          "summary": "Diffusion policies (DP) have demonstrated significant potential in visual navigation by capturing diverse multi-modal trajectory distributions. However, standard imitation learning (IL), which most DP methods rely on for training, often inherits sub-optimality and redundancy from expert demonstrations, thereby necessitating a computationally intensive \"generate-then-filter\" pipeline that relies on auxiliary selectors during inference. To address these challenges, we propose Self-Imitated Diffusion Policy (SIDP), a novel framework that learns improved planning by selectively imitating a set of trajectories sampled from itself. Specifically, SIDP introduces a reward-guided self-imitation mechanism that encourages the policy to consistently produce high-quality trajectories efficiently, rather than outputs of inconsistent quality, thereby reducing reliance on extensive sampling and post-filtering. During training, we employ a reward-driven curriculum learning paradigm to mitigate inefficient data utility, and goal-agnostic exploration for trajectory augmentation to improve planning robustness. Extensive evaluations on a comprehensive simulation benchmark show that SIDP significantly outperforms previous methods, with real-world experiments confirming its effectiveness across multiple robotic platforms. On Jetson Orin Nano, SIDP delivers a 2.5$\\times$ faster inference than the baseline NavDP, i.e., 110ms VS 273ms, enabling efficient real-time deployment.",
          "authors": [
            "Runhua Zhang",
            "Junyi Hou",
            "Changxu Cheng",
            "Qiyi Chen",
            "Tao Wang",
            "Wuyue Zhao"
          ],
          "published": "2026-01-30T13:27:59Z",
          "updated": "2026-01-30T13:27:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22965v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22930v1",
          "title": "MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving",
          "summary": "Trajectory planning is a core task in autonomous driving, requiring the prediction of safe and comfortable paths across diverse scenarios. Integrating Multi-modal Large Language Models (MLLMs) with Reinforcement Learning (RL) has shown promise in addressing \"long-tail\" scenarios. However, existing methods are constrained to single-turn reasoning, limiting their ability to handle complex tasks requiring iterative refinement. To overcome this limitation, we present MTDrive, a multi-turn framework that enables MLLMs to iteratively refine trajectories based on environmental feedback. MTDrive introduces Multi-Turn Group Relative Policy Optimization (mtGRPO), which mitigates reward sparsity by computing relative advantages across turns. We further construct an interactive trajectory understanding dataset from closed-loop simulation to support multi-turn training. Experiments on the NAVSIM benchmark demonstrate superior performance compared to existing methods, validating the effectiveness of our multi-turn reasoning paradigm. Additionally, we implement system-level optimizations to reduce data transfer overhead caused by high-resolution images and multi-turn sequences, achieving 2.5x training throughput. Our data, models, and code will be made available soon.",
          "authors": [
            "Xidong Li",
            "Mingyu Guo",
            "Chenchao Xu",
            "Bailin Li",
            "Wenjing Zhu",
            "Yangang Zou",
            "Rui Chen",
            "Zehuan Wang"
          ],
          "published": "2026-01-30T12:47:55Z",
          "updated": "2026-01-30T12:47:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22930v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22927v1",
          "title": "Toward Fully Autonomous Driving: AI, Challenges, Opportunities, and Needs",
          "summary": "Automated driving (AD) is promising, but the transition to fully autonomous driving is, among other things, subject to the real, ever-changing open world and the resulting challenges. However, research in the field of AD demonstrates the ability of artificial intelligence (AI) to outperform classical approaches, handle higher complexities, and reach a new level of autonomy. At the same time, the use of AI raises further questions of safety and transferability. To identify the challenges and opportunities arising from AI concerning autonomous driving functionalities, we have analyzed the current state of AD, outlined limitations, and identified foreseeable technological possibilities. Thereby, various further challenges are examined in the context of prospective developments. In this way, this article reconsiders fully autonomous driving with respect to advancements in the field of AI and carves out the respective needs and resulting research questions.",
          "authors": [
            "Lars Ullrich",
            "Michael Buchholz",
            "Klaus Dietmayer",
            "Knut Graichen"
          ],
          "published": "2026-01-30T12:45:44Z",
          "updated": "2026-01-30T12:45:44Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.ET"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22927v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22849v1",
          "title": "Robust Rigid Body Assembly via Contact-Implicit Optimal Control with Exact Second-Order Derivatives",
          "summary": "Efficient planning of assembly motions is a long standing challenge in the field of robotics that has been primarily tackled with reinforcement learning and sampling-based methods by using extensive physics simulations. This paper proposes a sample-efficient robust optimal control approach for the determination of assembly motions, which requires significantly less physics simulation steps during planning through the efficient use of derivative information. To this end, a differentiable physics simulation is constructed that provides second-order analytic derivatives to the numerical solver and allows one to traverse seamlessly from informative derivatives to accurate contact simulation. The solution of the physics simulation problem is made differentiable by using smoothing inspired by interior-point methods applied to both the collision detection as well as the contact resolution problem. We propose a modified variant of an optimization-based formulation of collision detection formulated as a linear program and present an efficient implementation for the nominal evaluation and corresponding first- and second-order derivatives. Moreover, a multi-scenario-based trajectory optimization problem that ensures robustness with respect to sim-to-real mismatches is derived. The capability of the considered formulation is illustrated by results where over 99\\% successful executions are achieved in real-world experiments. Thereby, we carefully investigate the effect of smooth approximations of the contact dynamics and robust modeling on the success rates. Furthermore, the method's capability is tested on different peg-in-hole problems in simulation to show the benefit of using exact Hessians over commonly used Hessian approximations.",
          "authors": [
            "Christian Dietz",
            "Sebastian Albrecht",
            "Gianluca Frison",
            "Moritz Diehl",
            "Armin Nurkanović"
          ],
          "published": "2026-01-30T11:21:20Z",
          "updated": "2026-01-30T11:21:20Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "math.OC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22849v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22830v1",
          "title": "A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions",
          "summary": "Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.",
          "authors": [
            "Ji Zhou",
            "Yilin Ding",
            "Yongqi Zhao",
            "Jiachen Xu",
            "Arno Eichberger"
          ],
          "published": "2026-01-30T10:58:24Z",
          "updated": "2026-01-30T10:58:24Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22830v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22823v1",
          "title": "Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment",
          "summary": "We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.",
          "authors": [
            "Mathieu Petitbois",
            "Rémy Portelas",
            "Sylvain Lamprier"
          ],
          "published": "2026-01-30T10:49:22Z",
          "updated": "2026-01-30T10:49:22Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22823v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22689v1",
          "title": "Assistive Robots and Reasonable Work Assignment Reduce Perceived Stigma toward Persons with Disabilities",
          "summary": "Robots are becoming more prominent in assisting persons with disabilities (PwD). Whilst there is broad consensus that robots can assist in mitigating physical impairments, the extent to which they can facilitate social inclusion remains equivocal. In fact, the exposed status of assisted workers could likewise lead to reduced or increased perceived stigma by other workers. We present a vignette study on the perceived cognitive and behavioral stigma toward PwD in the workplace. We designed four experimental conditions depicting a coworker with an impairment in work scenarios: overburdened work, suitable work, and robot-assisted work only for the coworker, and an offer of robot-assisted work for everyone. Our results show that cognitive stigma is significantly reduced when the work task is adapted to the person's abilities or augmented by an assistive robot. In addition, offering robot-assisted work for everyone, in the sense of universal design, further reduces perceived cognitive stigma. Thus, we conclude that assistive robots reduce perceived cognitive stigma, thereby supporting the use of collaborative robots in work scenarios involving PwDs.",
          "authors": [
            "Stina Klein",
            "Birgit Prodinger",
            "Elisabeth André",
            "Lars Mikelsons",
            "Nils Mandischer"
          ],
          "published": "2026-01-30T08:08:38Z",
          "updated": "2026-01-30T08:08:38Z",
          "primary_category": "cs.HC",
          "categories": [
            "cs.HC",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22689v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22686v1",
          "title": "FlyAware: Inertia-Aware Aerial Manipulation via Vision-Based Estimation and Post-Grasp Adaptation",
          "summary": "Aerial manipulators (AMs) are gaining increasing attention in automated transportation and emergency services due to their superior dexterity compared to conventional multirotor drones. However, their practical deployment is challenged by the complexity of time-varying inertial parameters, which are highly sensitive to payload variations and manipulator configurations. Inspired by human strategies for interacting with unknown objects, this letter presents a novel onboard framework for robust aerial manipulation. The proposed system integrates a vision-based pre-grasp inertia estimation module with a post-grasp adaptation mechanism, enabling real-time estimation and adaptation of inertial dynamics. For control, we develop an inertia-aware adaptive control strategy based on gain scheduling, and assess its robustness via frequency-domain system identification. Our study provides new insights into post-grasp control for AMs, and real-world experiments validate the effectiveness and feasibility of the proposed framework.",
          "authors": [
            "Biyu Ye",
            "Na Fan",
            "Zhengping Fan",
            "Weiliang Deng",
            "Hongming Chen",
            "Qifeng Chen",
            "Ximin Lyu"
          ],
          "published": "2026-01-30T08:02:33Z",
          "updated": "2026-01-30T08:02:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22686v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22672v1",
          "title": "Postural Virtual Fixtures for Ergonomic Physical Interactions with Supernumerary Robotic Bodies",
          "summary": "Conjoined collaborative robots, functioning as supernumerary robotic bodies (SRBs), can enhance human load tolerance abilities. However, in tasks involving physical interaction with humans, users may still adopt awkward, non-ergonomic postures, which can lead to discomfort or injury over time. In this paper, we propose a novel control framework that provides kinesthetic feedback to SRB users when a non-ergonomic posture is detected, offering resistance to discourage such behaviors. This approach aims to foster long-term learning of ergonomic habits and promote proper posture during physical interactions. To achieve this, a virtual fixture method is developed, integrated with a continuous, online ergonomic posture assessment framework. Additionally, to improve coordination between the operator and the SRB, which consists of a robotic arm mounted on a floating base, the position of the floating base is adjusted as needed. Experimental results demonstrate the functionality and efficacy of the ergonomics-driven control framework, including two user studies involving practical loco-manipulation tasks with 14 subjects, comparing the proposed framework with a baseline control framework that does not account for human ergonomics.",
          "authors": [
            "Theodora Kastritsi",
            "Marta Lagomarsino",
            "Arash Ajoudani"
          ],
          "published": "2026-01-30T07:44:41Z",
          "updated": "2026-01-30T07:44:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22672v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07007v1",
          "title": "ARGOS: Automated Functional Safety Requirement Synthesis for Embodied AI via Attribute-Guided Combinatorial Reasoning",
          "summary": "Ensuring functional safety is essential for the deployment of Embodied AI in complex open-world environments. However, traditional Hazard Analysis and Risk Assessment (HARA) methods struggle to scale in this domain. While HARA relies on enumerating risks for finite and pre-defined function lists, Embodied AI operates on open-ended natural language instructions, creating a challenge of combinatorial interaction risks. Whereas Large Language Models (LLMs) have emerged as a promising solution to this scalability challenge, they often lack physical grounding, yielding semantically superficial and incoherent hazard descriptions. To overcome these limitations, we propose a new framework ARGOS (AttRibute-Guided cOmbinatorial reaSoning), which bridges the gap between open-ended user instructions and concrete physical attributes. By dynamically decomposing entities from instructions into these fine-grained properties, ARGOS grounds LLM reasoning in causal risk factors to generate physically plausible hazard scenarios. It then instantiates abstract safety standards, such as ISO 13482, into context-specific Functional Safety Requirements (FSRs) by integrating these scenarios with robot capabilities. Extensive experiments validate that ARGOS produces high-quality FSRs and outperforms baselines in identifying long-tail risks. Overall, this work paves the way for systematic and grounded functional safety requirement generation, a critical step toward the safe industrial deployment of Embodied AI.",
          "authors": [
            "Dongsheng Chen",
            "Yuxuan Li",
            "Yi Lin",
            "Guanhua Chen",
            "Jiaxin Zhang",
            "Xiangyu Zhao",
            "Lei Ma",
            "Xin Yao",
            "Xuetao Wei"
          ],
          "published": "2026-01-30T05:40:11Z",
          "updated": "2026-01-30T05:40:11Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07007v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.11183v1",
          "title": "Mitigating Error Accumulation in Continuous Navigation via Memory-Augmented Kalman Filtering",
          "summary": "Continuous navigation in complex environments is critical for Unmanned Aerial Vehicle (UAV). However, the existing Vision-Language Navigation (VLN) models follow the dead-reckoning, which iteratively updates its position for the next waypoint prediction, and subsequently construct the complete trajectory. Then, such stepwise manner will inevitably lead to accumulated errors of position over time, resulting in misalignment between internal belief and objective coordinates, which is known as \"state drift\" and ultimately compromises the full trajectory prediction. Drawing inspiration from classical control theory, we propose to correct for errors by formulating such sequential prediction as a recursive Bayesian state estimation problem. In this paper, we design NeuroKalman, a novel framework that decouples navigation into two complementary processes: a Prior Prediction, based on motion dynamics and a Likelihood Correction, from historical observation. We first mathematically associate Kernel Density Estimation of the measurement likelihood with the attention-based retrieval mechanism, which then allows the system to rectify the latent representation using retrieved historical anchors without gradient updates. Comprehensive experiments on TravelUAV benchmark demonstrate that, with only 10% of the training data fine-tuning, our method clearly outperforms strong baselines and regulates drift accumulation.",
          "authors": [
            "Yin Tang",
            "Jiawei Ma",
            "Jinrui Zhang",
            "Alex Jinpeng Wang",
            "Deyu Zhang"
          ],
          "published": "2026-01-30T05:03:08Z",
          "updated": "2026-01-30T05:03:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.11183v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22550v1",
          "title": "Exo-Plore: Exploring Exoskeleton Control Space through Human-aligned Simulation",
          "summary": "Exoskeletons show great promise for enhancing mobility, but providing appropriate assistance remains challenging due to the complexity of human adaptation to external forces. Current state-of-the-art approaches for optimizing exoskeleton controllers require extensive human experiments in which participants must walk for hours, creating a paradox: those who could benefit most from exoskeleton assistance, such as individuals with mobility impairments, are rarely able to participate in such demanding procedures. We present Exo-plore, a simulation framework that combines neuromechanical simulation with deep reinforcement learning to optimize hip exoskeleton assistance without requiring real human experiments. Exo-plore can (1) generate realistic gait data that captures human adaptation to assistive forces, (2) produce reliable optimization results despite the stochastic nature of human gait, and (3) generalize to pathological gaits, showing strong linear relationships between pathology severity and optimal assistance.",
          "authors": [
            "Geonho Leem",
            "Jaedong Lee",
            "Jehee Lee",
            "Seungmoon Song",
            "Jungdam Won"
          ],
          "published": "2026-01-30T04:45:35Z",
          "updated": "2026-01-30T04:45:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.GR",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22550v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22545v1",
          "title": "Adapting Reinforcement Learning for Path Planning in Constrained Parking Scenarios",
          "summary": "Real-time path planning in constrained environments remains a fundamental challenge for autonomous systems. Traditional classical planners, while effective under perfect perception assumptions, are often sensitive to real-world perception constraints and rely on online search procedures that incur high computational costs. In complex surroundings, this renders real-time deployment prohibitive. To overcome these limitations, we introduce a Deep Reinforcement Learning (DRL) framework for real-time path planning in parking scenarios. In particular, we focus on challenging scenes with tight spaces that require a high number of reversal maneuvers and adjustments. Unlike classical planners, our solution does not require ideal and structured perception, and in principle, could avoid the need for additional modules such as localization and tracking, resulting in a simpler and more practical implementation. Also, at test time, the policy generates actions through a single forward pass at each step, which is lightweight enough for real-time deployment. The task is formulated as a sequential decision-making problem grounded in a bicycle model dynamics, enabling the agent to directly learn navigation policies that respect vehicle kinematics and environmental constraints in the closed-loop setting. A new benchmark is developed to support both training and evaluation, capturing diverse and challenging scenarios. Our approach achieves state-of-the-art success rates and efficiency, surpassing classical planner baselines by +96% in success rate and +52% in efficiency. Furthermore, we release our benchmark as an open-source resource for the community to foster future research in autonomous systems. The benchmark and accompanying tools are available at https://github.com/dqm5rtfg9b-collab/Constrained_Parking_Scenarios.",
          "authors": [
            "Feng Tao",
            "Luca Paparusso",
            "Chenyi Gu",
            "Robin Koehler",
            "Chenxu Wu",
            "Xinyu Huang",
            "Christian Juette",
            "David Paz",
            "Ren Liu"
          ],
          "published": "2026-01-30T04:35:49Z",
          "updated": "2026-01-30T04:35:49Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22545v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22517v1",
          "title": "RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing",
          "summary": "Achieving human-level competitive intelligence and physical agility in humanoid robots remains a major challenge, particularly in contact-rich and highly dynamic tasks such as boxing. While Multi-Agent Reinforcement Learning (MARL) offers a principled framework for strategic interaction, its direct application to humanoid control is hindered by high-dimensional contact dynamics and the absence of strong physical motion priors. We propose RoboStriker, a hierarchical three-stage framework that enables fully autonomous humanoid boxing by decoupling high-level strategic reasoning from low-level physical execution. The framework first learns a comprehensive repertoire of boxing skills by training a single-agent motion tracker on human motion capture data. These skills are subsequently distilled into a structured latent manifold, regularized by projecting the Gaussian-parameterized distribution onto a unit hypersphere. This topological constraint effectively confines exploration to the subspace of physically plausible motions. In the final stage, we introduce Latent-Space Neural Fictitious Self-Play (LS-NFSP), where competing agents learn competitive tactics by interacting within the latent action space rather than the raw motor space, significantly stabilizing multi-agent training. Experimental results demonstrate that RoboStriker achieves superior competitive performance in simulation and exhibits sim-to-real transfer. Our website is available at RoboStriker.",
          "authors": [
            "Kangning Yin",
            "Zhe Cao",
            "Wentao Dong",
            "Weishuai Zeng",
            "Tianyi Zhang",
            "Qiang Zhang",
            "Jingbo Wang",
            "Jiangmiao Pang",
            "Ming Zhou",
            "Weinan Zhang"
          ],
          "published": "2026-01-30T03:51:58Z",
          "updated": "2026-01-30T03:51:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22517v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22467v1",
          "title": "CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control",
          "summary": "Recent advances in Vision-Language-Action (VLA) models have shown promise for robot control, but their dependence on action supervision limits scalability and generalization. To address this challenge, we introduce CARE, a novel framework designed to train VLA models for robotic task execution. Unlike existing methods that depend on action annotations during pretraining, CARE eliminates the need for explicit action labels by leveraging only video-text pairs. These weakly aligned data sources enable the model to learn continuous latent action representations through a newly designed multi-task pretraining objective. During fine-tuning, a small set of labeled data is used to train the action head for control. Experimental results across various simulation tasks demonstrate CARE's superior success rate, semantic interpretability, and ability to avoid shortcut learning. These results underscore CARE's scalability, interpretability, and effectiveness in robotic control with weak supervision.",
          "authors": [
            "Jiaqi Shi",
            "Xulong Zhang",
            "Xiaoyang Qu",
            "Jianzong Wang"
          ],
          "published": "2026-01-30T02:28:32Z",
          "updated": "2026-01-30T02:28:32Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22467v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22445v1",
          "title": "High-Definition 5MP Stereo Vision Sensing for Robotics",
          "summary": "High-resolution (5MP+) stereo vision systems are essential for advancing robotic capabilities, enabling operation over longer ranges and generating significantly denser and accurate 3D point clouds. However, realizing the full potential of high-angular-resolution sensors requires a commensurately higher level of calibration accuracy and faster processing -- requirements often unmet by conventional methods. This study addresses that critical gap by processing 5MP camera imagery using a novel, advanced frame-to-frame calibration and stereo matching methodology designed to achieve both high accuracy and speed. Furthermore, we introduce a new approach to evaluate real-time performance by comparing real-time disparity maps with ground-truth disparity maps derived from more computationally intensive stereo matching algorithms. Crucially, the research demonstrates that high-pixel-count cameras yield high-quality point clouds only through the implementation of high-accuracy calibration.",
          "authors": [
            "Leaf Jiang",
            "Matthew Holzel",
            "Bernhard Kaplan",
            "Hsiou-Yuan Liu",
            "Sabyasachi Paul",
            "Karen Rankin",
            "Piotr Swierczynski"
          ],
          "published": "2026-01-30T01:29:34Z",
          "updated": "2026-01-30T01:29:34Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22445v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22406v1",
          "title": "Accurate Pedestrian Tracking in Urban Canyons: A Multi-Modal Fusion Approach",
          "summary": "The contribution describes a pedestrian navigation approach designed to improve localization accuracy in urban environments where GNSS performance is degraded, a problem that is especially critical for blind or low-vision users who depend on precise guidance such as identifying the correct side of a street. To address GNSS limitations and the impracticality of camera-based visual positioning, the work proposes a particle filter based fusion of GNSS and inertial data that incorporates spatial priors from maps, such as impassable buildings and unlikely walking areas, functioning as a probabilistic form of map matching. Inertial localization is provided by the RoNIN machine learning method, and fusion with GNSS is achieved by weighting particles based on their consistency with GNSS estimates and uncertainty. The system was evaluated on six challenging walking routes in downtown San Francisco using three metrics related to sidewalk correctness and localization error. Results show that the fused approach (GNSS+RoNIN+PF) significantly outperforms GNSS only localization on most metrics, while inertial-only localization with particle filtering also surpasses GNSS alone for critical measures such as sidewalk assignment and across street error.",
          "authors": [
            "Shahar Dubiner",
            "Peng Ren",
            "Roberto Manduchi"
          ],
          "published": "2026-01-29T23:23:59Z",
          "updated": "2026-01-29T23:23:59Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22406v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22387v1",
          "title": "Plant-Inspired Robot Design Metaphors for Ambient HRI",
          "summary": "Plants offer a paradoxical model for interaction: they are ambient, low-demand presences that nonetheless shape atmosphere, routines, and relationships through temporal rhythms and subtle expressions. In contrast, most human-robot interaction (HRI) has been grounded in anthropomorphic and zoomorphic paradigms, producing overt, high-demand forms of engagement. Using a Research through Design (RtD) methodology, we explore plants as metaphoric inspiration for HRI; we conducted iterative cycles of ideation, prototyping, and reflection to investigate what design primitives emerge from plant metaphors and morphologies, and how these primitives can be combined into expressive robotic forms. We present a suite of speculative, open-source prototypes that help probe plant-inspired presence, temporality, form, and gestures. We deepened our learnings from design and prototyping through prototype-centered workshops that explored people's perceptions and imaginaries of plant-inspired robots. This work contributes: (1) Set of plant-inspired robotic artifacts; (2) Designerly insights on how people perceive plant-inspired robots; and (3) Design consideration to inform how to use plant metaphors to reshape HRI.",
          "authors": [
            "Victor Nikhil Antony",
            "Adithya R N",
            "Sarah Derrick",
            "Zhili Gong",
            "Peter M. Donley",
            "Chien-Ming Huang"
          ],
          "published": "2026-01-29T22:55:04Z",
          "updated": "2026-01-29T22:55:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22387v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22381v1",
          "title": "Lantern: A Minimalist Robotic Object Platform",
          "summary": "Robotic objects are simple actuated systems that subtly blend into human environments. We design and introduce Lantern, a minimalist robotic object platform to enable building simple robotic artifacts. We conducted in-depth design and engineering iterations of Lantern's mechatronic architecture to meet specific design goals while maintaining a low build cost (~40 USD). As an extendable, open-source platform, Lantern aims to enable exploration of a range of HRI scenarios by leveraging human tendency to assign social meaning to simple forms. To evaluate Lantern's potential for HRI, we conducted a series of explorations: 1) a co-design workshop, 2) a sensory room case study, 3) distribution to external HRI labs, 4) integration into a graduate-level HRI course, and 5) public exhibitions with older adults and children. Our findings show that Lantern effectively evokes engagement, can support versatile applications ranging from emotion regulation to focused work, and serves as a viable platform for lowering barriers to HRI as a field.",
          "authors": [
            "Victor Nikhil Antony",
            "Zhili Gong",
            "Guanchen Li",
            "Clara Jeon",
            "Chien-Ming Huang"
          ],
          "published": "2026-01-29T22:43:37Z",
          "updated": "2026-01-29T22:43:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22381v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22356v1",
          "title": "PoSafeNet: Safe Learning with Poset-Structured Neural Nets",
          "summary": "Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.",
          "authors": [
            "Kiwan Wong",
            "Wei Xiao",
            "Daniela Rus"
          ],
          "published": "2026-01-29T22:03:32Z",
          "updated": "2026-01-29T22:03:32Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22356v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22289v1",
          "title": "ReloPush-BOSS: Optimization-guided Nonmonotone Rearrangement Planning for a Car-like Robot Pusher",
          "summary": "We focus on multi-object rearrangement planning in densely cluttered environments using a car-like robot pusher. The combination of kinematic, geometric and physics constraints underlying this domain results in challenging nonmonotone problem instances which demand breaking each manipulation action into multiple parts to achieve a desired object rearrangement. Prior work tackles such instances by planning prerelocations, temporary object displacements that enable constraint satisfaction, but deciding where to prerelocate remains difficult due to local minima leading to infeasible or high-cost paths. Our key insight is that these minima can be avoided by steering a prerelocation optimization toward low-cost regions informed by Dubins path classification. These optimized prerelocations are integrated into an object traversability graph that encodes kinematic, geometric, and pushing constraints. Searching this graph in a depth-first fashion results in efficient, feasible rearrangement sequences. Across a series of densely cluttered scenarios with up to 13 objects, our framework, ReloPush-BOSS, exhibits consistently highest success rates and shortest pushing paths compared to state-of-the-art baselines. Hardware experiments on a 1/10 car-like pusher demonstrate the robustness of our approach. Code and footage from our experiments can be found at: https://fluentrobotics.com/relopushboss.",
          "authors": [
            "Jeeho Ahn",
            "Christoforos Mavrogiannis"
          ],
          "published": "2026-01-29T20:03:33Z",
          "updated": "2026-01-29T20:03:33Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22289v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22242v2",
          "title": "Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data",
          "summary": "A driving algorithm that aligns with good human driving practices, or at the very least collaborates effectively with human drivers, is crucial for developing safe and efficient autonomous vehicles. In practice, two main approaches are commonly adopted: (i) supervised or imitation learning, which requires comprehensive naturalistic driving data capturing all states that influence a vehicle's decisions and corresponding actions, and (ii) reinforcement learning (RL), where the simulated driving environment either matches or is intentionally more challenging than real-world conditions. Both methods depend on high-quality observations of real-world driving behavior, which are often difficult and costly to obtain. State-of-the-art sensors on individual vehicles can gather microscopic data, but they lack context about the surrounding conditions. Conversely, roadside sensors can capture traffic flow and other macroscopic characteristics, but they cannot associate this information with individual vehicles on a microscopic level. Motivated by this complementarity, we propose a framework that reconstructs unobserved microscopic states from macroscopic observations, using microscopic data to anchor observed vehicle behaviors, and learns a shared policy whose behavior is microscopically consistent with the partially observed trajectories and actions and macroscopically aligned with target traffic statistics when deployed population-wide. Such constrained and regularized policies promote realistic flow patterns and safe coordination with human drivers at scale.",
          "authors": [
            "Zhihao Zhang",
            "Keith Redmill",
            "Chengyang Peng",
            "Bowen Weng"
          ],
          "published": "2026-01-29T19:09:28Z",
          "updated": "2026-02-09T15:05:47Z",
          "primary_category": "cs.MA",
          "categories": [
            "cs.MA",
            "cs.LG",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22242v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22153v1",
          "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
          "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.",
          "authors": [
            "Haozhe Xie",
            "Beichen Wen",
            "Jiarui Zheng",
            "Zhaoxi Chen",
            "Fangzhou Hong",
            "Haiwen Diao",
            "Ziwei Liu"
          ],
          "published": "2026-01-29T18:59:51Z",
          "updated": "2026-01-29T18:59:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22153v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22090v1",
          "title": "ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection",
          "summary": "Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.",
          "authors": [
            "Runsheng Wang",
            "Katelyn Lee",
            "Xinyue Zhu",
            "Lauren Winterbottom",
            "Dawn M. Nilsen",
            "Joel Stein",
            "Matei Ciocarlie"
          ],
          "published": "2026-01-29T18:26:51Z",
          "updated": "2026-01-29T18:26:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22090v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22074v1",
          "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning",
          "summary": "We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.",
          "authors": [
            "Kevin Zakka",
            "Qiayuan Liao",
            "Brent Yi",
            "Louis Le Lay",
            "Koushil Sreenath",
            "Pieter Abbeel"
          ],
          "published": "2026-01-29T18:11:26Z",
          "updated": "2026-01-29T18:11:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22074v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22018v2",
          "title": "PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy",
          "summary": "Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.",
          "authors": [
            "Jinhao Zhang",
            "Zhexuan Zhou",
            "Huizhe Li",
            "Yichen Lai",
            "Wenlong Xia",
            "Haoming Song",
            "Youmin Gong",
            "Jie Mei"
          ],
          "published": "2026-01-29T17:23:25Z",
          "updated": "2026-01-30T13:52:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22018v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21998v1",
          "title": "Causal World Modeling for Robot Control",
          "summary": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.",
          "authors": [
            "Lin Li",
            "Qihang Zhang",
            "Yiming Luo",
            "Shuai Yang",
            "Ruilin Wang",
            "Fei Han",
            "Mingrui Yu",
            "Zelin Gao",
            "Nan Xue",
            "Xing Zhu",
            "Yujun Shen",
            "Yinghao Xu"
          ],
          "published": "2026-01-29T17:07:43Z",
          "updated": "2026-01-29T17:07:43Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21998v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21988v1",
          "title": "Generalized Information Gathering Under Dynamics Uncertainty",
          "summary": "An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.",
          "authors": [
            "Fernando Palafox",
            "Jingqi Li",
            "Jesse Milzman",
            "David Fridovich-Keil"
          ],
          "published": "2026-01-29T17:00:35Z",
          "updated": "2026-01-29T17:00:35Z",
          "primary_category": "cs.LG",
          "categories": [
            "cs.LG",
            "cs.AI",
            "cs.MA",
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21988v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21976v1",
          "title": "Macro-Scale Electrostatic Origami Motor",
          "summary": "Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.",
          "authors": [
            "Alex S. Miller",
            "Leo McElroy",
            "Jeffrey H. Lang"
          ],
          "published": "2026-01-29T16:53:24Z",
          "updated": "2026-01-29T16:53:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "physics.app-ph"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21976v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21971v1",
          "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts",
          "summary": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.",
          "authors": [
            "Lorenzo Mazza",
            "Ariel Rodriguez",
            "Rayan Younis",
            "Martin Lelis",
            "Ortrun Hellig",
            "Chenpan Li",
            "Sebastian Bodenstedt",
            "Martin Wagner",
            "Stefanie Speidel"
          ],
          "published": "2026-01-29T16:50:14Z",
          "updated": "2026-01-29T16:50:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21971v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21926v2",
          "title": "Information Filtering via Variational Regularization for Robot Manipulation",
          "summary": "Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.",
          "authors": [
            "Jinhao Zhang",
            "Wenlong Xia",
            "Yaojia Wang",
            "Zhexuan Zhou",
            "Huizhe Li",
            "Yichen Lai",
            "Haoming Song",
            "Youmin Gong",
            "Jie Mei"
          ],
          "published": "2026-01-29T16:17:42Z",
          "updated": "2026-02-01T09:53:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21926v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22199v2",
          "title": "Game-Based and Gamified Robotics Education: A Comparative Systematic Review and Design Guidelines",
          "summary": "Robotics education fosters computational thinking, creativity, and problem-solving, but remains challenging due to technical complexity. Game-based learning (GBL) and gamification offer engagement benefits, yet their comparative impact remains unclear. We present the first PRISMA-aligned systematic review and comparative synthesis of GBL and gamification in robotics education, analyzing 95 studies from 12,485 records across four databases (2014-2025). We coded each study's approach, learning context, skill level, modality, pedagogy, and outcomes (k = .918). Three patterns emerged: (1) approach-context-pedagogy coupling (GBL more prevalent in informal settings, while gamification dominated formal classrooms [p < .001] and favored project-based learning [p = .009]); (2) emphasis on introductory programming and modular kits, with limited adoption of advanced software (~17%), advanced hardware (~5%), or immersive technologies (~22%); and (3) short study horizons, relying on self-report. We propose eight research directions and a design space outlining best practices and pitfalls, offering actionable guidance for robotics education.",
          "authors": [
            "Syed T. Mubarrat",
            "Byung-Cheol Min",
            "Tianyu Shao",
            "E. Cho Smith",
            "Bedrich Benes",
            "Alejandra J. Magana",
            "Christos Mousas",
            "Dominic Kao"
          ],
          "published": "2026-01-29T15:54:36Z",
          "updated": "2026-02-04T03:41:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.HC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22199v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21884v1",
          "title": "Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation",
          "summary": "Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.",
          "authors": [
            "Pratik Ingle",
            "Jørn Lambertsen",
            "Kasper Støy",
            "Andres Faina"
          ],
          "published": "2026-01-29T15:46:50Z",
          "updated": "2026-01-29T15:46:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21884v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21876v1",
          "title": "LLM-Driven Scenario-Aware Planning for Autonomous Driving",
          "summary": "Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.",
          "authors": [
            "He Li",
            "Zhaowei Chen",
            "Rui Gao",
            "Guoliang Li",
            "Qi Hao",
            "Shuai Wang",
            "Chengzhong Xu"
          ],
          "published": "2026-01-29T15:42:13Z",
          "updated": "2026-01-29T15:42:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21876v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.22198v1",
          "title": "Advanced techniques and applications of LiDAR Place Recognition in Agricultural Environments: A Comprehensive Survey",
          "summary": "An optimal solution to the localization problem is essential for developing autonomous robotic systems. Apart from autonomous vehicles, precision agriculture is one of the elds that can bene t most from these systems. Although LiDAR place recognition is a widely used technique in recent years to achieve accurate localization, it is mostly used in urban settings. However, the lack of distinctive features and the unstructured nature of agricultural environments make place recognition challenging. This work presents a comprehensive review of state-of-the-art the latest deep learning applications for agricultural environments and LPR techniques. We focus on the challenges that arise in these environments. We analyze the existing approaches, datasets, and metrics used to evaluate LPR system performance and discuss the limitations and future directions of research in this eld. This is the rst survey that focuses on LiDAR based localization in agricultural settings, with the aim of providing a thorough understanding and fostering further research in this specialized domain.",
          "authors": [
            "Judith Vilella-Cantos",
            "Mónica Ballesta",
            "David Valiente",
            "María Flores",
            "Luis Payá"
          ],
          "published": "2026-01-29T15:28:15Z",
          "updated": "2026-01-29T15:28:15Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.ET"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.22198v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21829v1",
          "title": "GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration",
          "summary": "This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.",
          "authors": [
            "Bsher Karbouj",
            "Baha Eddin Gaaloul",
            "Jorg Kruger"
          ],
          "published": "2026-01-29T15:12:58Z",
          "updated": "2026-01-29T15:12:58Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21829v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21772v1",
          "title": "Flocking behavior for dynamic and complex swarm structures",
          "summary": "Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.",
          "authors": [
            "Carmen D. R. Pita-Romero",
            "Pedro Arias-Perez",
            "Miguel Fernandez-Cortizas",
            "Rafael Perez-Segui",
            "Pascual Campoy"
          ],
          "published": "2026-01-29T14:22:53Z",
          "updated": "2026-01-29T14:22:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21772v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21713v1",
          "title": "Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations",
          "summary": "Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.",
          "authors": [
            "Donatien Delehelle",
            "Fei Chen",
            "Darwin Caldwell"
          ],
          "published": "2026-01-29T13:41:35Z",
          "updated": "2026-01-29T13:41:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21713v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21712v2",
          "title": "CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation",
          "summary": "Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.",
          "authors": [
            "Xuanran Zhai",
            "Binkai Ou",
            "Qiaojun Yu",
            "Ce Hao",
            "Yaohua Liu"
          ],
          "published": "2026-01-29T13:40:46Z",
          "updated": "2026-02-03T08:23:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21712v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07005v1",
          "title": "Admittance-Based Motion Planning with Vision-Guided Initialization for Robotic Manipulators in Self-Driving Laboratories",
          "summary": "Self driving laboratories (SDLs) are highly automated research environments that leverage advanced technologies to conduct experiments and analyze data with minimal human involvement. These environments often involve delicate laboratory equipment, unpredictable environmental interactions, and occasional human intervention, making compliant and force aware control essential for ensuring safety, adaptability, and reliability. This paper introduces a motion-planning framework centered on admittance control to enable adaptive and compliant robotic manipulation. Unlike conventional schemes, the proposed approach integrates an admittance controller directly into trajectory execution, allowing the manipulator to dynamically respond to external forces during interaction. This capability enables human operators to override or redirect the robot's motion in real time. A vision algorithm based on structured planar pose estimation is employed to detect and localize textured planar objects through feature extraction, homography estimation, and depth fusion, thereby providing an initial target configuration for motion planning. The vision based initialization establishes the reference trajectory, while the embedded admittance controller ensures that trajectory execution remains safe, adaptive, and responsive to external forces or human intervention. The proposed strategy is validated using textured image detection as a proof of concept. Future work will extend the framework to SDL environments involving transparent laboratory objects where compliant motion planning can further enhance autonomy, safety, and human-robot collaboration.",
          "authors": [
            "Shifa Sulaiman",
            "Tobias Jensen",
            "Francesco Schetter",
            "Simon Bøgh"
          ],
          "published": "2026-01-29T13:29:56Z",
          "updated": "2026-01-29T13:29:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07005v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21667v1",
          "title": "From Instruction to Event: Sound-Triggered Mobile Manipulation",
          "summary": "Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.",
          "authors": [
            "Hao Ju",
            "Shaofei Huang",
            "Hongyu Li",
            "Zihan Ding",
            "Si Liu",
            "Meng Wang",
            "Zhedong Zheng"
          ],
          "published": "2026-01-29T13:02:10Z",
          "updated": "2026-01-29T13:02:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21667v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.00149v1",
          "title": "SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles",
          "summary": "3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.",
          "authors": [
            "Shucong Li",
            "Xiaoluo Zhou",
            "Yuqian He",
            "Zhenyu Liu"
          ],
          "published": "2026-01-29T12:52:16Z",
          "updated": "2026-01-29T12:52:16Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.RO",
            "eess.IV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.00149v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21602v2",
          "title": "AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation",
          "summary": "While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose \\textbf{AIR-VLA}, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object \\& spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. \\textbf{AIR-VLA} establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://github.com/SpencerSon2001/AIR-VLA.",
          "authors": [
            "Jianli Sun",
            "Bin Tian",
            "Qiyao Zhang",
            "Chengxiang Li",
            "Zihan Song",
            "Zhiyong Cui",
            "Yisheng Lv",
            "Yonglin Tian"
          ],
          "published": "2026-01-29T12:09:00Z",
          "updated": "2026-02-03T07:41:10Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21602v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21570v1",
          "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots",
          "summary": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.",
          "authors": [
            "Zixing Lei",
            "Genjia Liu",
            "Yuanshuo Zhang",
            "Qipeng Liu",
            "Chuan Wen",
            "Shanghang Zhang",
            "Wenzhao Lian",
            "Siheng Chen"
          ],
          "published": "2026-01-29T11:33:49Z",
          "updated": "2026-01-29T11:33:49Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21570v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21548v1",
          "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
          "summary": "Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.",
          "authors": [
            "Irene Ambrosini",
            "Ingo Blakowski",
            "Dmitrii Zendrikov",
            "Cristiano Capone",
            "Luna Gava",
            "Giacomo Indiveri",
            "Chiara De Luca",
            "Chiara Bartolozzi"
          ],
          "published": "2026-01-29T11:05:23Z",
          "updated": "2026-01-29T11:05:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.ET"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21548v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21506v1",
          "title": "IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation",
          "summary": "Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.",
          "authors": [
            "Joonhee Lee",
            "Hyunseung Shin",
            "Jeonggil Ko"
          ],
          "published": "2026-01-29T10:25:14Z",
          "updated": "2026-01-29T10:25:14Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21506v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21504v1",
          "title": "Don't double it: Efficient Agent Prediction in Occlusions",
          "summary": "Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.",
          "authors": [
            "Anna Rothenhäusler",
            "Markus Mazzola",
            "Andreas Look",
            "Raghu Rajan",
            "Joschka Bödecker"
          ],
          "published": "2026-01-29T10:22:38Z",
          "updated": "2026-01-29T10:22:38Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21504v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21474v1",
          "title": "DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching",
          "summary": "For contact-intensive tasks, the ability to generate policies that produce comprehensive tactile-aware motions is essential. However, existing data collection and skill learning systems for dexterous manipulation often suffer from low-dimensional tactile information. To address this limitation, we propose DexTac, a visuo-tactile manipulation learning framework based on kinesthetic teaching. DexTac captures multi-dimensional tactile data-including contact force distributions and spatial contact regions-directly from human demonstrations. By integrating these rich tactile modalities into a policy network, the resulting contact-aware agent enables a dexterous hand to autonomously select and maintain optimal contact regions during complex interactions. We evaluate our framework on a challenging unimanual injection task. Experimental results demonstrate that DexTac achieves a 91.67% success rate. Notably, in high-precision scenarios involving small-scale syringes, our approach outperforms force-only baselines by 31.67%. These results underscore that learning multi-dimensional tactile priors from human demonstrations is critical for achieving robust, human-like dexterous manipulation in contact-rich environments.",
          "authors": [
            "Xingyu Zhang",
            "Chaofan Zhang",
            "Boyue Zhang",
            "Zhinan Peng",
            "Shaowei Cui",
            "Shuo Wang"
          ],
          "published": "2026-01-29T09:52:46Z",
          "updated": "2026-01-29T09:52:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21474v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21454v1",
          "title": "4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving",
          "summary": "4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.",
          "authors": [
            "Shanliang Yao",
            "Zhuoxiao Li",
            "Runwei Guan",
            "Kebin Cao",
            "Meng Xia",
            "Fuping Hu",
            "Sen Xu",
            "Yong Yue",
            "Xiaohui Zhu",
            "Weiping Ding",
            "Ryan Wen Liu"
          ],
          "published": "2026-01-29T09:30:41Z",
          "updated": "2026-01-29T09:30:41Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21454v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21449v2",
          "title": "Nimbus: A Unified Embodied Synthetic Data Generation Framework",
          "summary": "Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.",
          "authors": [
            "Zeyu He",
            "Yuchang Zhang",
            "Yuanzhen Zhou",
            "Miao Tao",
            "Hengjie Li",
            "Hui Wang",
            "Yang Tian",
            "Jia Zeng",
            "Tai Wang",
            "Wenzhe Cai",
            "Yilun Chen",
            "Ning Gao",
            "Jiangmiao Pang"
          ],
          "published": "2026-01-29T09:27:31Z",
          "updated": "2026-02-09T06:57:47Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.DC"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21449v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21416v1",
          "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
          "summary": "The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.",
          "authors": [
            "Alexandre Chapin",
            "Bruno Machado",
            "Emmanuel Dellandréa",
            "Liming Chen"
          ],
          "published": "2026-01-29T08:55:53Z",
          "updated": "2026-01-29T08:55:53Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21416v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21413v1",
          "title": "Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates",
          "summary": "A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.",
          "authors": [
            "Andreas Mueller"
          ],
          "published": "2026-01-29T08:52:37Z",
          "updated": "2026-01-29T08:52:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21413v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21409v2",
          "title": "DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation",
          "summary": "Adaptive navigation in unfamiliar indoor environments is crucial for household service robots. Despite advances in zero-shot perception and reasoning from vision-language models, existing navigation systems still rely on single-pass scoring at the decision layer, leading to overconfident long-horizon errors and redundant exploration. To tackle these problems, we propose Dual-Stance Cooperative Debate Navigation (DSCD-Nav), a decision mechanism that replaces one-shot scoring with stance-based cross-checking and evidence-aware arbitration to improve action reliability under partial observability. Specifically, given the same observation and candidate action set, we explicitly construct two stances by conditioning the evaluation on diverse and complementary objectives: a Task-Scene Understanding (TSU) stance that prioritizes goal progress from scene-layout cues, and a Safety-Information Balancing (SIB) stance that emphasizes risk and information value. The stances conduct a cooperative debate and make policy by cross-checking their top candidates with cue-grounded arguments. Then, a Navigation Consensus Arbitration (NCA) agent is employed to consolidate both sides' reasons and evidence, optionally triggering lightweight micro-probing to verify uncertain choices, preserving NCA's primary intent while disambiguating. Experiments on HM3Dv1, HM3Dv2, and MP3D demonstrate consistent improvements in success and path efficiency while reducing exploration redundancy.",
          "authors": [
            "Weitao An",
            "Qi Liu",
            "Chenghao Xu",
            "Jiayi Chai",
            "Xu Yang",
            "Kun Wei",
            "Cheng Deng"
          ],
          "published": "2026-01-29T08:47:55Z",
          "updated": "2026-01-31T05:43:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21409v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21394v1",
          "title": "Towards Space-Based Environmentally-Adaptive Grasping",
          "summary": "Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.",
          "authors": [
            "Leonidas Askianakis",
            "Aleksandr Artemov"
          ],
          "published": "2026-01-29T08:31:03Z",
          "updated": "2026-01-29T08:31:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21394v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21363v2",
          "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
          "summary": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.Code and videos: https://lift-humanoid.github.io",
          "authors": [
            "Weidong Huang",
            "Zhehan Li",
            "Hangxin Liu",
            "Biao Hou",
            "Yao Su",
            "Jingwen Zhang"
          ],
          "published": "2026-01-29T07:43:24Z",
          "updated": "2026-02-13T01:46:40Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21363v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21346v1",
          "title": "HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control",
          "summary": "Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.",
          "authors": [
            "Wei Zuo",
            "Chengyang Li",
            "Yikun Wang",
            "Bingyang Cheng",
            "Zeyi Ren",
            "Shuai Wang",
            "Derrick Wing Kwan Ng",
            "Yik-Chung Wu"
          ],
          "published": "2026-01-29T07:15:39Z",
          "updated": "2026-01-29T07:15:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21346v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21297v1",
          "title": "Deep QP Safety Filter: Model-free Learning for Reachability-based Safety Filter",
          "summary": "We introduce Deep QP Safety Filter, a fully data-driven safety layer for black-box dynamical systems. Our method learns a Quadratic-Program (QP) safety filter without model knowledge by combining Hamilton-Jacobi (HJ) reachability with model-free learning. We construct contraction-based losses for both the safety value and its derivatives, and train two neural networks accordingly. In the exact setting, the learned critic converges to the viscosity solution (and its derivative), even for non-smooth values. Across diverse dynamical systems -- even including a hybrid system -- and multiple RL tasks, Deep QP Safety Filter substantially reduces pre-convergence failures while accelerating learning toward higher returns than strong baselines, offering a principled and practical route to safe, model-free control.",
          "authors": [
            "Byeongjun Kim",
            "H. Jin Kim"
          ],
          "published": "2026-01-29T05:49:48Z",
          "updated": "2026-01-29T05:49:48Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SY"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21297v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21251v1",
          "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
          "summary": "Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.",
          "authors": [
            "Ce Hao",
            "Xuanran Zhai",
            "Yaohua Liu",
            "Harold Soh"
          ],
          "published": "2026-01-29T04:17:56Z",
          "updated": "2026-01-29T04:17:56Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21251v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21188v1",
          "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
          "summary": "Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.",
          "authors": [
            "Hao Cheng",
            "Feitian Zhang"
          ],
          "published": "2026-01-29T02:36:26Z",
          "updated": "2026-01-29T02:36:26Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21188v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21173v1",
          "title": "InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios",
          "summary": "With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.",
          "authors": [
            "Zeyi Liu",
            "Shuang Liu",
            "Jihai Min",
            "Zhaoheng Zhang",
            "Jun Cen",
            "Pengyu Han",
            "Songqiao Hu",
            "Zihan Meng",
            "Xiao He",
            "Donghua Zhou"
          ],
          "published": "2026-01-29T02:18:24Z",
          "updated": "2026-01-29T02:18:24Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21173v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21129v1",
          "title": "WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics",
          "summary": "Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.",
          "authors": [
            "Guangping Liu",
            "Tipu Sultan",
            "Vittorio Di Giorgio",
            "Nick Hawkins",
            "Flavio Esposito",
            "Madi Babaiasl"
          ],
          "published": "2026-01-29T00:02:51Z",
          "updated": "2026-01-29T00:02:51Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21129v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21126v1",
          "title": "AI-Augmented Density-Driven Optimal Control (D2OC) for Decentralized Environmental Mapping",
          "summary": "This paper presents an AI-augmented decentralized framework for multi-agent (multi-robot) environmental mapping under limited sensing and communication. While conventional coverage formulations achieve effective spatial allocation when an accurate reference map is available, their performance deteriorates under uncertain or biased priors. The proposed method introduces an adaptive and self-correcting mechanism that enables agents to iteratively refine local density estimates within an optimal transport-based framework, ensuring theoretical consistency and scalability. A dual multilayer perceptron (MLP) module enhances adaptivity by inferring local mean-variance statistics and regulating virtual uncertainty for long-unvisited regions, mitigating stagnation around local minima. Theoretical analysis rigorously proves convergence under the Wasserstein metric, while simulation results demonstrate that the proposed AI-augmented Density-Driven Optimal Control consistently achieves robust and precise alignment with the ground-truth density, yielding substantially higher-fidelity reconstruction of complex multi-modal spatial distributions compared with conventional decentralized baselines.",
          "authors": [
            "Kooktae Lee",
            "Julian Martinez"
          ],
          "published": "2026-01-28T23:55:02Z",
          "updated": "2026-01-28T23:55:02Z",
          "primary_category": "cs.MA",
          "categories": [
            "cs.MA",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21126v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21063v1",
          "title": "Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned",
          "summary": "Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.",
          "authors": [
            "Pierre-Yves Lajoie",
            "Karthik Soma",
            "Haechan Mark Bong",
            "Alice Lemieux-Bourque",
            "Rongge Zhang",
            "Vivek Shankar Varadharajan",
            "Giovanni Beltrame"
          ],
          "published": "2026-01-28T21:40:46Z",
          "updated": "2026-01-28T21:40:46Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21063v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21027v1",
          "title": "Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing",
          "summary": "This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.",
          "authors": [
            "Youngim Nam",
            "Jungbin Kim",
            "Kyungtae Kang",
            "Cheolhyeon Kwon"
          ],
          "published": "2026-01-28T20:41:23Z",
          "updated": "2026-01-28T20:41:23Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21027v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.21011v1",
          "title": "Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems",
          "summary": "The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.",
          "authors": [
            "Anshul Ranjan",
            "Anoosh Damodar",
            "Neha Chougule",
            "Dhruva S Nayak",
            "Anantharaman P. N",
            "Shylaja S S"
          ],
          "published": "2026-01-28T20:06:30Z",
          "updated": "2026-01-28T20:06:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.MA",
            "cs.OS",
            "cs.SE"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.21011v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20968v1",
          "title": "Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs",
          "summary": "In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the \"Red-Green-Gray\" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.",
          "authors": [
            "Yulie Arad",
            "Stav Ashur",
            "Nancy M. Amato"
          ],
          "published": "2026-01-28T19:07:31Z",
          "updated": "2026-01-28T19:07:31Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20968v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20846v1",
          "title": "End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting",
          "summary": "Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.",
          "authors": [
            "Jamie Hathaway",
            "Alireza Rastegarpanah",
            "Rustam Stolkin"
          ],
          "published": "2026-01-28T18:45:55Z",
          "updated": "2026-01-28T18:45:55Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20846v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20831v1",
          "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents",
          "summary": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.",
          "authors": [
            "Vishnu Sashank Dorbala",
            "Dinesh Manocha"
          ],
          "published": "2026-01-28T18:31:17Z",
          "updated": "2026-01-28T18:31:17Z",
          "primary_category": "cs.AI",
          "categories": [
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20831v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20797v1",
          "title": "A Methodology for Designing Knowledge-Driven Missions for Robots",
          "summary": "This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.",
          "authors": [
            "Guillermo GP-Lenza",
            "Carmen DR. Pita-Romero",
            "Miguel Fernandez-Cortizas",
            "Pascual Campoy"
          ],
          "published": "2026-01-28T17:39:03Z",
          "updated": "2026-01-28T17:39:03Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20797v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.07000v1",
          "title": "Hierarchical JEPA Meets Predictive Remote Control in Beyond 5G Networks",
          "summary": "In wireless networked control systems, ensuring timely and reliable state updates from distributed devices to remote controllers is essential for robust control performance. However, when multiple devices transmit high-dimensional states (e.g., images or video frames) over bandwidth-limited wireless networks, a critical trade-off emerges between communication efficiency and control performance. To address this challenge, we propose a Hierarchical Joint-Embedding Predictive Architecture (H-JEPA) for scalable predictive control. Instead of transmitting states, device observations are encoded into low-dimensional embeddings that preserve essential dynamics. The proposed architecture employs a three-level hierarchical prediction, with high-level, medium-level, and low-level predictors operating across different temporal resolutions, to achieve long-term prediction stability, intermediate interpolation, and fine-grained refinement, respectively. Control actions are derived within the embedding space, removing the need for state reconstruction. Simulation results on inverted cart-pole systems demonstrate that H-JEPA enables up to 42.83 % more devices to be supported under limited wireless capacity without compromising control performance.",
          "authors": [
            "Abanoub M. Girgis",
            "Ibtissam Labriji",
            "Mehdi Bennis"
          ],
          "published": "2026-01-28T17:04:06Z",
          "updated": "2026-01-28T17:04:06Z",
          "primary_category": "eess.SY",
          "categories": [
            "eess.SY",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.07000v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20776v1",
          "title": "Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy",
          "summary": "This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.",
          "authors": [
            "Huanyu Tian",
            "Martin Huber",
            "Lingyun Zeng",
            "Zhe Han",
            "Wayne Bennett",
            "Giuseppe Silvestri",
            "Gerardo Mendizabal-Ruiz",
            "Tom Vercauteren",
            "Alejandro Chavez-Badiola",
            "Christos Bergeles"
          ],
          "published": "2026-01-28T17:03:43Z",
          "updated": "2026-01-28T17:03:43Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20776v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20720v1",
          "title": "Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction",
          "summary": "End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.",
          "authors": [
            "Matej Halinkovic",
            "Nina Masarykova",
            "Alexey Vinel",
            "Marek Galinski"
          ],
          "published": "2026-01-28T15:53:32Z",
          "updated": "2026-01-28T15:53:32Z",
          "primary_category": "cs.CV",
          "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20720v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20701v1",
          "title": "One Step Is Enough: Dispersive MeanFlow Policy Optimization",
          "summary": "Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation, dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with 5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world applicability.",
          "authors": [
            "Guowei Zou",
            "Haitao Wang",
            "Hejun Wu",
            "Yukun Qian",
            "Yuhang Wang",
            "Weibing Li"
          ],
          "published": "2026-01-28T15:34:29Z",
          "updated": "2026-01-28T15:34:29Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20701v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20682v1",
          "title": "Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model",
          "summary": "Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.",
          "authors": [
            "Péter Polcz",
            "Katalin Schäffer",
            "Miklós Koller"
          ],
          "published": "2026-01-28T15:10:02Z",
          "updated": "2026-01-28T15:10:02Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20682v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20668v1",
          "title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control",
          "summary": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.",
          "authors": [
            "Shuhao Liao",
            "Peizhuo Li",
            "Xinrong Yang",
            "Linnan Chang",
            "Zhaoxin Fan",
            "Qing Wang",
            "Lei Shi",
            "Yuhong Cao",
            "Wenjun Wu",
            "Guillaume Sartoretti"
          ],
          "published": "2026-01-28T14:49:52Z",
          "updated": "2026-01-28T14:49:52Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20668v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20577v2",
          "title": "MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization",
          "summary": "Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.",
          "authors": [
            "Baiqing Wang",
            "Helei Cui",
            "Bo Zhang",
            "Xiaolong Zheng",
            "Bin Guo",
            "Zhiwen Yu"
          ],
          "published": "2026-01-28T13:15:58Z",
          "updated": "2026-02-13T09:56:37Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20577v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20555v1",
          "title": "Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands",
          "summary": "Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.",
          "authors": [
            "Wadhah Zai El Amri",
            "Nicolás Navarro-Guerrero"
          ],
          "published": "2026-01-28T12:49:39Z",
          "updated": "2026-01-28T12:49:39Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SP"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20555v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20529v2",
          "title": "A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests",
          "summary": "Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.",
          "authors": [
            "Julia Richter",
            "David Oberacker",
            "Gabriela Ligeza",
            "Valentin T. Bickel",
            "Philip Arm",
            "William Talbot",
            "Marvin Grosse Besselmann",
            "Florian Kehl",
            "Tristan Schnell",
            "Hendrik Kolvenbach",
            "Rüdiger Dillmann",
            "Arne Roennau",
            "Marco Hutter"
          ],
          "published": "2026-01-28T12:10:56Z",
          "updated": "2026-01-30T14:02:50Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20529v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.15872v1",
          "title": "MARVL: Multi-Stage Guidance for Robotic Manipulation via Vision-Language Models",
          "summary": "Designing dense reward functions is pivotal for efficient robotic Reinforcement Learning (RL). However, most dense rewards rely on manual engineering, which fundamentally limits the scalability and automation of reinforcement learning. While Vision-Language Models (VLMs) offer a promising path to reward design, naive VLM rewards often misalign with task progress, struggle with spatial grounding, and show limited understanding of task semantics. To address these issues, we propose MARVL-Multi-stAge guidance for Robotic manipulation via Vision-Language models. MARVL fine-tunes a VLM for spatial and semantic consistency and decomposes tasks into multi-stage subtasks with task direction projection for trajectory sensitivity. Empirically, MARVL significantly outperforms existing VLM-reward methods on the Meta-World benchmark, demonstrating superior sample efficiency and robustness on sparse-reward manipulation tasks.",
          "authors": [
            "Xunlan Zhou",
            "Xuanlin Chen",
            "Shaowei Zhang",
            "Xiangkun Li",
            "ShengHua Wan",
            "Xiaohai Hu",
            "Yuan Lei",
            "Le Gan",
            "De-chuan Zhan"
          ],
          "published": "2026-01-28T11:25:13Z",
          "updated": "2026-01-28T11:25:13Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.15872v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.06995v1",
          "title": "When Simultaneous Localization and Mapping Meets Wireless Communications: A Survey",
          "summary": "The availability of commercial wireless communication and sensing equipment combined with the advancements in intelligent autonomous systems paves the way towards robust joint communications and simultaneous localization and mapping (SLAM). This paper surveys the state-of-the-art in the nexus of SLAM and Wireless Communications, attributing the bidirectional impact of each with a focus on visual SLAM (V-SLAM) integration. We provide an overview of key concepts related to wireless signal propagation, geometric channel modeling, and radio frequency (RF)-based localization and sensing. In addition to this, we show image processing techniques that can detect landmarks, proactively predicting optimal paths for wireless channels. Several dimensions are considered, including the prerequisites, techniques, background, and future directions and challenges of the intersection between SLAM and wireless communications. We analyze mathematical approaches such as probabilistic models, and spatial methods for signal processing, as well as key technological aspects. We expose techniques and items towards enabling a highly effective retrieval of the autonomous robot state. Among other interesting findings, we observe that monocular V-SLAM would benefit from RF relevant information, as the latter can serve as a proxy for the scale ambiguity resolution. Conversely, we find that wireless communications in the context of 5G and beyond can potentially benefit from visual odometry that is central in SLAM. Moreover, we examine other sources besides the camera for SLAM and describe the twofold relation with wireless communications. Finally, integrated solutions performing joint communications and SLAM are still in their infancy: theoretical and practical advancements are required to add higher-level localization and semantic perception capabilities to RF and multi-antenna technologies.",
          "authors": [
            "Konstantinos Gounis",
            "Sotiris A. Tegos",
            "Dimitrios Tyrovolas",
            "Panagiotis D. Diamantoulakis",
            "George K. Karagiannidis"
          ],
          "published": "2026-01-28T09:49:21Z",
          "updated": "2026-01-28T09:49:21Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.CV",
            "cs.IT",
            "cs.MA"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.06995v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20381v1",
          "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
          "summary": "Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.",
          "authors": [
            "Alexandre Chapin",
            "Emmanuel Dellandréa",
            "Liming Chen"
          ],
          "published": "2026-01-28T08:46:04Z",
          "updated": "2026-01-28T08:46:04Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20381v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20377v2",
          "title": "RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification",
          "summary": "Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.",
          "authors": [
            "Xinyan Chen",
            "Qinchun Li",
            "Ruiqin Ma",
            "Jiaqi Bai",
            "Li Yi",
            "Jianfei Yang"
          ],
          "published": "2026-01-28T08:43:48Z",
          "updated": "2026-02-02T08:31:09Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "eess.SP"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20377v2",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2602.02533v1",
          "title": "HMVLA: Hyperbolic Multimodal Fusion for Vision-Language-Action Models",
          "summary": "Vision Language Action (VLA) models have recently shown great potential in bridging multimodal perception with robotic control. However, existing methods often rely on direct fine-tuning of pre-trained Vision-Language Models (VLMs), feeding semantic and visual features directly into a policy network without fully addressing the unique semantic alignment challenges in the VLA domain. In this paper, we propose HMVLA, a novel VLA framework that exploits the inherent hierarchical structures in vision and language for comprehensive semantic alignment. Unlike traditional methods that perform alignment in Euclidean space, our HMVLA embeds multimodal features in hyperbolic space, enabling more effective modeling of the hierarchical relationships present in image text data. Furthermore, we introduce a sparsely gated Mixture of Experts (MoE) mechanism tailored for semantic alignment, which enhances multimodal comprehension between images and text while improving efficiency. Extensive experiments demonstrate that HMVLA surpasses baseline methods in both accuracy and generalization. In addition, we validate its robustness by reconstructing datasets to further test cross domain adaptability.",
          "authors": [
            "Kun Wang",
            "Xiao Feng",
            "Mingcheng Qu",
            "Tonghua Su"
          ],
          "published": "2026-01-28T07:50:30Z",
          "updated": "2026-01-28T07:50:30Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2602.02533v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20334v1",
          "title": "Demonstration-Free Robotic Control via LLM Agents",
          "summary": "Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim",
          "authors": [
            "Brian Y. Tsui",
            "Alan Y. Fang",
            "Tiffany J. Hwu"
          ],
          "published": "2026-01-28T07:49:35Z",
          "updated": "2026-01-28T07:49:35Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20334v1",
          "field": "cs.RO"
        },
        {
          "id": "http://arxiv.org/abs/2601.20321v2",
          "title": "TaF-VLA: Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation",
          "summary": "Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.",
          "authors": [
            "Yuzhe Huang",
            "Pei Lin",
            "Wanlin Li",
            "Daohan Li",
            "Jiajun Li",
            "Jiaming Jiang",
            "Chenxi Xiao",
            "Ziyuan Jiao"
          ],
          "published": "2026-01-28T07:34:41Z",
          "updated": "2026-01-30T13:45:08Z",
          "primary_category": "cs.RO",
          "categories": [
            "cs.RO"
          ],
          "pdf_url": "https://arxiv.org/pdf/2601.20321v2",
          "field": "cs.RO"
        }
      ]
    }
  ],
  "errors": [
    "cs.CV: HTTP Error 429: Unknown Error",
    "cs.CL: HTTP Error 429: Unknown Error",
    "cs.SY: HTTP Error 429: Unknown Error"
  ]
}