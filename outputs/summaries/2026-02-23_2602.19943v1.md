[0] Paper metadata  
- Title: Scaling Law of Neural Koopman Operators  
- arXiv ID: 2602.19943v1  
- Date: 2026-02-23T15:13:43Z  
- Task domain: Model-based control of nonlinear robotic dynamical systems (manipulation, locomotion, pendula) via learned linear latent dynamics  
- Key resources (code/dataset if mentioned): Code repository at https://github.com/intelligent-control-lab/Koopman-Scaling; Kinova dataset (collected by Zhongqi Wei); simulation environments: PyBullet (Franka), Isaac Sim (Unitree Go2/G1), Drake (Kinova Gen3); pretrained RL expert policy (used in Strategy III); scipy.integrate.odeint (pendulum integration)  

[1] System pipeline  
Perception → representation → policy → execution:  
- **Perception**: Raw state $x_t$ (e.g., joint positions/velocities, base pose, torques) and control input $u_t$ (e.g., joint velocity commands or desired positions) are observed directly — no raw sensor preprocessing (vision/tactile/language) is used or described. Inputs are assumed fully observed and numerically clean.  
- **Representation**: State $x_t$ is lifted via neural encoder $\Psi_\theta$ into nonlinear features; full embedding is $\Phi_\theta(x_t) = [x_t; \Psi_\theta(x_t)] \in \mathbb{R}^n$, where $n = (n_{\text{mult}} + 1) \cdot n_x$. This defines the latent state $z_t$.  
- **Policy**: A linear Koopman model $z_{t+1} = A z_t + B u_t$ governs latent dynamics. Policy is *not* a learned neural controller — instead, the linear latent model is embedded into a **Model Predictive Control (MPC)** framework. The MPC solves a finite-horizon quadratic program (QP) using the linear dynamics, with cost defined in physical space via projection $x_\tau = P z_\tau$.  
- **Execution**: Only the first control input $u_t^*$ from the MPC solution is applied to the robot; receding-horizon optimization repeats at each timestep.  
Learning components: End-to-end training of $\Psi_\theta$, $A$, and $B$ via composite loss ($\mathcal{L}_{\text{pred}} + w_{\text{cov}} \mathcal{L}_{\text{cov}} + w_{\text{ctrl}} \mathcal{L}_{\text{ctrl}}$); MPC is non-parametric and convex (no learning).  

[2] Representation design  
Only state and control modalities are used — no vision, tactile, or language representations appear in the paper.  
- **State representation**:  
  - Modality source: Fully observed physical state $x_t$ (e.g., for Unitree: base quaternion, linear/angular velocities, joint positions/velocities; for Franka/Kinova: joint positions/velocities, end-effector pose)  
  - Encoder: MLP with two 256-unit hidden layers, ReLU, residual connections (C022); final layer orthogonally initialized (C015)  
  - Embedding format: Concatenated vector $z_t = [x_t; \Psi_\theta(x_t)] \in \mathbb{R}^n$  
  - Temporal modeling: None — dynamics modeled *exclusively* via linear transition $z_{t+1} = A z_t + B u_t$; no RNN/attention over time  
  - Alignment: Physical state $x_t$ is explicitly included in $z_t$ and recovered via fixed linear projection $P$ s.t. $x_t = P z_t$ (C003, C015)  
- **Control representation**:  
  - Modality source: Actuation command $u_t$ (joint velocities for Franka/Kinova; desired joint positions for Unitree, executed via embedded PD)  
  - Encoder: None — $u_t$ is used *as-is*, not lifted or encoded (C015)  
  - Embedding format: Flat vector $u_t \in \mathbb{R}^{n_u}$, treated as exogenous linear input  
  - Temporal modeling: Not applicable — no temporal structure imposed on $u_t$ sequence  
  - Alignment: Enforced via inverse control loss $\mathcal{L}_{\text{ctrl}} = \|u_\tau - B^\dagger(z_{\tau+1} - A z_\tau)\|^2$, ensuring $B$ is full-rank and invertible (C016, C030)  

[3] Policy architecture  
- Policy type: **Model-based control via Koopman–MPC**, not a learned policy network.  
- Planning vs control decomposition: Fully decomposed — *planning* is convex QP-based MPC using the linear latent model; *control* is open-loop trajectory optimization followed by receding-horizon execution. No hierarchical or latent-space planning.  
- Action parameterization/tokenization: Continuous, low-level actuation $u_t \in \mathbb{R}^{n_u}$; no discretization or tokenization.  
- Stochastic vs deterministic: Deterministic — all components (embedding, Koopman matrices, MPC) are deterministic. No stochastic policies or uncertainty quantification.  
- Horizon: Finite-horizon MPC with horizon $T$ (unspecified value; multi-step prediction loss uses horizon $T$ in C016); receding-horizon execution implies online replanning at every timestep.  

[4] Training signals  
- Objectives/losses: Composite loss $\mathcal{L} = \mathcal{L}_{\text{pred}} + w_{\text{cov}} \mathcal{L}_{\text{cov}} + w_{\text{ctrl}} \mathcal{L}_{\text{ctrl}}$:  
  - $\mathcal{L}_{\text{pred}}$: Multi-step MSE in physical space: $(1/W)\sum_{j=1}^T \beta^j \|x_{t+j} - P(A^j z_t + \sum_{i=0}^{j-1} A^i B u_{t+j-1-i})\|^2$ (C016)  
  - $\mathcal{L}_{\text{cov}}$: Covariance regularization: $\|\text{offdiag}(G_{\text{batch}})\|_F^2$, where $G_{\text{batch}}$ is empirical Gram matrix of $\Phi_\theta(s_i)$ (C016, C008)  
  - $\mathcal{L}_{\text{ctrl}}$: Inverse control loss: $\|u_\tau - B^\dagger(z_{\tau+1} - A z_\tau)\|^2$ (C016, C030)  
- Supervision sources: Fully supervised on state-control trajectories $\{(x_t, u_t, x_{t+1})\}$ — no reward, preferences, or demonstrations beyond data collection.  
- Offline vs online: Offline — all training is batch, from pre-collected datasets (C019, C020).  
- RL vs imitation vs preference: None — purely supervised dynamics modeling (imitation of system transitions, not behavior).  
- Multi-stage schedule: Not specified — no curriculum, pretraining, or staged loss weighting described.  

[5] Experimental protocol  
- Robot platform(s): Simulated: Damping system, Double Pendulums, Franka Panda (PyBullet), Unitree Go2 & G1 (Isaac Sim); Physical: Kinova Gen3 7-DoF manipulator (Drake-controlled) (C020, C033)  
- Sensors: Not specified — assumes full-state observability; no camera, IMU, or tactile sensor modalities mentioned.  
- Data collection: Three strategies (C019, C020, C033):  
  - Strategy I: Uniform random initial states + randomized control (polynomial, pendula)  
  - Strategy II: Randomized joint velocity commands from home config (Franka)  
  - Strategy III: Expert-guided exploration (RL policy + Koopman-MPC tracking) or Drake-based safe trajectory optimization (Kinova, Unitree)  
- Tasks: Open-loop prediction (multi-step state forecasting); closed-loop control: Franka end-effector figure-8 tracking; Unitree G1/Go2 locomotion survival (C021, C027, C033)  
- Metrics:  
  - Open-loop: Prediction error (MSE) — primary metric for scaling analysis (C023–C026)  
  - Closed-loop: Franka — End-Effector Tracking Error; G1/Go2 — Mean Survival Steps (timesteps before joint error exceeds safety limits: >0.10 rad for G1, >0.16 rad for Go2) (C021)  
- Baselines: EDMD-Poly (fixed polynomial basis), NNDM (unstructured MLP dynamics model matched in parameter count), ablated Koopman variants (no aux, +Ctrl only, +Cov only) (C028, C027)  
- Ablations: Covariance loss (ℒ_cov), inverse control loss (ℒ_ctrl), their combination (+Both); latent dimension $n_{\text{mult}} \in \{1,2,4,8,16\}$; dataset size $m \in \{1000,4000,16000,64000,140000\}$; five random seeds per configuration (C022, C024–C027)  

[6] Core contribution  
- First derivation of **non-asymptotic, explicit scaling laws** for Neural Koopman Operators, decomposing total approximation error into sampling error ($\mathcal{O}(m^{-1/2})$) and projection error ($\mathcal{O}(n^{(1-2\alpha)/2})$) with rates tied to dataset size $m$ and latent dimension $n$ — enabling principled capacity/data allocation (C001, C004, C007–C010).  
- Identification and theoretical justification of an **irreducible optimization error floor**, explaining empirical saturation in scaling curves and motivating structural regularization over pure capacity scaling (C013, C023).  
- Introduction of two lightweight, theory-grounded regularizers: **covariance loss** (to stabilize latent basis conditioning and suppress sampling error variance) and **inverse control loss** (to enforce $B$-invertibility and ensure control identifiability), which negligibly affect prediction but dramatically improve closed-loop robustness — especially for underactuated, contact-rich systems (C001, C008, C016, C027, C029–C030).  
- Empirical validation across **seven diverse robotic systems**, from simple pendula to high-DOF legged robots and physical hardware, confirming unified scaling $m = \Omega(n \ln n)$ and demonstrating that structured regularization—not just scale—enables stable closed-loop control in data-limited regimes (C001, C020, C024–C026, C031, C033).  

[7] Research gaps  
- **Generalization**: No evaluation of zero-shot transfer across tasks, domains, or dynamics shifts (e.g., payload change, friction variation); generalization assessed only via held-out test trajectories within same environment.  
- **Embodiment**: No integration of raw embodiment signals — all experiments assume perfect, noiseless, full-state observation; no handling of partial observability, sensor noise, or latency.  
- **Scalability**: Latent dimension $n$ scales linearly with state dimension $n_x$ (via $n = (n_{\text{mult}} + 1)n_x$); no ablation on *absolute* $n$ beyond multipliers, and no discussion of inference latency or real-time feasibility on embedded controllers.  
- **Sim2real**: Only one physical experiment (Kinova Gen3) using Strategy III (safe trajectory optimization + expert guidance); no sim2real transfer analysis, domain randomization, or adaptation mechanism reported.  
- **Representation bottlenecks**: Exclusively state/control — no multimodal grounding (vision, language, touch); representation expressivity limited to MLP encoder with no architectural innovation (e.g., equivariance, symmetry encoding) despite theoretical emphasis on spectral decay and eigenfunction alignment (C009, C012).  

[8] Extension ideas  
- **Hypothesis**: Spectral bias (frequency principle) can be explicitly leveraged to prioritize Koopman eigenfunctions corresponding to *physically persistent modes* (e.g., center-of-mass motion) over transient ones (e.g., high-frequency joint vibrations).  
  **Change**: Replace standard MLP encoder with a **physics-informed spectral encoder**, e.g., a shallow CNN over time-windowed state sequences or a GNN over kinematic chains, initialized to emphasize low-frequency Fourier modes.  
  **Expected gain**: Improved sample efficiency for long-horizon control tasks and better generalization to unseen disturbances by aligning inductive bias with dominant Koopman eigenstructure.  

- **Hypothesis**: Covariance regularization alone cannot resolve *directional* ill-conditioning (e.g., rank-deficiency in control-relevant subspaces); enforcing task-aligned subspace geometry will improve MPC performance.  
  **Change**: Augment $\mathcal{L}_{\text{cov}}$ with a **task-aware subspace alignment loss**, e.g., minimizing angle between span$(B)$ and the controllable subspace estimated via Krylov iteration on $A,B$.  
  **Expected gain**: Higher closed-loop success rates on underactuated systems (G1/Go2) by ensuring latent dynamics encode physically controllable directions, reducing MPC conservatism.  

- **Hypothesis**: Optimization error floor arises partly from non-convex bilinear coupling ($\theta,K$); decoupling representation learning from dynamics identification will reduce local minima.  
  **Change**: Adopt a **two-stage training**: (1) Pretrain $\Psi_\theta$ via contrastive learning on state-transition pairs to learn invariant features, then (2) freeze $\Psi_\theta$ and solve for $A,B$ via least-squares EDMD.  
  **Expected gain**: Faster convergence, lower optimization residual $\epsilon_{\text{opt}}$, and more consistent scaling law adherence — especially in low-data regimes.  

- **Hypothesis**: Current MPC uses physical-space cost ($Q=I,R=0$) but ignores *latent-space geometry*; optimizing in latent space with learned metrics could improve tracking fidelity.  
  **Change**: Learn a diagonal **latent-space metric $Q_z$** as part of training, and define MPC cost as $\sum_\tau \|z_\tau - z_\tau^{\text{ref}}\|_{Q_z}^2$, where $z_\tau^{\text{ref}} = \Phi_\theta(x_\tau^{\text{ref}})$.  
  **Expected gain**: Smoother, more dynamically consistent reference tracking by accounting for nonlinear distortion in the embedding — particularly beneficial for high-curvature trajectories (e.g., Franka figure-8).  

- **Hypothesis**: Scaling laws assume ergodic sampling, but real robot data is highly structured (e.g., near equilibrium); active data collection guided by Koopman uncertainty can close the gap.  
  **Change**: Integrate **uncertainty-aware exploration**: compute posterior covariance of $A,B$ via Laplace approximation or ensemble, then collect data maximizing expected reduction in Koopman operator error bound.  
  **Expected gain**: Reduced sample complexity for hardware deployment (e.g., Kinova), especially for unstable or safety-critical regimes where Strategy III is overly conservative.  

- **Hypothesis**: Inverse control loss assumes linear control-affine structure, but real actuators exhibit saturation, delay, and coupling; mismatch harms $B$-identifiability.  
  **Change**: Replace $B u_t$ with a **learned, input-conditioned linear map** $B(u_t) z_t$, where $B(\cdot)$ is a small MLP, and modify $\mathcal{L}_{\text{ctrl}}$ to reconstruct $u_t$ from $(z_{t+1} - A z_t)$ *and* $z_t$.  
  **Expected gain**: More accurate $B$ estimation under actuator nonlinearities, leading to improved MPC feasibility and reduced tracking drift on physical hardware.  

[9] Evidence pointers  
- C001: (Scaling Law of Neural Koopman Operators, paragraphs 1-2, anchor N/A)  
- C002: (1 Introduction, paragraphs 1-3, anchor S1)  
- C003: (2 Preliminary: Koopman Operator Theory, paragraphs 1-8, anchor S2)  
- C004: (3 Theoretical analysis of Scaling Law, paragraphs 1-1, anchor S3)  
- C005: (3.1.1 Koopman Operator Theory, paragraphs 1-2, anchor S3.SS1.SSS1)  
- C006: (3.1.2 EDMD (Extended Dynamic Mode Decomposition), paragraphs 1-4, anchor S3.SS1.SSS2)  
- C007: (3.1.3 Koopman Approximation Error, paragraphs 1-4, anchor S3.SS1.SSS3)  
- C008: (3.2 Convergence of Sampling error, paragraphs 1-28, anchor S3.SS2)  
- C009: (3.3 Convergence of Projection Error, paragraphs 1-16, anchor S3.SS3)  
- C010: (3.4 Convergence of Koopman Operator, paragraphs 1-14, anchor S3.SS4)  
- C011: (3.5.1 Assumption 6: Spectral Decay, paragraphs 1-2, anchor S3.SS5.SSS1)  
- C012: (3.5.2 Assumption 7: Subspace Approximation Capability, paragraphs 1-6, anchor S3.SS5.SSS2)  
- C013: (3.5.3 Optimization Error and the Scaling Floor, paragraphs 1-4, anchor S3.SS5.SSS3)  
- C014: (4 Practical Neural Koopman Operators, paragraphs 1-1, anchor S4)  
- C015: (4.1 Koopman Model Architecture, paragraphs 1-5, anchor S4.SS1)  
- C016: (4.2 Training Objective, paragraphs 1-12, anchor S4.SS2)  
- C017: (4.3 Koopman-based Model Predictive Control, paragraphs 1-4, anchor S4.SS3)  
- C018: (5 Experimental Design, paragraphs 1-1, anchor S5)  
- C019: (5.1 Data Collection Strategy, paragraphs 1-2, anchor S5.SS1)  
- C020: (5.2 Experimental Setup, paragraphs 1-3, anchor S5.SS2)  
- C021: (5.3 Evaluation Metrics, paragraphs 1-1, anchor S5.SS3)  
- C022: (5.4 Model Architecture and Training Configuration, paragraphs 1-2, anchor S5.SS4)  
- C023: (6.1 Scaling Laws of Koopman Models, paragraphs 1-2, anchor S6.SS1)  
- C024: (6.1.1 Scaling with Training Sample Size, paragraphs 1-4, anchor S6.SS1.SSS1)  
- C025: (6.1.2 Scaling with Model Size, paragraphs 1-4, anchor S6.SS1.SSS2)  
- C026: (6.1.3 Unified Scaling: Balancing Data and Model Capacity, paragraphs 1-4, anchor S6.SS1.SSS3)  
- C027: (6.2 Effectivess of the Proposed Auxiliary losses, paragraphs 1-3, anchor S6.SS2)  
- C028: (6.3.1 Effectiveness of Neural Koopman Operators, paragraphs 1-2, anchor S6.SS3.SSS1)  
- C029: (6.3.2 Relationship Between Covariance Regularization and Prediction Error, paragraphs 1-2, anchor S6.SS3.SSS2)  
- C030: (6.3.3 Relationship Between Inverse Control Loss and Prediction Error, paragraphs 1-3, anchor S6.SS3.SSS3)  
- C031: (7 Conclusion, paragraphs 1-2, anchor S7)  
- C032: (Acknowledgment, paragraphs 1-1, anchor Sx1)  
- C033: (.1 Experimental Setup, paragraphs 1-7, anchor A0.SS1)  
- C034: (.2 Case Study: System Nonlinearity and Scaling Law, paragraphs 1-6, anchor A0.SS2)