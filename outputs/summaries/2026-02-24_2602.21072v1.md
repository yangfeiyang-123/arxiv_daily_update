[1] 文章做了什么事
- 提出了一种名为 LoDADA 的新方法，用于离线动力学外强化学习（Off-dynamics offline RL）。
- 通过对源数据和目标数据的转移进行聚类，估计集群级的动力学差异。
- 根据差异大小筛选源数据，保留差异小的集群数据，过滤差异大的集群数据。
- 提供了理论见解，并在具有多样动力学 shift 的环境中进行了广泛实验。

[2] 文章的创新点是什么
- 利用局部动力学不匹配（localized dynamics mismatch）来更好地复用源数据，而非依赖全局假设。
- 提出了一种基于集群级差异的细粒度且可扩展的数据选择策略。
- 避免了过于粗略的全局假设和昂贵的逐样本过滤计算成本。

[3] 文章解决了什么问题
- 解决了现有全局方法在处理动力学不匹配时可能忽略局部跨域相似性的问题。
- 解决了通过逐点数据过滤处理动力学不匹配时计算成本高的问题。
- 解决了如何在目标数据有限且源数据动力学不同的情况下有效学习策略的问题。

[4] 效果怎么样
- 在具有多样全局和局部动力学 shift 的环境中进行了广泛实验验证。
- 结果一致优于现有的最先进离线动力学外 RL 方法。
- 通过更好地利用局部分布不匹配实现了性能提升。

[依据]
- "We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data."
- "LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination."
- "Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost."
- "Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch."